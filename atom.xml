<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Johnny&#39;s Odyssey</title>
  
  <subtitle>Stay foolish. Stay hungry.</subtitle>
  <link href="https://liuninglin.github.io/atom.xml" rel="self"/>
  
  <link href="https://liuninglin.github.io/"/>
  <updated>2022-10-01T22:30:41.136Z</updated>
  <id>https://liuninglin.github.io/</id>
  
  <author>
    <name>Johnny Liu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Amplify-POC</title>
    <link href="https://liuninglin.github.io/posts/1998f291.html"/>
    <id>https://liuninglin.github.io/posts/1998f291.html</id>
    <published>2022-08-13T23:27:31.000Z</published>
    <updated>2022-10-01T22:30:41.136Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/1998f291/amplify_banner.jpeg" alt="amplify image"></p><p>This document is aimed to help you familiar with Amplify quickly. In addition, I’ll take you to go through the code structure and deployment architecture of my POC.</p><h2 id="What-is-Amplify"><a href="#What-is-Amplify" class="headerlink" title="What is Amplify"></a>What is Amplify</h2><p>Amplify is an all-in-one platform for frontend developers to develop, test, and deploy the whole website or mobile app. The frontend developers only need to know vanilla JavaScript or React or Vue.js, the knowledge of RestAPI, and database management is not necessary for this platform cause Amplify Studio can help frontend developers to maintain these through the UI.</p><span id="more"></span><h3 id="Pros"><a href="#Pros" class="headerlink" title="Pros"></a>Pros</h3><ul><li>The learning curve is not higher for frontend developers.</li><li>Easily to connect with other AWS services (Cognito, Lambda, AppSync, DynamoDB)</li><li>Less maintenance works.</li><li>Automatic CI&#x2F;CD.</li><li>Serverless.</li></ul><h3 id="Cons"><a href="#Cons" class="headerlink" title="Cons"></a>Cons</h3><ul><li><p>The learning curve is a little bit steep for backed developers. (Developers should learn React and GraphQL, and know how to use Figma)</p></li><li><p>Not good for developing a complex website, you still need to know some backend knowledge to develop advanced features.</p></li><li><p>For UI components, you can’t design a Figma template by yourself, you need to modify the fixed template created by Amplify team.</p></li></ul><h3 id="Use-Cases"><a href="#Use-Cases" class="headerlink" title="Use Cases"></a>Use Cases</h3><p>Based on the analysis above, Amplify is great for simple website development or mobile app with basic CRUD functions.</p><p>Like a file management system, or a simple CRM system.</p><h2 id="How-to-Start"><a href="#How-to-Start" class="headerlink" title="How to Start"></a>How to Start</h2><p>You can follow the link below to go through this workshop quickly. It provides almost everything you need to know in Amplify.</p><p><a href="https://catalog.us-east-1.prod.workshops.aws/workshops/84db0afb-0279-4d29-ae26-1609043d5bfd/en-US">https://catalog.us-east-1.prod.workshops.aws/workshops/84db0afb-0279-4d29-ae26-1609043d5bfd/en-US</a></p><h2 id="Tools-for-Development"><a href="#Tools-for-Development" class="headerlink" title="Tools for Development"></a>Tools for Development</h2><ul><li><p>Amplify Studio</p><ul><li>This is an online design platform, you can design your website or mobile app on this for most features. But for some custom features, you still need to develop in your local machine, then use Amplify CLI to sync your local changes to the remote system.</li></ul></li><li><p>Amplify CLI</p><ul><li>You can use Amplify CLI to complete all configuration and development and not to use Amplify Studio</li><li>You can reach out to this link to learn more about this. <a href="https://docs.amplify.aws/cli/">https://docs.amplify.aws/cli/</a></li></ul></li><li><p>Git CLI</p><ul><li><p>For CI&#x2F;CD and deployment, you need to commit your local codes and push them to the remote repository to activate CI&#x2F;CD. After that, you can access your remote website through a fixed URL.</p></li><li><p>For local deployment, you can use the npm command to preview changes.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm start</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>I still want to summarize all steps for my Amplify POC, you can check this first to get a landscape of development in Amplify.</p><h3 id="Steps-for-Amplify-Development"><a href="#Steps-for-Amplify-Development" class="headerlink" title="Steps for Amplify Development"></a>Steps for Amplify Development</h3><ol><li><p>Initialization of Amplify</p><ul><li><p>Install Amplify CLI</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g @aws-amplify/cli</span><br></pre></td></tr></table></figure></li><li><p>Amplify configuration</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">amplify configure</span><br></pre></td></tr></table></figure></li><li><p>Create a new React app</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npx create-react-app amplify-poc</span><br></pre></td></tr></table></figure></li><li><p>Amplify initialization</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">amplify init</span><br></pre></td></tr></table></figure></li><li><p>Install necessary packages</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install aws-amplify @aws-amplify/ui-react</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Authentication (Using AWS Cognito for SSO)</p><ol><li><p>Find an existing Cognito User Pool (e.g. Okta ) and create a new app client in it.</p></li><li><p>Create a new identity pool that connected with this Cognito User Pool and the new app client</p></li><li><p>Authentication configuration in Amplify</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">amplify add auth</span><br></pre></td></tr></table></figure><p>Follow the instruction in the command line and choose the user pool and identity pool, then push your changes to Amplify studio <code>amplify push </code>.</p><p><strong>Important Caveat</strong></p><p><strong>After this authentication configuration, you might find there’re two sets of Cognito Configuration. One is for Amplify Studio authentication, and another is for this new website or new mobile app authentication. So, please be aware of which one is amplify created automatically, and which one is created by yourself.</strong></p></li></ol></li><li><p>Importing UI components from Figma</p><ul><li><p>You can only use an Amplify-built Figma template, Amplify can’t import components built by yourself. The template link shows below.</p><p><a href="https://www.figma.com/community/file/1047600760128127424">https://www.figma.com/community/file/1047600760128127424</a></p><p>You can make a copy of this template and do some modifications based on this template. (like creating a new page component by using an existing UI component).</p></li><li><p>BTW, you can also use my POC template.</p><p><a href="https://www.figma.com/file/w9yXc7rxVcFQCAElNNmmDO/AWS-Amplify-UI-Kit-Community">https://www.figma.com/file/w9yXc7rxVcFQCAElNNmmDO/AWS-Amplify-UI-Kit-Community</a></p></li></ul></li><li><p>Data model design</p><p>Login to Amplify Studio, then you can design your data model in this online platform.</p><p>![amplify-johnny - staging - Amplify Studio 2022-08-10 14-37-20](Amplify-POC&#x2F;amplify-johnny - staging - Amplify Studio 2022-08-10 14-37-20.png)</p></li><li><p>S3 configuration</p><p>Follow instructions on AWS Amplify.</p><p>By default, the files you uploaded will store in the public folder.</p></li><li><p>Frontend development</p><p>Develop in your local machine, and you can preview by using the npm command <code>npm start</code></p><p>For your frontend development, all codes are located in the src folder.</p><img src="/posts/1998f291/Get Started — amplify-johnny 2022-08-10 14-41-53.png" alt="Get Started — amplify-johnny 2022-08-10 14-41-53" style="zoom:50%;"></li><li><p>API development</p><p>After you finished data model creation, Amplify Studio will create all related GraphQL queries for you automatically.</p><ol><li><p>Write and test your GrahQL script in Amplify Studio.</p><p>![amplify-johnny - staging - Amplify Studio 2022-08-10 14-45-15](Amplify-POC&#x2F;amplify-johnny - staging - Amplify Studio 2022-08-10 14-45-15.png)</p><p>![AppSyncConsole 2022-08-10 14-48-56](Amplify-POC&#x2F;AppSyncConsole 2022-08-10 14-48-56.png)</p></li><li><p>Copy scripts into your local codes.</p></li><li><p>Import GraphSQL queries into your React codes.</p></li></ol></li><li><p>Lambda development</p><ol><li><p>Amplify CLI</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">amplify add <span class="keyword">function</span></span><br></pre></td></tr></table></figure></li><li><p>Development in your local machine</p><p>Find your created Lambda function, and write your features in the src folder.</p><img src="/posts/1998f291/Get Started — amplify-johnny 2022-08-10 14-51-22.png" alt="Get Started — amplify-johnny 2022-08-10 14-51-22" style="zoom:50%;"></li><li><p>Add endpoint for Lambda</p><p>Using Amplify CLI and choosing REST as the calling method</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">amplify add api</span><br></pre></td></tr></table></figure><p>Then, in your React codes, you can call this API by <code>API.get</code></p><p>In addition, we can also use DynanomoDB to trigger a Lambda function. Just like the screenshot below.</p><p><img src="/posts/1998f291/image-20220810150705985.png" alt="image-20220810150705985"></p></li></ol></li></ol><h2 id="Amplify-POC"><a href="#Amplify-POC" class="headerlink" title="Amplify POC"></a>Amplify POC</h2><h3 id="Business-Requirements"><a href="#Business-Requirements" class="headerlink" title="Business Requirements"></a>Business Requirements</h3><p>This POC is aimed to recreate a Quant Portal website that is originally created based on SharePoint.</p><p>You can treat this Quant Portal as an online file management system.</p><p>The main business requirements show below.</p><img src="/posts/1998f291/Quant Portal POC@2x.png" alt="Quant Portal POC@2x" style="zoom: 33%;"><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>This is the whole architecture of this Amplify POC.</p><p>Before calling GraphQL each time, the frontend will get a token from Cognito automatically for authentication. After successful verification, it will transfer requests to AppSync, API Gateway, or S3 built-in REST API.</p><p>In your React codes, you need to use <code>API.get</code> to call AppSync through QraphQL or use <code>API.get</code> to call a REST API through API Gateway.</p><p>There are two ways of Lambda integration, one is triggered by the DynamoDB table, and another is triggered by API Gateway.</p><p>![Amplify POC](Amplify-POC&#x2F;Amplify POC.png)</p><h3 id="Code-Structure"><a href="#Code-Structure" class="headerlink" title="Code Structure"></a>Code Structure</h3><img src="/posts/1998f291/index.py — amplify-johnny 2022-08-11 13-44-12.png" alt="index.py — amplify-johnny 2022-08-11 13-44-12" style="zoom:50%;"><ul><li>amplify<ul><li>all backend codes (contains authentication, API, Lambda function, storage, etc.)</li></ul></li><li>public<ul><li>all static files (images, icons, and static HTML files)</li></ul></li><li>src<ul><li>all React codes</li></ul></li></ul><p>Next, I’ll dive into each item above to explain.</p><ol><li><p>src</p><img src="/posts/1998f291/index.js — amplify-johnny 2022-08-11 13-58-04.png" alt="index.js — amplify-johnny 2022-08-11 13-58-04" style="zoom:50%;"><ul><li>components (self-created)<ul><li>All Amplify-created pages are located under the “ui-components” folder. So, it’s better to create a new folder to put your custom page codes. Because after you type <code>amplify pull</code> command, Amplify will replace all pages under ui-components automatically for making sure all page codes are consistent with the UI module in Amplify Studio.</li></ul></li><li>contexts (self-created)<ul><li>For putting login and logout logic</li></ul></li><li>graphql (self-created)<ul><li>For putting all GraphQL queries codes</li></ul></li><li>models<ul><li>GraphQL model data</li></ul></li><li>ui-components (self-created)<ul><li>All page codes and are consistent with Amplify Studio</li><li>You can use <code>amplify pull</code> to update when you did some changes in Amplify Studio</li></ul></li><li>App.js<ul><li>The root code of React</li></ul></li><li>index.js<ul><li>React code</li></ul></li><li>aws-export.js<ul><li>Most important configuration file, don’t change this.</li><li>Contains configuration of auth, API, storage, etc.</li></ul></li></ul></li><li><p>amplify</p><img src="/posts/1998f291/aws-exports.js — amplify-johnny 2022-08-11 14-01-57.png" alt="aws-exports.js — amplify-johnny 2022-08-11 14-01-57" style="zoom:50%;"><p>In folder amplify, you only need to write your codes under function, and don’t change other folders.</p><p>Under folder function, it contains all Lambda functions you created by using the command <code>amplify add function</code>, and you need to choose the Lambda name you created, and write your codes in the index.py file if you choose the python platform for your Lambda.</p></li></ol><h3 id="Authentication-Configuration"><a href="#Authentication-Configuration" class="headerlink" title="Authentication Configuration"></a>Authentication Configuration</h3><p>For integrating with AWS Cognito, you not only need to configure by using the command <code>amplify add auth</code>, but also you need to add these codes into your App.js file for authentication configuration.</p><p>Because AWS Cognito User Pool has a redirect URL, it’s not convenient to use the same authentication configuration for the local development environment and remote test environment. It’s better to add two sets of authentication configurations. BTW, you need to pay attention to whether the authentication configuration belongs to the test environment or not when you push your codes to the remote branch for CI&#x2F;CD.</p><img src="/posts/1998f291/code.png" alt="code" style="zoom:40%;"><h2 id="Links-you-can-refer-to"><a href="#Links-you-can-refer-to" class="headerlink" title="Links you can refer to"></a>Links you can refer to</h2><ul><li><p>Amplify Immersion Day Workshop</p><p><a href="https://catalog.us-east-1.prod.workshops.aws/workshops/84db0afb-0279-4d29-ae26-1609043d5bfd/en-US">https://catalog.us-east-1.prod.workshops.aws/workshops/84db0afb-0279-4d29-ae26-1609043d5bfd/en-US</a></p></li><li><p>Amplify UI document</p><p><a href="https://ui.docs.amplify.aws/">https://ui.docs.amplify.aws/</a></p></li><li><p>Amplify Framework Documentation</p><p><a href="https://docs.amplify.aws/">https://docs.amplify.aws/</a></p></li><li><p>Amplify CLI</p><p><a href="https://docs.amplify.aws/cli/">https://docs.amplify.aws/cli/</a></p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/1998f291/amplify_banner.jpeg&quot; alt=&quot;amplify image&quot;&gt;&lt;/p&gt;
&lt;p&gt;This document is aimed to help you familiar with Amplify quickly. In addition, I’ll take you to go through the code structure and deployment architecture of my POC.&lt;/p&gt;
&lt;h2 id=&quot;What-is-Amplify&quot;&gt;&lt;a href=&quot;#What-is-Amplify&quot; class=&quot;headerlink&quot; title=&quot;What is Amplify&quot;&gt;&lt;/a&gt;What is Amplify&lt;/h2&gt;&lt;p&gt;Amplify is an all-in-one platform for frontend developers to develop, test, and deploy the whole website or mobile app. The frontend developers only need to know vanilla JavaScript or React or Vue.js, the knowledge of RestAPI, and database management is not necessary for this platform cause Amplify Studio can help frontend developers to maintain these through the UI.&lt;/p&gt;</summary>
    
    
    
    <category term="Front-end" scheme="https://liuninglin.github.io/categories/Front-end/"/>
    
    <category term="React" scheme="https://liuninglin.github.io/categories/Front-end/React/"/>
    
    
    <category term="AWS" scheme="https://liuninglin.github.io/tags/AWS/"/>
    
  </entry>
  
  <entry>
    <title>Using Github + Travis + Heroku + Slack for team development</title>
    <link href="https://liuninglin.github.io/posts/3d8e0aef.html"/>
    <id>https://liuninglin.github.io/posts/3d8e0aef.html</id>
    <published>2021-09-19T20:56:39.000Z</published>
    <updated>2022-10-01T20:38:34.748Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/3d8e0aef/image-20210919192821128.png" alt="image-20210919192821128"></p><p>Traditionally, using Gitlab + Jenkins + K8s + Docker may better for some e-commerce systems development. But you need to put more time to configure and maintain this setup. This is not easy and some unfortunate issues may comes frequently. </p><span id="more"></span><p>For small development team, espcially for Django development, we need some quick and easy CI&#x2F;CD to help the small team to get out of thee burdensome maintainance. </p><p>Here, I tried some online tools below to ahiceve this goal.</p><p>Django</p><ul><li>Quick development</li><li>Some basic function without configuration, out-of-box</li><li>Quick to learn</li></ul><p>Github</p><ul><li>Code Repository </li><li>Set a clear commit rule and branch management</li></ul><p>Travis</p><ul><li>Linting, testing and CI</li><li>Easy connect with Github</li></ul><p>Heroku</p><ul><li>CD</li><li>Easy connect with Github</li></ul><p>Slack</p><ul><li>Team Communication</li><li>Get notification after CI&#x2F;CD</li></ul><h3 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h3><p>​Github -&gt; Travis -&gt; Heroku -&gt; Slack</p><p>​After you push your code to the Github, Travis will automatically run CI process and test your codes. After that, Heroku can fetch your codes from a specific git branch for CD. Finally, the notification can send to Slack for notifying the result of CI&#x2F;CD.</p><h3 id="Travis-Configuration"><a href="#Travis-Configuration" class="headerlink" title="Travis Configuration"></a>Travis Configuration</h3><ol><li><p>Connect with Github (follow the steps in Travis)</p></li><li><p>Travis configuration file (.travis.yml)</p><ul><li>For specifying the development language, the commands for CI, and the script</li><li>Put this file in the root of your Django project</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">language:</span> <span class="string">python</span></span><br><span class="line"><span class="attr">python:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">3.6</span></span><br><span class="line"><span class="attr">install:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">pip</span> <span class="string">install</span> <span class="string">-r</span> <span class="string">requirements.txt</span></span><br><span class="line"><span class="attr">script:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">python</span> <span class="string">manage.py</span> <span class="string">test</span></span><br></pre></td></tr></table></figure></li><li><p>PIP install list file (requirements.txt)</p><ul><li>Put this file in the root of your Django project</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">django&gt;=2.0.11</span><br><span class="line">psycopg2</span><br><span class="line">psycopg2-binary</span><br><span class="line">dj-database-url==0.5.0</span><br><span class="line">gunicorn</span><br><span class="line">whitenoise</span><br><span class="line">django-heroku</span><br><span class="line">pytz</span><br><span class="line">sqlparse</span><br></pre></td></tr></table></figure></li></ol><h3 id="Heroku-Configuraion"><a href="#Heroku-Configuraion" class="headerlink" title="Heroku Configuraion"></a>Heroku Configuraion</h3><ol><li><p>Connect with Github (you can specify the git branch for code hook monitored)</p></li><li><p>Heroku configuration file (Procfile)</p><ul><li>For specifying the web server (Gunicorn)</li><li>Put this file in the root of your Django project</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">web:</span> <span class="string">gunicorn</span> <span class="string">webapps.wsgi</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="Some-possible-issues"><a href="#Some-possible-issues" class="headerlink" title="Some possible issues"></a>Some possible issues</h3><h4 id="Failed-to-deploy-for-collectstatic-directory"><a href="#Failed-to-deploy-for-collectstatic-directory" class="headerlink" title="Failed to deploy for collectstatic directory"></a>Failed to deploy for collectstatic directory</h4><p><img src="/posts/3d8e0aef/image-20210919190053772.png" alt="image-20210919190053772"></p><p>Two options for resolving </p><ol><li><p>Disable collectstatic</p><p>run heroku command:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">heroku config:set DISABLE_COLLECTSTATIC=1 </span><br></pre></td></tr></table></figure></li><li><p>Configure collectstatic in “settings.py”</p><ol><li>configure settings.py</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Static files (CSS, JavaScript, Images)</span></span><br><span class="line"><span class="comment"># https://docs.djangoproject.com/en/1.9/howto/static-files/</span></span><br><span class="line">STATIC_ROOT = os.path.join(BASE_DIR, <span class="string">&#x27;staticfiles&#x27;</span>)</span><br><span class="line">STATIC_URL = <span class="string">&#x27;/static/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Extra places for collectstatic to find static files.</span></span><br><span class="line">STATICFILES_DIRS = (</span><br><span class="line">    os.path.join(BASE_DIR, <span class="string">&#x27;static&#x27;</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ol start="2"><li>create static directory in the root of your project (not to create collectstatic folder, this will create automatically by heroku)</li></ol></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/3d8e0aef/image-20210919192821128.png&quot; alt=&quot;image-20210919192821128&quot;&gt;&lt;/p&gt;
&lt;p&gt;Traditionally, using Gitlab + Jenkins + K8s + Docker may better for some e-commerce systems development. But you need to put more time to configure and maintain this setup. This is not easy and some unfortunate issues may comes frequently. &lt;/p&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="Django" scheme="https://liuninglin.github.io/categories/Back-end/Django/"/>
    
    
    <category term="Python" scheme="https://liuninglin.github.io/tags/Python/"/>
    
    <category term="Django" scheme="https://liuninglin.github.io/tags/Django/"/>
    
    <category term="CI/CD" scheme="https://liuninglin.github.io/tags/CI-CD/"/>
    
  </entry>
  
  <entry>
    <title>Cheat Sheet for AWS SAA-02</title>
    <link href="https://liuninglin.github.io/posts/a60a7db0.html"/>
    <id>https://liuninglin.github.io/posts/a60a7db0.html</id>
    <published>2021-05-21T16:36:05.000Z</published>
    <updated>2022-10-01T20:38:34.567Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/a60a7db0/image-20210919194848724.png" alt="image-20210919194848724"></p><h2 id="IAM"><a href="#IAM" class="headerlink" title="IAM"></a>IAM</h2><ol><li>IAM role policy only define <strong>which API actions</strong> can be made to that role.</li></ol><span id="more"></span><p>##VPC</p><ol><li><p>each account can create 5 VPC, and each vpc can create 200 subnets</p></li><li><p>private subnet &#x3D;&gt; NAT Gateway &#x3D;&gt; IGW</p></li><li><p>Direct Connect can provide 1Gbps to 10Gbps private network, but not encryption.</p></li><li><p>For accessing applications in different regions privately, you can configure inter-region VPC peering and create a VPC endpoint for specific service or application</p></li><li><p>NACLs &amp; SG</p><ul><li><strong>By default, new created SG only allow all connection from the outbound. New created NACLs deny both inbound and outbound connection</strong></li><li><strong>However, default NACLs is configured to allow all traffic to flow in and out of the subnets to which it is associated.</strong></li><li>NACLs<ul><li>The lower number rule has precedence</li><li>If a request comes into a web server in your VPC from a computer on the internet, your network ACL must have an outbound rule to enable traffic destined for ports 49152-65535 (ephemeral ports)</li></ul></li><li>SGs<ul><li>Inbound Rules: by default, disallow all traffic</li><li>Outbound Rules: by default, allow all traffic</li></ul></li></ul></li><li><p>VPC Endpoint</p><ul><li>VPC Endpoint Policy<ul><li>VPC has a policy which by default allows all actions on all S3 buckets. We can restrict access to certainn S3 buckets and certain actions on this policy. In such cases, for accessing any new buckets or for any new actions, the VPC endpoint policy needs to be modified accordingly.</li></ul></li><li>VPC Endpoints does not supported outside VPC</li><li>Two types<ul><li>Interface Endpoint<ul><li>is an elastic network interface (ENI) with a private IP address</li></ul></li><li>Gateway Endpoint<ul><li>Currently supports S3 and DynamoDB</li></ul></li></ul></li></ul></li><li><p>VPN</p><ul><li>Site-to-Site VPN provide encryption connection through Internet, not private network.</li><li>Policy-based VPNs using <strong>one or more</strong> pairs of security associations drop already existing connections when new connection requests are generated with different security associations. This cause intermittent packet loss and other connectivity failures.</li><li>Using Route-Based VPNs can get rid of connectivity issues of Policy-Based VPNs</li><li>CloudHub<ul><li><strong>To use AWS VPN CloudHub, one must create a Virtual Private Gateway with multiple Customer Gateways, each with a unique Border Gateway Protocol (BGP) Autonomous System Number (ASN)</strong></li><li><strong>VPN CloudHub operates on a simple hub-and-spoke model that you can use with or without a VPC.</strong> This design is good for primary or backup connectivity between remote offices.</li><li>The sites must not have overlapping IP ranges</li><li>Each Customer Gateway must attach a public IP address. You must use a unique Border Gateway Protocol (BGP) Autonomous System Number (ASN) for each Customer Gateway.</li><li>VGW by default acts as a Hub and spoke &amp; no additional configuration needs to be done at the VGW end.</li><li>Each router in each spoke needs to have BGP peering only with VGW &amp; not with routers in other locations.</li><li><strong>This design is suitable if you have multiple branch offices and existing internet connections and would like to implement a convinient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices</strong></li></ul></li><li>Process for creating Site-to-Site VPN<ul><li>Specify the type of routing that you plan to use (static or dynamic)</li><li>Update the route table for your subnet</li></ul></li><li>Static and Dynamic Routing<ul><li>the type of routing that you select can depend on the make and model of your customer gateway device.</li><li>If your customer gateway device support Border Gateway Protocol (BGP), specify dynamic routing. If not, specify static.</li></ul></li><li><strong>Reducing backup time of large data size by using VPN</strong><ul><li><strong>Enable ECMP on on-premises devices to forward traffic on both VPN endpoints</strong></li><li>ECMP (Equal Cost Multi-Path) can be used to carry traffic on both VPN endpoints, increasing performance and faster data transfer.</li><li>ECMP needs to be enabled on Client end devices and not on the VGW end.</li></ul></li><li>Customization<ul><li><strong>AWS Site-to-Site VPN offers cutomizable tunnel options including inside tunnel IP address, pre-shared key, and Border Gateway Protocol Autonomous System Number (BGP ASN). In this way, you can set up multiple secure VPN tunnels to increase the bandwidth for your applications or for resiliency in case of a down time. In addition, equal-cost multi-path routing (ECMP) is available with AWS Site-to-Site VPN on AWS Transit Gateway to help increase the traffic bandwidth over multiple paths.</strong></li></ul></li></ul></li><li><p>LAGs</p><ul><li>LAG stands for Link Aggregation Groups</li><li>You can use multiple connections for redundancy.</li><li>A LAG is a logical interface that uses the Link Aggregation Control Protocol (LACP) to aggregate multiple connections at a single AWS Direct Connect endpoint, allowing you to treat them as a single, managed connection.</li><li><img src="/posts/a60a7db0/IMG_410A544D389B-1.jpeg" alt="IMG_410A544D389B-1"></li><li><strong>For higher throughput, LAG can aggregate multiple DX connections to give a maximum of 50 Gig bandwidth.</strong></li></ul></li><li><p>A NAT Gateway cannot send traffic over VPC endpoints, VPN connections, AWS Direct Connect, or VPC Peering connections.</p></li><li><p>You can associate secondary IPv4 CIDR blocks with your VPC. When you associate a CIDR block with your VPC, a route is automatically added to your VPC route tables to enable routing within the VPC (the destination is the CIDR block ad the target is local).</p></li><li><p>A subnet’s CIDR cannot be edited once created</p></li><li><p>Route Tables Target Naming</p><ul><li>VPC Peering: pcx-xxxx</li><li>VPN: vgw-xxxx</li><li>Direct Connect: vgw-xxxx</li></ul></li><li><p>VPC Peering</p><ul><li><strong>For the private connections between regions, VPC peering should be used. Then VPC endpoint allows users to access the DynamoDB service privately.</strong></li><li>doesn’t support transitive routing</li></ul></li><li><p><strong>Visting websites that belongs to the same region, the latency will be almost same.</strong></p></li><li><p>Transit Gateway</p><ul><li>AWS Transit Gateway centralize outbound internet traffic from multiple VPCs using hub-and-spoke design.</li></ul></li><li><p>Hybrid Option: Direct Connect + VPN</p><ul><li>VPN is needed as it creats an IPsec connection. Direct Connect is also required because it establishes a private connection with high bandwidth throughput.</li><li>With Direct Connect + VPN, you can create IPsec-encrypted private connection.</li></ul></li><li><p><strong>For resources can be accessed from Internet</strong></p><ul><li><p>Need an IGW</p></li><li><p>The route table needs to be attched to an IGW that is created for the VPC</p></li></ul></li></ol><p>##EC2</p><ol><li>Instance Type<ul><li>R: more ram</li><li>C: more CPU</li><li>M: balanced type</li><li>I: more I&#x2F;O</li><li>G: more GPU</li></ul></li><li>Instance type offers different compute, memory, and storage capabilities, and is grouped in an instance family based on these capabilities.</li><li>Launch Mode<ul><li>On-Demand</li><li>Spot Instance</li><li>Reserved</li><li>Dedicated</li></ul></li><li>Placement Group<ul><li>Types<ul><li>Cluster (better for HPC)<ul><li><strong>instances are all in one AZ</strong></li><li><strong>cluster cannot be multi-AZ</strong></li><li>cluster is not available in t2.micro</li><li>for HPC</li><li>all the instances are placed in the same rack in the same AZ</li></ul></li><li>Spread (better for resolving simultaneous failures)<ul><li><strong>can be multi-AZ</strong></li><li><strong>cannot span across multiple regions</strong></li><li><strong>supports a maximum of 7 running instances per AZ</strong></li><li>appropriate for availability scenarios</li><li>the insatnces in different racks. Every rack has its own hardware and power source.</li></ul></li><li>Partition (better for Big Data)<ul><li><strong>within one AZ</strong></li><li><strong>each partition do not share the underlying hardware with each other</strong></li></ul></li></ul></li><li>Placement Group supports migrating instances between palcement groups, but not merging them</li><li>Placement Group cannot span multiple regions</li></ul></li><li><strong>EC2 Hibernate</strong><ul><li><strong>Pre-warm EC2 instance</strong></li><li><strong>The instance needs to be launched with an EBS root volume</strong></li><li><strong>Note: You cannot hibernate an instance in an ASG or used by ECS</strong></li><li><strong>The instance retains its private IPv4 addresses and any IPv6 addresses when hibernated and started.</strong></li><li><strong>When an EC2 instance is in the Hibernate state, you pay only for the EBS volumes and Elastic IP addresses attached to it.</strong></li></ul></li><li>Metadata<ul><li><a href="http://169.254.169.254/latest/meta-data">http://169.254.169.254/latest/meta-data</a></li></ul></li></ol><h2 id="ASG"><a href="#ASG" class="headerlink" title="ASG"></a>ASG</h2><ol><li><p>ASG launches new instances based on the configuration defined in Launch Configuration</p></li><li><p>AMI ID is set during the creation of launch configuration and cannot be modified.</p></li><li><p>Using Auto Scaling is good for</p><ul><li>Better fault tolerance</li><li>Better availability</li></ul></li><li><p>Default metric type for Simple Policy</p><ul><li>ALB Request Count Per Target</li><li>Average Network In</li><li>Average Network Out</li><li><del>Memory Utilization</del></li></ul></li><li><p>Default metric type for Step Policy</p><ul><li>CPU Utilization</li><li>Disk Reads</li><li>Disk Read Operations</li><li>Disk Writes</li><li>Disk Write Operations</li><li>Network In</li><li>Network Out</li></ul></li><li><p>ASG Scaling Policies</p><ul><li>Target Tracking Scaling Policy<ul><li>Maintains a specific metric at a target value</li><li>ex: want average CPU to stay at 40%</li></ul></li><li>Simple Scaling Policy (based on a single adjustment)<ul><li>Scales when an alarm is breached</li><li>ex: when a CloudWatch alarm is triggered (CPU &gt; 70%), then add 2 units</li></ul></li><li>Scaling Policies with Steps (based on step adjustments)<ul><li>Scales when an alarm is breached, can escalates based on alarm value changing </li><li>Main difference between Simple Scaling Policy and Step Simple Scaling Policy is the step adjustments</li><li>The adjustments vary based on the size of the alarm breach</li><li>ASG react to the lower and upper bound metrics value</li><li><strong>AWS recommends Step Scaling Policies as a better choice than simple scaling polices.</strong></li></ul></li><li>Scheduled Actions</li></ul></li><li><p><strong>You can have multiple scaling policies in force at the same time</strong></p><p>ex: multiple target tracking scaling policies for an ASG, provided that each of them uses a different metric.</p></li><li><p><strong>If two policies are executed at the same time, EC2 Auto Scaling follows the policy with the greater impact. For example, if you have one policy to add two instances and another policy to add four instances, EC2 Auto Scaling adds four instances when both policies are triggered simultaneously.</strong></p></li><li><p>Termination Policy</p><ul><li>OldestInstance<ul><li>Terminate the oldest instance in the group</li></ul></li><li>NewestInstance<ul><li>Terminate the newest instance in the group</li></ul></li><li>OldestLaunchConfiguration<ul><li>Terminate instances that have the oldest launch configuration</li></ul></li><li>ClosestToNextInstanceHour<ul><li>Terminate instances that are closest to the next billing hour</li></ul></li><li><strong>Default</strong><ul><li>Find the AZ which has the most number of instances</li><li>If there are multiple instances in the AZ, delete the one with the oldest launch configuration</li><li>If there are multiple instances, choose the instance which are closest to the next billing hour</li><li>If there are multiple instances, select one of them at <strong>random</strong></li></ul></li></ul></li><li><p><strong>ASG tries the balance thee number of instances across AZ by default</strong></p></li><li><p><strong>Lifecycle Hooks</strong></p><ul><li>By default as soon as an instance is lanched in an ASG it’s in service</li><li>You have the ability to perform extra steps before the instance goes in service (Pending State)</li><li>You have the ability to perform some actions before the instance is terminated (Terminating State)</li><li><img src="/posts/a60a7db0/IMG_0994.jpg" alt="IMG_0994"></li></ul></li><li><p><strong>Salcing Cooldown</strong></p></li></ol><ul><li>Cooldown period helps to ensure that your ASG doesn’t luanch or terminate additional instances before the previous scaling activity takes effect</li><li>We can create cooldowns that apply to a specific scaling policy</li></ul><ol start="13"><li><p><strong>IAM roles attached to an ASG will get assigned to EC2 instances</strong></p></li><li><p><strong>ASG is free, but the underlying resources are not free</strong></p></li><li><p>better scaling rules that are directly managed by EC2</p><ul><li>Target Average CPU Usage</li><li>Number of requests on the ELB per instance</li><li>Average Network In</li><li>Average Network Out</li></ul></li><li><p>By default, Amazon EC2 Auto Scaling health checks use the results of the EC2 status checks to determine the health status of an instance</p></li><li><p><strong>Health Check Grace Period</strong></p><ul><li>AS instance that has just come into service needs to warm up before it can pass the health check.</li><li>EC2 AG waits until the health check grace period ends before checking the health status of the instance</li></ul></li><li><p>ASG supports a mix of On-Demand &amp; Spot instances, which helps design a cost-optimized solution without impacting the performance. You can choose the percentage of On-Demand &amp; Spot instances based on the application requirement (OnDemandPercentageAboveBaseCapacity)</p></li></ol><p>##EBS</p><ol><li>Storage Type<ul><li>gp2: <ul><li><strong>general purpose, max 16,000 IOPS, max 250 MB&#x2F;s throughput</strong></li><li>IOPS is increased with volume size</li></ul></li><li>io1: <ul><li><strong>high IOPS, max 64,000 IOPS, max 1,000 MB&#x2F;s throughput</strong></li><li>IOPS is not increased with volume size</li></ul></li><li>st1: <ul><li><strong>high throughput, HDD, max 500 IOPSmax 500MB&#x2F;s throughput</strong></li></ul></li><li>sc1: <ul><li><strong>lowest price, HDD, max 250 IOPS, max250MB&#x2F;s throughput</strong></li></ul></li></ul></li><li>EBS Snapshot Lifecycle<ul><li>You cann use Amazon Data Lifecycle Management (DLM) to automate the creation, retention, and deletion of snapshots taken to back up your EBS volumes.</li><li>Automating snapshot management helps you to<ul><li>Protect valuable data by enforcing a regular backup schedule</li><li>Retain backups as required by auditors or internal compliance</li><li>Reduce storage costs by deleting outdated backups</li></ul></li></ul></li><li><strong>You can back up the data on your EBS volumes to S3 by taking point-in-time snapshots. Snapshots are incremental backups, which means that only the blocks on the device that have changed after you most recent snapshot are saved.</strong> This minimizes the time required to create the snapshot and saves on storage costs by not duplicating data. When you delete a snapshot, only the data unique to that snapshot is removed. <strong>Each snapshot contains all of the information needed to restore your data (from the moment when the snapshot was taken) to a new EBS volume.</strong></li><li>EBS Performance Tips<ul><li>EBS-optimized instance<ul><li>EBS-optimized instance uses an optimized configuration stack and provides additional, dedicated capacity for EBS I&#x2F;O. This optimization will minimize contention between EBS I&#x2F;O and other traffic from your instance.</li></ul></li><li>Use a Modern Linux Kernel</li><li>Use RAID 0 to Maximize Utilizationn of Instance Resource<ul><li>You can join multiple gp2, io1, st1, or sc1 volumes together in a RAID 0 configuration to use available bandwidth for these instances.</li></ul></li></ul></li><li>EBS Encryption<ul><li>Ensuring the security of both data-at-rest and data-in-transit between an instance and its attached EBS storage.</li><li>You can enable encryption while copying a snapshot from an unencrypted snapshot</li><li>You cannot remove encryption from an encrypted snapshot</li><li>You cannot create an encrypted snapshot from an uncrypted volume</li></ul></li><li>CloudWatch Metrics for EBS<ul><li>VolumeReadBytes &#x2F; VolumeWriteBytes</li><li>VolumeReadOps &#x2F; VolumeWriteOps</li><li>VolumeTotalReadTime &#x2F; VolumeTotalWriteTime</li><li>VolumeIdleTime</li><li>VolumeQueueLength</li><li>VolumeThroughputPercentage</li><li>VolumeConsumedReadWriteOps</li><li>BurstBalance</li><li><del>VolumeRemainingSize</del></li></ul></li><li>EBS Elastic Volumes<ul><li>With EBS Elastic Volumes, you can increase the volume size, change the volume type, or adjust the performance of your EBS volumes. If you instance supports Elastic Volumes, you can do so without detaching the volume or restarting the instance.</li></ul></li></ol><h2 id="Instance-Store"><a href="#Instance-Store" class="headerlink" title="Instance Store"></a>Instance Store</h2><ol><li>Data in the instance store is lost under any of the following circumstances<ul><li>The underlying disk drive fails</li><li>The instance stops</li><li>The instance terminates</li></ul></li><li>Data will not lost during reboot</li><li>You can only specify the size of your instance store when you launch an instance. You can’t change it or attach new after you’ve launched it.</li></ol><h2 id="EFS"><a href="#EFS" class="headerlink" title="EFS"></a>EFS</h2><ol><li>EFS works with EC2 instances in multi-AZ</li><li>Uses SG to control access to EFS</li><li><strong>Performance Mode</strong> (set at EFS creation time)<ul><li>General purpose (default): latency-sensitive use cases (web server, CMS)</li><li>Max I&#x2F;O - higher latency, throughput, highly parallel (big data, media proessingn)</li></ul></li><li><strong>The performance mode of an EFS cannot be changed after the file system has been created</strong></li><li>Throughput Mode<ul><li>Bursting Throughput: With Bursting Throughput mode, a file system’s throughput scales as the amount of data stored in the EFS standard or one zone storage class grows.</li><li>Provisioned Throughput: Provisioned throughput is available for applications with high throughput to storage</li></ul></li><li>Storage Class (life cycle management feature - move files after N days)<ul><li>Standard: for frequently accessed files</li><li>Infrequent access (EFS-IA): cost to retrieve files, lower price to store</li></ul></li><li>Can leverage EFS-IA for cost saving</li><li>Note: An EFS file system can only have mount targets in one VPC at a time</li><li>When you use a VPC peering connection or VPC Transit Gateway to connect VPCs, EC2 instances in one VPC can access EFS in another VPC, even if the VPCs belong to different accounts.</li><li>Mount targets can have associated SGs</li><li>And SG on mount targets need inbound rules for allowing TCP 2049 from SG of EC2</li><li>Encryption of data at rest can only be enabled during file system creation</li><li>NFS is nont an encrypted protocol</li><li>When you need encryption in transit, you can use Amazon EFS mount helper during mounting</li><li>Diff between EFS and EBS<ul><li>Availability and durability<ul><li>EFS<ul><li>Data is stored redundantly across multiple AZs</li></ul></li><li>EBS<ul><li>Data is stored redundantly in a single AZ</li></ul></li></ul></li><li>Acess<ul><li>EFS<ul><li>thounds of EC2 instances from multi-AZs can connect</li></ul></li><li>EBS<ul><li>single EC2 instance in a single AZ can connect</li></ul></li></ul></li><li>Use Cases<ul><li>EFS<ul><li>Big data and analytics, media processing, content management, web serving</li></ul></li><li>EBS<ul><li>Boot volumes, transcational and NoSQL databases, data warehousing, and ETL</li></ul></li></ul></li></ul></li><li>You can mount EFS over VPC connections by using VPC peering <strong>within a single AWS Region</strong>, <strong>not support inter-region</strong> VPC peering.</li><li>Mount Target<ul><li>You can create one mount target in each AZ (recommended way)</li><li>If the VPC has multiple subets in an AZ, you can create a mount target in only one of those subnets. All EC2 instances in the AZ can share the single mount target</li></ul></li></ol><h2 id="Serverless"><a href="#Serverless" class="headerlink" title="Serverless"></a>Serverless</h2><ul><li>S3</li><li>Athena</li><li>DynamoDB</li><li>Lambda</li><li>SNS, SQS</li><li>Aurora Serverless</li><li>API Gateway</li></ul><p>##S3</p><ol><li><p>4 9’s avaialbility, and 11 9’s durability</p></li><li><p>S3 is a managed service and not part of VPC. So, VPC flow logs does not report traffic sent to the S3 bucket.</p></li><li><p>Storage Class</p><ul><li>Standard</li><li>Standard-IA<ul><li>DR, backup</li><li>access less than a month, additional retrieve fee needs</li></ul></li><li>One Zone-IA<ul><li>only exist in one AZ, additionnal retrieve fee needs</li><li>secondary backups for on-premise data</li></ul></li><li>Intelligent Tiering<ul><li>using ML to analyse your usage and determine the appropriate storage class</li><li>When the access pattern to web application using S3 storage buckets is unpredictable, using intelligent tier.</li><li>Intelligent Tiering storage class includes two access tiers<ul><li>frequent access</li><li>infrequent access</li></ul></li><li>Intelligent Tiering storage class has the same performance as that of Standard storage class</li></ul></li><li>Glacier<ul><li>for long-term cold storage</li></ul></li><li>Glacier Deep Archive<ul><li>the lowest cost storage class.</li><li>Data retrieval time is 12 hours</li></ul></li></ul></li><li><p>Strong Consistency</p><ul><li><strong>What you write is what you will read, and the results of a LIST will be an accurate reflection of what’s in the bucket.</strong> This applies to all existing and new S3 objects, works in all regions, and is available to<br>you at no extra charge!</li></ul></li><li><p>CRR &amp; SRR</p><ul><li><strong>must turn on “versioning” on source and destination bucket</strong></li><li>replication cannot be chaining</li><li>only replicate new objects, do not be retroactive</li><li><strong>CRR can copy encrypted objects across buckets in different regions</strong></li><li>Users can choose one or more KMS keys in the replication rule.</li><li>re-encryption is not required for the CRR</li></ul></li><li><p>Lifecycle Management</p><ul><li>help to move objects to different storage class or delete objects in time</li><li>Actions for Lifecycle Management<ul><li>Transition actions</li><li>Expiration actions</li></ul></li></ul></li><li><p>Performance</p><ul><li>if you use KMS, your S3’s performance may impacted by KMS</li></ul></li><li><p>How to improve performance</p><ul><li>Upload<ul><li>Multi-Part Uploads</li><li>S3 Transfer Acceleration (compatible with Multi-Part Upload)<ul><li>using edge location to speed up</li></ul></li></ul></li><li>Download<ul><li>S3 “Byte-Range” HTTP header in a GET request to download the specified range bytes of an object</li></ul></li></ul></li><li><p><strong>S3 Select &amp; Glacier Select</strong></p><ul><li>Retrieve specific data <strong>using SQL</strong> by performing server side filtering</li><li>S3 Glacier Select can directly query data from S3 Glacier &amp; restoration of data to the S3 bucket is not required for querying this data.</li><li>For using S3 Select, objects need to be <strong>stored in an S3 bucket with CSV, JSON, or Apache Parquet format</strong>. <strong>GZIP &amp; BZIP2 compression is supported with CSV or JSON format with server-side encryption</strong>.</li></ul></li><li><p>Event Notification</p></li></ol><ul><li><strong>3 targets for events: SNS, SQS, Lambda</strong></li><li>If you don’t want miss any notifications, you need to enable versioning</li></ul><ol start="11"><li><p>Make a static website</p></li><li><p>Properties choose “use this bucket to a static website”</p></li><li><p>Permission “Public Access Settings”</p></li><li><p>Permission “Bucket Policy”</p></li><li><p>Bucket Policy</p></li></ol><ul><li>If a bucket policy contains Effect as Deny. You must whitelist all the IAM resources which need access on the bucket. Otherwise, IAM resources cannot access the S3 bucket even if they have full access.</li></ul><ol start="13"><li>Server Access Logging</li></ol><ul><li>Server Access Logging provides detailed records for the requsets that are made to a bucket. Can be useful in security and access audits</li></ul><ol start="14"><li>Metadata</li></ol><ul><li>System Metadata<ul><li>such as object creation date is system controlled where only S3 can modify the value</li></ul></li><li>User-Defined Metadata<ul><li>When uploading an object, you can also assign metadata to the object. You provide this optional information as a name-value pair when you send PUT or POST request to create the object. When you upload objects using the REST API, the optional user-defined metadata names must begin with <strong>“x-amz-meta-“</strong> to distinguish them from other HTTP headers.</li></ul></li></ul><ol start="15"><li>URLs for accessing objects</li></ol><ul><li>Virtual Hosted-Style Requests<ul><li><a href="https://bucket-name.s3.region.amazonaws.com/key">https://bucket-name.s3.Region.amazonaws.com/key</a> name</li></ul></li><li>Path-Style Requests<ul><li><a href="https://s3.region.amazonaws.com/bucket-name/key">https://s3.Region.amazonaws.com/bucket-name/key</a> name</li></ul></li></ul><ol start="16"><li><p><strong>Old version of an existing object is also be charged by AWS.</strong></p></li><li><p>Types of SSE (at-rest)</p></li></ol><ul><li>SSE-S3</li><li>SSE-KMS</li><li>SSE-C</li></ul><ol start="18"><li><p><strong>S3 bucket owner can create Pre-Signed URLs to upload images to S3</strong></p></li><li><p><strong>Object ACLs</strong></p></li></ol><ul><li><p>Object level, not bucket level</p></li><li><p>Using Object ACLs provides a granular control on each file in S3 bucket</p></li></ul><ol start="20"><li>S3 Select vs Athena</li></ol><ul><li>S3 Select only support simple SELECT statement, no joins or subqueries</li><li>Athena supports full standard SQL</li></ul><ol start="21"><li><p><strong>With version enabled S3 buckets, each version of an object can have a different retention period</strong></p></li><li><p>S3 CORS</p></li></ol><ul><li><strong>what is different origin</strong><ul><li><strong>different doamin or subdomain</strong></li><li><strong>different protocol or different ports</strong></li></ul></li><li>the limit number of CORS is 100</li><li><strong>The scheme, the hostname, and the port values in the Origin request header must match the AllowedOrigin elements in the CORSRule.</strong></li><li><strong>For example, if you set the CORSRule to allow the origin “<a href="http://www.example.com&quot;/">http://www.example.com&quot;</a>, then both “<a href="https://www.example.com&quot;/">https://www.example.com&quot;</a> and “<a href="http://www.example.com:80&quot;">http://www.example.com:80&quot;</a> origins in your request don’t match the allowed origin in your configuration. By the way, “<a href="http://www.example.com&quot;/">www.example.com&quot;</a> and “example.com” are not the same hostname</strong></li><li>CORS configuration can use JSON or XML format.</li><li>CORS only supports GET, PUT, POST, DELETE, and HEAD.</li></ul><ol start="23"><li><p>S3 bucket policy should allow the “S3:GetObject” action if the Principal comes from the CloudFront Origin Access Identity.</p></li><li><p>Bucket Policy for OAI</p></li></ol><p>   <img src="/posts/a60a7db0/IMG_981A4CE6475E-1.jpeg" alt="IMG_981A4CE6475E-1"></p><ol start="25"><li>Object Lock<ul><li>Object Lock should be enabled to store objects using write once and read many (WORM) models.</li><li>You can prevent the S3 objects from being deleted or overwritten for a fixed amount of time or indefinitely.</li><li>Note: Versioning does not prevent objects from being deleted or modified.</li></ul></li><li>Event Notification<ul><li>you can use Event Notification from the S3 bucket to invoke the Lambda fuction whenever the file is uploaded.</li></ul></li></ol><h2 id="S3-Glacier"><a href="#S3-Glacier" class="headerlink" title="S3 Glacier"></a>S3 Glacier</h2><ol><li><p>You cannot directly upload files to Glacier through S3 console</p></li><li><p>Retrieval</p><ul><li>Expedited Retrieval<ul><li>Expedited retrievals allow you to access data in 1-5 mins.</li><li>Expedited retrievals allow you to quickly access your data when occassional urgent requests for a subnet of archives are required</li></ul></li><li>Standard Retrieval<ul><li>need 3-5 hours</li></ul></li><li>Bulk Retrieval<ul><li>need 5-12 hours</li></ul></li></ul></li><li><p>Vault Lock</p><ul><li>S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual S3 Glacier vaults with a vault lock policy.</li><li>You can specify controls such as “write once read many” (WORM) in a vault lock policy and lock the policy from the future edits.</li><li>Once locked, the policy can no longer be changed.</li></ul></li><li><p>Objects in Glacier Deep Archive</p><ul><li>You cannot directly move objects to another storage class. These need to be restored first &amp; then copied to the disired storage class.</li><li>S3 Glacier console can be used to access vaults and objects in them. But is cannot be used to restore the objects.</li></ul></li></ol><h2 id="Athena"><a href="#Athena" class="headerlink" title="Athena"></a>Athena</h2><ol><li><strong>charged per query and amount of data scanned</strong></li><li>for SSE-KMS, Athena can determine the proper materials to decrypt the dataset when creating the table. You do not need to provide the key information to Athena.</li><li><strong>Athena can create the table for the S3 data encryption by SSE-KMS</strong></li><li>Workgroup<ul><li>A separate Workgroup can be created based upon users, teams, applications or workloads. This will <strong>minimize the amount of data scanned for each query, improve performance and reducing cost</strong>.</li><li>Using Workgroup to isolate queries for teams, applications, or different workloads.</li></ul></li></ol><p>##Route 53</p><ol><li><p><strong>is a global service, not regional</strong></p></li><li><p>is highly available and scalable DNS web service. You can use Route 53 perform three main functions</p><ul><li>Register domain names</li><li>Route internet traffic to the resources for your domain</li><li>Check the health of your resources</li></ul></li><li><p><strong>Route 53 is not used for load-balancing traffic among individual resource instances</strong></p></li><li><p>Record Type</p><ul><li>A: convert to an IPv4 address</li><li>AAAA: convert to an IPv6 address</li><li>CNAME: convert to another hostname<ul><li><strong>only for non root domain</strong></li></ul></li><li>Alias: convert to a specific AWS resource<ul><li><strong>works for root domains and subdomains</strong></li><li><strong>free of charge</strong></li><li>native health check</li></ul></li></ul></li><li><p>Alias Records for your domain and subdomain</p><ul><li>Instead of using IP addresses, the alias records use S3 website endpoints.</li><li><strong>S3 maintains a mapping between the alias records and the IP addresses where S3 buckets reside.</strong></li><li><strong>A record set can only have one Alias Target</strong></li></ul></li><li><p>DNS TTL</p><ul><li>it enables the client to cache the response of a DNS query</li></ul></li><li><p>Routing Policy</p><ul><li>Simple Routing Policy<ul><li>if you set multiple values, it will return a random one</li><li>using command(dig) will find multiple values returned</li><li>can’t attach a Health Check</li></ul></li><li>Weighted Routing Policy<ul><li>only see one value returned, not multiple values</li><li>can attach a Health Check</li></ul></li><li>Latency Routing Policy</li><li>Failover Routig Policy<ul><li>must attach a Health Check</li><li>Choose one for primary, and another for secondary</li></ul></li><li>GeoLocation Routing Policy</li><li>Multi Value Routing Policy<ul><li>Multi Value almost like Simple Policy, the only diff is the Healch Check</li></ul></li></ul></li><li><p><strong>Traffic can route to the following services</strong></p><ul><li>CloudFront</li><li>EC2</li><li>Beanstalk</li><li>ELB</li><li>RDS</li><li>S3</li><li>WorkMail</li></ul></li><li><p>Reasons for displaying “Server not found” error</p><ul><li>You didn’t create a record for the domain or subdomain name</li><li>You created a record but specified the wrong value</li><li>The resource that you’re routing traffic to is unavailable</li></ul></li><li><p>Logging and Monitoring Route 53</p></li></ol><ul><li><p>Monitoring Health Checks using CloudWatch</p><ul><li>By default, metric data for Route 53 health checks is automatically sent to CloudWatch at 1m intervals</li></ul></li><li><p>Monitoring Domain Registrations, including</p><ul><li>Status of new domain registrations</li><li>Status of domain transfers to Route 53</li><li>List of domains that are approaching the expiration date</li></ul></li><li><p>Logging Route 53 API calls with CloudTrail</p></li></ul><ol start="11"><li><p>Types of Route 53 Health Checks</p><ul><li>Health Checks that monitor an endpoint</li><li>Health Checks that monitor other health checks</li><li>Health Checks that monitor CloudWatch alarms</li></ul></li><li><p><strong>There is no interface endpoint for Route 53</strong></p></li><li><p><strong>Route 53 is not inside the AWS backbone</strong></p></li></ol><p>##ELB</p><ol><li><p>An ELB must have at least two AZs, and <strong>ELB can’t cross region</strong></p></li><li><p>Types of ELB</p><ul><li>ALB<ul><li>Layer 7</li><li>WAF can be attached to ALB</li><li><strong>SG can attach to it</strong></li><li>ALB do not have the spot type</li><li>Target Groups:<ul><li>EC2 instance</li><li>ECS tasks</li><li>Lambda functions</li><li>IP addresses</li></ul></li><li>ALB can route to multiple target groups</li><li>The application must check X-Forward-For in HTTP request header for requiring IP address of users</li><li><strong>ALB does not charge users based on the number of enabled AZs</strong></li><li><strong>Support dynamic mapping</strong></li></ul></li><li>NLB<ul><li>Layer 4 (Transport)</li><li><strong>SG cannot attach to it</strong></li><li><strong>Support dynamic mapping</strong></li></ul></li><li>Gateway LB<ul><li>Layer 3 (Network)</li></ul></li><li>CLB (legacy)<ul><li>It does not support dynamic mapping</li></ul></li></ul></li><li><p>Troubleshooting</p><ul><li>LB shows 503 means there is no registered target</li><li>if LB cannot connect to your application, please check SG</li></ul></li><li><p>Load Balancer Stickiness</p><ul><li>Works for ALB and CLB</li><li>Use case: make sure users don’t lose their session data</li></ul></li><li><p>Cross Load Balancing</p><ul><li>ALB<ul><li><strong>Always on</strong></li><li>no charge for inter AZ data</li></ul></li><li>NLB<ul><li>default off</li><li>you pay inter AZ data if enable</li></ul></li><li>CLB<ul><li><strong>no charge for inter AZ data if enable</strong></li></ul></li></ul></li><li><p>SSL&#x2F;TLS</p><ul><li>using SNI to resolve multiple SSL certificates onto one web server</li><li>only works for ALB &amp; NLB, CloudFront</li><li>for CLB, must use multiple CLB for different hostnames</li></ul></li><li><p>TLS listeners</p><ul><li><p>To use a TLS listener, you must deploy at least one server certificate on your load balancer. The load balancer uses a server certificate to terminate the front-end connection and then to decrypt requests from clients before sending them to the targets</p></li><li><p>ELB uses a TLS negotiation configuration, known as security policy, to negotiate TLS connections between a client and the load balancer.</p></li><li><p><strong>A security group is a combination of protocols and ciphers.</strong></p></li><li><p><strong>The protocol establishes a secure connection between a client and a server and ensures that all data passed between the client and your load balancer is private.</strong></p></li><li><p>A cipher is an encryption algorithm that uses encryption keys to create a coded message.</p></li><li><p><strong>NLB does not support a custom security policy</strong></p></li><li><p><strong>NLB requires one certificate per TLS connection to encrypt traffic between client &amp; NLB annd forward decrypted traffic to target servers. Using AWS Certificate Manager is a preferred option, as these certificates are automatically renewed on expiry</strong></p></li></ul></li><li><p>Connection Draining</p><ul><li>when existing connection shows unhealthy, the users must wait the response, and this period means draining mode. The new requests from other users will</li><li>redirect to other targets.</li><li>If you set draining value is 0, it means the connection will be droped, and the user will receive an error from ELB</li><li>CLB: names Connection Draining</li><li>ALB &amp; NLB: in Target Group and names Deregistration Delay</li></ul></li><li><p>ELB rules of Traffic</p></li></ol><ul><li>Listener: incoming traffic is evaluated by ports</li><li>Rules: listener then will invoke rules to decide what to do with the traffic</li><li>Target Groups</li></ul><ol start="10"><li>Health Check</li></ol><ul><li>ELB doesn’t terminate unhealthy instances, it just redirect traffic to the healthy one</li><li>for NLB and ALB, Health Checks locate in Target Group</li></ul><ol start="11"><li>Monitor ALB</li></ol><ul><li>CloudWatch metrics</li><li>Access logs</li><li>Request tracing</li><li>CloudTrail logs</li></ul><ol start="12"><li><p>Reasons for connection failure of Internet-facing load balancer</p><ul><li>Your internet-facing load balancer is attached to a private subnet</li><li>A SG or NACL does not allow traffic</li></ul></li><li><p>Target Health Status of a Registered Target</p><ul><li>Initial</li><li>Healthy</li><li>Unhealthy</li><li>Unused</li><li>draining (deregistration)</li></ul></li><li><p>Reasons for unhealthy</p><ul><li>A Security Group of the instance does not allow traffic</li><li>NACLs does not allow traffic</li><li>The ping path does not exist</li><li>The connection times out</li><li>The target did not return a successful response code</li></ul></li><li><p>Target Type (you cannot change its target type)</p><ul><li><strong>instance</strong><ul><li>The targets are specified by instance ID</li></ul></li><li><strong>ip</strong><ul><li>The targets are specified by IP address</li><li>You can’t specify publicly routable IP addresses</li></ul></li><li>If you specify targets using an instance ID, traffic to instances using the primary private IP address specified in the primary network interface for the instance</li><li>If you specify targets using IP addresses, you can route traffic to an instance using any private IP address from one or more network interfaces.</li></ul></li><li><p>Integration with ECS (Dynamic Mapping)</p><ul><li>Since ALB&#x2F;NLB supports dynamic mapping. We can configure the ECS service to use the load balancer, and a dynamic port will be selected for each ECS task automatically. With Dynamic mapping, multiple copies of a task can run on the same instance</li></ul></li><li><p><strong>ELB + ASG is good for fault tolerance</strong></p><ul><li><strong>Using ELB with ASG, both should be in the same region and launch in the same VPC.</strong></li></ul></li><li><p>CloudWatch metrics</p><ul><li>Latency<ul><li>The total time elapsed, in seconds, from the time the load balancer sent the request to a registered instance until the instance started to sennd the response headers.</li></ul></li><li>RequestCount<ul><li>The number of requests completed or connections made during the specified interval</li></ul></li></ul></li></ol><p>##CloudFront</p><ol><li><p>CDN</p><p>For improving read performance, content is cached in the Edge Location</p></li><li><p>can integrate with Shield and WAF for DDoS protection</p></li><li><p>can expose HTTPS and can talk to internal HTTPS backends</p></li><li><p>It can be used to cache web content from the origin server to <strong>provide users with low latency access, and offload origin server loads</strong></p></li><li><p><strong>For files less than 1 Gb, using CloudFront would provide better performance than S3 Transfer Acceleration</strong></p></li><li><p>CloudFront Origins</p><ul><li><p>S3 Bucket</p><ul><li>cache content at the edge</li><li>enhanced security with Cloud Origin Access Identity (OAI)</li><li>can be as an ingress</li></ul></li><li><p>Custom Origin(HTTP)</p><ul><li><p>ALB</p></li><li><p>EC2 </p></li><li><p>S3 website</p></li><li><p>Route 53</p></li><li><p>any HTTP backend</p></li></ul></li></ul></li><li><p>Distribution</p><p>A collection of Edge Locations</p></li><li><p>CloudFront vs S3 CRR</p><ul><li>CloudFront<ul><li>great for static content</li><li>files cached for a TTL</li></ul></li><li>S3 CRR<ul><li>great for dynamic content</li><li>near real-time</li><li>only read</li></ul></li></ul></li><li><p>OAI (Origin Access Identity)</p><p>Using OAI to restrict S3 to be accessed only by this identity</p></li><li><p>Signed URL</p></li></ol><p>   Using SDK API to generate Signed URL for restricting visit.</p><p>   Signed URL to CloudFront, and OAI to S3 can create a simple media sharing website.</p><ul><li>Signed URL: Access to one file</li><li>Signed Cookie: Access to a bunch of files</li></ul><ol start="11"><li><p>The data transfer out to the internet or origin is not free. Adifferent rate is charged depending on the region.</p></li><li><p>Data transfer from origin to CloudFront Edge Locations is free</p></li><li><p>Because for each custom SSL certificate associated with one or more CloudFront distributions using the Dedicated IP version of custom SSL certificate support, you are charged at $600 per month</p></li><li><p>If you want to increase the cache durationn for certain contents, you can add a Cache-Control header to control how long the objects stay in the CloudFront cache</p></li><li><p>Error page can be customized through CloudFront</p></li><li><p><strong>Invalidating</strong></p><ul><li>Invalidating the object removes it from the CloudFront edge cache to return the correct file to the user.</li></ul></li><li><p>Redirect HTTP to HTTPs</p><ul><li>Configure the Viewer Protocol Policy of the CloudFront distribution to be “Redirect HTTP to HTTPs”</li></ul></li><li><p>If you run PCI or HIPAA-compliant workloads based on the AWS Shared Responsibility Model, we recommend that you log your CloudFront usage data for the last 365 days for future auditing purpose. To log usage data, you can do the following</p><ul><li>Enable CloudFront access logs</li><li>Capture requests that are sent to the CloudFront API</li></ul></li><li><p><strong>Query String Forwarding</strong></p><ul><li>CloudFront Query String Forwarding only supports Web distribution. For query string forwarding, the delimiter character must always be a “&amp;” character. Parameters’ names and values used in the query string are case sensitive. Parameter Names and Values should use the same case.</li></ul></li></ol><h2 id="Global-Accelerator"><a href="#Global-Accelerator" class="headerlink" title="Global Accelerator"></a>Global Accelerator</h2><ol><li>For global users to access application that deployed in AWS, minimize latency and provide a straight connection to AWS resources</li><li>Unicast IP vs Anycast IP<ul><li>Unicast IP: one server holds one IP address</li><li>Anycast IP: all servers hold the same IP address and clients are routed to the nearest one</li></ul></li><li>Global Accelerator using Anycast IP<ul><li><strong>2 Anycast static IP addresses are created for your application</strong></li><li>Anycast IP send traffic to the Edge Location, then Edge Location send traffic to ALB or something else</li></ul></li><li>Improve performance and availability of the application</li><li>Works with Elastic IP, EC2, ALB, NLB, public or private</li><li>No caching</li><li>DDoS protection by Shield</li><li>Global Accelerator vs CloudFront<ul><li>Same<ul><li>Using Edge Locations around the world</li><li>Integrate with Shield for DDoS</li></ul></li><li>Diff<ul><li>GA<ul><li>great for application serving global users</li><li>all requests redirect from Edge Locations to AWS services, no caching</li><li>great for TCP and UDP</li><li>fast regional failover</li></ul></li><li>CloudFront<ul><li>great for static and dynamic content</li><li>content is cached at the Edge Location</li></ul></li></ul></li></ul></li></ol><h2 id="API-Gateway"><a href="#API-Gateway" class="headerlink" title="API Gateway"></a>API Gateway</h2><ol><li><p>Support for WebSocket protocol</p></li><li><p>Handle API versioning</p></li><li><p>Handle different environments</p></li><li><p>Handle authentication and authorization</p></li><li><p>Handle request throttling</p></li><li><p><strong>Cache API response</strong></p><ul><li>Default TTL value for API Caching: 300s</li><li>Maximum TTL value: 3600s (60m)</li></ul></li><li><p>Integration Type</p><ul><li>Lambda<ul><li>Easy way to expose REST API backed by AWS Lambda</li></ul></li><li>HTTP<ul><li>Expose HTTP endpoints in the backends (HTTP API on premises, ALB)</li></ul></li><li>AWS Services<ul><li>Expose any AWS API through API Gateway (Step Function workflow, SQS)</li></ul></li><li>Mock</li><li>VPC Link<ul><li>A way to connect to the resources within a private VPC</li></ul></li></ul></li><li><p>Endpoint Types</p><ul><li>Edge-Optimized (default): for global users<ul><li>through Edge Location</li><li>API Gateway still lives in one region</li></ul></li><li>Regional: for clients within the same region<ul><li>Cloud manually combine with CloudFront (more control on caching strategies and distribution)</li></ul></li><li>Private<ul><li>Can only be accessed from VPC using ENI</li></ul></li></ul></li><li><p><strong>Endpoint Integration inside a Private VPC</strong></p><ul><li>You can also now use API Gateway to front APIs hosted by backends that exist privately in your own data centers, using AWS Direct Connect links to your VPC.</li></ul></li><li><p>Authentication &amp; Authorization</p></li></ol><ul><li><p>IAM Permissions</p><ul><li>Good for Authentication + Authorization</li><li>For authorizing users which are inner ones</li><li>Using Sig v4 capacity where IAM credentials are in headers</li></ul></li><li><p>Lambda Authorizer (formerly Custom Authorizer)</p><ul><li>Good for Authentication + Authorization</li><li>Using Lambda to verify token in headers being passed</li><li>Option to cache result of authentication</li><li>Helps to use OAuth &#x2F; SAML &#x2F; 3rd party type of authentication</li><li>Lambda must retun an IAM policy for the user</li></ul></li><li><p>Cognito User Pools</p><ul><li><strong>Cognito helps with Authentication, not Authorization</strong></li><li>Cognito fully managed user lifecycle</li><li>API Gateway automatically verify</li></ul></li></ul><ol start="11"><li><p>Throttling Limit Setting</p><ul><li>Server-side throttling limits are applied across all clients. These limit settings exist to prevent your API</li><li>Per-client throttling limits are applied to clients that use API keys associated with your usage policy as client identifier</li></ul></li><li><p>Accout-level throttling per Region</p><ul><li>When request submissionns exceed the steady-state request rate and burst limits, API Gateway fails the limit-exceeding requests and returns 429 Too Many Request errors responses to the client.</li><li><strong>Burst limit corresponds to the maximum number of concurrent request submission that API Gateway can fulfill at any moment.</strong></li><li>Ex: given a burst limit of 5,000 and account-level rate limit of 10,000 request per second in the Region<ul><li>If the caller sends 10,000 in the first millisecond, API Gateway serves 5,000 of those reuqests and throttles the rest in the one-second period</li></ul></li></ul></li><li><p>Usage Plan</p><ul><li>A Usage Plan is a set of rules that operates as a barrier between the client and the target of the API Gateway. This set of rules be applied to one or more APIs and stages</li><li>API Key must be associated with a usage plan, one or more; otherwise, it will not be attached to any API. Once attached, the API keys are applied to each API under the usage plan.</li><li>API Key feature useful to filter unsolicited requests. It’s not a proper way to apply authorization to the API method</li><li><strong>Client put api key in request header “x-api-key”</strong></li></ul></li><li><p><strong>Mehtod Level Throttling can override Stage Level Throttling in a Usage Plan</strong></p></li><li><p>Controlling Access to an API in API Gateway</p><ul><li><strong>Resource Policies</strong><ul><li>Using resource policies to allow your API to be securely invoked by<ul><li>Users from a specified AWS account</li><li>Specified source IP address ranges or CIDR blocks</li><li>Specified VPC or VPC endpoints</li></ul></li><li>API Gateway resource policies are attached to resources, while IAM policies are attached to IAM entities</li></ul></li><li>Standard AWS IAM roles and policies</li><li>CORS</li><li>Lambda Authorizers</li><li>Amazon Cognito User Pools</li><li>Client-side SSL Certificates<ul><li>Can be used to verify that HTTP requests to your backend system from API Gateway</li></ul></li><li>Usage Plans</li></ul></li><li><p>Security Measures</p><ul><li>API Gateway supports throttling settings for each method in your APIs, you can set a standard rate limit and a burst limit per second for each method in your REST APIs. Further, API Gateway automatically protects your backend sysetems from distributed denial-of-service (DDoS) attacks, whether attacked with counterfeit requests (Layer 7) or SYN floods (Layer 3)</li></ul></li><li><p>In Cache settings, the actions that you can do manually</p><ul><li>Flush entire cache</li><li>Change cache capacity</li><li>Encrypt cache data</li></ul></li><li><p>Logs</p><ul><li><p>CloudWatch Logs</p><ul><li>Loged data includes errors or execution traces (such as request or response parameter values or payloads)</li></ul></li><li><p>Access Logging</p><ul><li>In access logging, as an developer, want to log who has accessed your API and how the caller accessed the API. You can create your own log group or choose an existing one, which could be managed by API Gateway</li></ul></li></ul></li><li><p>Permissions</p><ul><li>Controlling access to API Gateway <strong>with IAM permissions</strong> by controlling access to the two API Gateway component processes<ul><li>Management Component<ul><li>create, deploy, and manage an API in API Gateway</li><li>must grant the API developer permissions</li></ul></li><li>Execution Component<ul><li>call a deployed API or refresh teh API caching</li><li>must grant the API caller permissions</li></ul></li></ul></li></ul></li></ol><h2 id="RDS"><a href="#RDS" class="headerlink" title="RDS"></a>RDS</h2><ol><li><p>RDS Backups</p><ul><li>Automated Backups<ul><li>daily full backup of the database (during the maintenance window)</li><li><strong>every 5 minutes backup transaction logs</strong></li><li>7 days retention</li><li>Storage I&#x2F;O may be suspended during backup</li></ul></li><li>DB Snapshots</li></ul></li><li><p><strong>During automated backup, RDS creates a storage volume snapshot of the entire Database Instance. RDS uploads transaction logs for DB instances to S3 every 5 mins. To restore DB instance at a specific point in time, a new DB instance is created using DB snapshot.</strong></p></li><li><p><strong>If you disable automated backups, it disables point-in-time recovery.</strong></p></li><li><p>RDS Read Replicas</p><ul><li>Up to 5 Read Replicas</li><li>Within AZ, Cross AZ or Cross Region</li><li>Replicas are ASYNC, so reads are eventually consistent</li><li>Read Replicas can be promoted to DB</li><li>Use case: split workload for BI, data analytics, etc…</li><li>If you create your Read Replicas in another AZ, you need to pay connection fee between different AZs</li></ul></li><li><p>RDS Multi AZ (DR)</p><ul><li><strong>SYNC Replication</strong></li><li>One DNS Name - automatic failover to standby</li><li>Read Replicas be setup as Multi AZ for DR</li><li>provide enhanced availability and durability for DB instances.</li></ul></li><li><p>RDS Encryption</p><ul><li>at rest encryption<ul><li>Using KMS and defined at launch time</li><li>If the master don’t encrypt, read replicas also can’t be encrypted</li><li>Transparent Data Encryption (TDE) is available for Oracle and SQL  Server</li></ul></li><li>in-flight<ul><li>Using SSL to enforce SSL (PostgreSQL by set value, and MySQL by typing SQL command)</li></ul></li></ul></li><li><p>IAM database authentication</p><ul><li>works with MySQL and PostgreSQL</li><li>You don’t need a password, just an authentication token obtained from IAM and RDS API calls</li><li>Auth token has a lifetime of 15 minutes</li></ul></li><li><p>RDS Failover Mechanism</p><ul><li>Failover mechanism automatically changes the DNS CNAME record of the DB instance to point to the standby DB instance</li></ul></li><li><p>Solution for Read-Heavy</p><ul><li>read replicas</li><li>ElastiCache</li><li>Sharding the dataset</li></ul></li><li><p>Solution for too many PUT</p></li></ol><ul><li>Creating an SQS queue and store these PUT requests in the message queue and then process it accordingly</li></ul><ol start="11"><li>DB Parameter Groups<ul><li>You manage your DB engine configuration through the use of parameters in a DB parameter group.</li><li>DB parameter groups act as a container for engine configuration values that are applied to one or more DB instances.</li><li>A default DB parameter group is created if you create a DB instance without specifying a customer-created DB parameter group.</li><li><strong>You can’t modify the parameter settings of a default DB parameter group. You must create your own DB parameter group to change parameter settings from their default value</strong></li><li><strong>If you want to use your own DB parameter group, you simple create a new DB parameter group, modify the desired parameters, and modify your DB instance to use the new DB parameter group</strong></li></ul></li></ol><h2 id="Aurora"><a href="#Aurora" class="headerlink" title="Aurora"></a>Aurora</h2><ol><li>Aurora cost more than RDS (20% more) - but is more efficient</li><li>Data is hold in 6 replicas, across 3 AZs</li><li>Auto healing capability</li><li>Multi AZ, auto scaling read replicas</li><li>Aurora database can be global for DR or latency purpose</li><li>Auto scaling storage from 10GB - 64TB</li><li><strong>Aurora Serverless Option</strong></li><li><strong>Support for CRR</strong></li><li><strong>Aurora can span multiple regions by Aurora Global Database</strong></li></ol><h2 id="DynamoDB"><a href="#DynamoDB" class="headerlink" title="DynamoDB"></a>DynamoDB</h2><ol><li><p>Highly avaialbe with replication across 3 AZs</p></li><li><p>Distributed NoSQL database</p></li><li><p>Integrate with IAM for authentication and authorization</p></li><li><p>Enable event driven progarmming with DynamoDB Streams</p></li><li><p>Features</p><ul><li>DynamoDB is made of tables</li><li>each table has a primary key</li><li>each table has a infinite number of items</li><li>each item has attributes</li><li>max size of item is 400KB</li></ul></li><li><p>Provisioned Throughputs</p><ul><li>Table have provisioned read and write capacity units</li><li>Read Capacity Unit (RCU): throughput for reads<ul><li>1 RCU &#x3D; 1 strongly consistent read of 4KB per second</li><li>1 RCU &#x3D; 2 eventually consistent read of 4KB per second</li></ul></li><li>Write Capacity Unit (WCU): throughput for writes<ul><li>1 WCU &#x3D; 1 write of 1KB per second</li></ul></li><li>Option to setup auto-scaling of throughput</li><li>Throughput can be exceeded temporarily using “burst credit”<ul><li>If burst credit are empty, you’ll get a “ProvisionedThroughputException”</li></ul></li></ul></li><li><p>DynamoDB Accelerator (DAX)</p><ul><li>Seamless cache for DynamoDB, no application re-write</li><li>Writes go through DAX to DynamoDB</li><li>Solves the Hot Key Problem</li><li>5 minutes TTL for cache by default</li><li>up to 10 nodes in the cluster</li><li>Multi AZ</li></ul></li><li><p>DynamoDB Streams</p><ul><li><p>It can monitor the changes to a DynamoDB table.</p></li><li><p>this stream can be read by Lambda, and then we can do</p><ul><li>react to changes in real time</li><li>analytics</li><li>create derivative tables&#x2F;views</li><li>insert into ElasticSearch</li></ul></li><li><p>using Streams to implement CRR</p></li><li><p>Stream has 24 hours of data retention</p></li><li><p>When you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table’s stream. Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.</p></li></ul></li><li><p>Transaction</p><ul><li>Coordinate with Insert, Update, Delete across multiple tables</li><li>Include up to 10 unique items or up to 4 MB of data</li></ul></li><li><p>On Demand Option</p></li></ol><ul><li>No capacity planning needed (WCU&#x2F;RCU) - scales automatically</li><li>2.5x more expensive than provisioned capacity</li></ul><ol start="11"><li>Global Tables<ul><li>Multi region, fully replicated, high performance</li><li>Must enable DynamoDB Streams</li><li><strong>Useful for low latency, DR purposes</strong></li></ul></li><li>Capacity Planning<ul><li>Planned capacity: Provisioned WCU &amp; RCU, can enable auto scaling</li><li>On-demand capacity: get unlimited WCU &amp; RCU, no throttle, more expensive</li></ul></li><li>DMS can migrate data from Mongo, Oracle, MySQL, S3, etc… to DynamoDB</li><li>Better for storing metadata</li><li>It doesn’t have the feature of Read Replica</li><li>Auto Scaling<ul><li>With DynamoDb Auto Scaling, it can automatically increase its write capacity for the spike and decrease the throughput after the spike.</li><li>It’s good for applications where database utilization cannot be predicted.</li><li>It can help to scale dynamically to any load <strong>for both DynamoDB tables and Global Secondary Index</strong></li></ul></li></ol><h2 id="Lambda"><a href="#Lambda" class="headerlink" title="Lambda"></a>Lambda</h2><ol><li><p>Pay per request and compute time</p></li><li><p>Lambda is good for running code, not packaging</p></li><li><p>Lambda Limits - per region (apply to configuration, deployments, and execution)</p><ul><li><strong>Function Memory allocation</strong><ul><li><strong>128MB - 3008MB (64MB increments)</strong></li></ul></li><li><strong>Function Timeout</strong><ul><li><strong>900s (15m)</strong></li></ul></li><li><strong>Function Environment Variables</strong><ul><li><strong>4KB</strong></li></ul></li><li><strong>Function Resource-based Policy</strong><ul><li><strong>20KB</strong></li></ul></li><li><strong>Function Layers</strong><ul><li><strong>5 layers</strong></li></ul></li><li><strong>Function burst concurrency</strong><ul><li><strong>500 - 3000 (varies per Region)</strong></li></ul></li><li><strong>Invocation Payload (request and response)</strong><ul><li><strong>6 MB (synchronous)</strong></li><li><strong>256 KB (asynchronous)</strong></li></ul></li><li><strong>Deployment Package</strong><ul><li><strong>50 MB (zipped)</strong></li><li><strong>256 MB (unzipped)</strong></li></ul></li><li><strong>Disk</strong><ul><li><strong>Disk capacity: 512MB</strong></li><li><strong>Can use &#x2F;tmp directory (500MB) to load other files at startup</strong></li></ul></li></ul></li><li><p>Lambda@Edge</p><ul><li>Use case<ul><li>When you want to run a global AWS Lambda, and build more responsive applications</li><li>Implement request filtering before reaching your application<ul><li>Viewer request</li><li>Origin request</li><li>Origin response</li><li>Viewer response</li></ul></li><li>Website Security and Privacy</li><li>Dynamic Web Application at the Edge</li><li>SEO</li><li><strong>A&#x2F;B Testing</strong></li><li><strong>User Prioritization</strong></li><li>User Tracking and Analytics</li><li>User Authentication and Authorization</li></ul></li><li><strong>Lambda@Edge function does provide the capability to cutomize content.</strong> Lambda@Edge allows users to run their own Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS Regions close to the viewer. Lambda functions runn in response to CloudFront events, without provisioning or managing servers.</li></ul></li><li><p>By default Lambda run in No VPC</p></li><li><p>Disadvantage for Serverless services: Cold Start</p></li><li><p>Lambda funcntion environment variables are used to <strong>configure additional parameters that can be pssed to lambda function</strong></p></li><li><p>Services that invoke Lambda Functions Synchronously</p><ul><li>ELB (ALB)</li><li>Cognito</li><li>API Gateway</li><li>CloudFront (Lambda@Edge)</li><li>Kinesis Firehose</li><li>Step Functions</li><li>S3 Batch</li><li>Lex</li><li>Alexa</li></ul></li><li><p>Services that invoke Lambda Fuctions Asynchronously</p><ul><li>S3</li><li>SNS</li><li>SES</li><li>CloudFormation</li><li>CloudWatch Logs</li><li>CloudWatch Events</li><li>CodeCommit</li><li>CodePipeline</li><li>Config</li><li>IoT</li><li>IoT Events</li></ul></li><li><p>Lambda supports the following poll-based services</p></li></ol><ul><li>Kinesis</li><li>DynamoDB</li><li>SQS</li></ul><ol start="11"><li>Debugging and error handling</li></ol><ul><li><p>For asynchronous invocation</p></li><li><p>If you don’t specify a DLQ for failed event, this event will be discard after several failed retries</p></li><li><p>DLQ Resources</p><ul><li>SNS</li><li>SQS</li></ul></li></ul><ol start="12"><li><p><strong>If your functions runs out of memeory, the Linux kernel will kill your process immediately. There is no supported way at this time to catch and handle this error either.</strong></p></li><li><p>When using Lambda, you are only responsible for your code. AWS will perform  provisioning capacity, monitoring, deploying your code and logging on your behalf.</p></li><li><p>Lambda event source mappings support <strong>SQS standard and SQS FIFO</strong></p></li><li><p>Event Source Options</p><ul><li>Enabled<ul><li>A flag to signal Lambda that it should start polling your SQS queue</li></ul></li><li>EventSourceArn<ul><li>The ARN of your SQS queue that Lambda is monitoring for new messages</li></ul></li><li>FunctionArn<ul><li>The Lambda function to invoke</li></ul></li><li><strong>BatchSize</strong><ul><li>The number of records to send to the function in each batch. For a standard queue this can be up to 10,000 records. For a FIFO queue the maximum is 10.</li></ul></li></ul></li><li><p>You can also invoke a Lambda function by Lambda’s invoke API.</p></li><li><p>When you updating a Lambda function, there will be a brief window of time, typically less than a minute, when requests could be served by either the old or the new version of your function.</p></li><li><p>Lambda Function ARN</p><ul><li>Qualified ARN - with version suffix<ul><li>arn:aws:lambda:aws-region:acct-id:function:helloworld:$LATEST</li></ul></li><li>Unqualified ARN - without version suffix<ul><li>arn:aws:lambda:aws-region:acct-id:function:helloworld</li><li>You cannot use Unqualified ARN to create an alias.</li><li>This Unqualified ARN will invoke a LATEST version</li></ul></li></ul></li><li><p>Lambda Alias</p><ul><li>Invokers don’t need to change Lambda ARN when usig Lambda Alias, creators just need to remap Lambda Alias to a new version of Lambda after publishing a new version</li></ul></li><li><p>Publishing Lambda</p><ul><li>When you publish a version, Lambda makes a snapshot copy of the Lambda function code (and configuration) in the $LATEST version. <strong>A published version is immutable (both code and configuration).</strong></li></ul></li><li><p>Version numbers are never reused, even for a function that has been deleted and recreated</p></li><li><p>Not recommended for using $LATEST ARN in PRODUCTION mode, there are chances that the configuration can be meddled and can cause unwanted issues.</p></li><li><p><strong>Function Policy</strong></p><ul><li>Grant cross-account permissions <strong>(not on the execution role policy)</strong></li><li><strong>function policy cannnot be edited from the AWS console (using either CLI or SDK)</strong></li></ul></li><li><p><strong>Lambda accessing Private VPC</strong></p><ul><li>If your Lambda function accesses a VPC, you must make sure that your VPC has sufficient ENI capacity to support the scale requirements of your Lambda function. Using formula to determine the ENI capacity</li><li><strong>Peak cocurrency executions &#x3D; Peak Requests per Second * Average Function Duration (in seconds)</strong></li><li><strong>ENI capacity &#x3D; Projected peak concurrent execution * (Memory &#x2F; 3 GB)</strong></li></ul></li><li><p>Enviroment Variable Encryption</p><ul><li>By default, all data in environment variables are encrypted by KMS, then automatically decrypted to Lambda code. <strong>(not encrpted during deployment process, only after deployment)</strong></li><li>Using <strong>encryption helper and decryption helper</strong> to encrypted and decrypted sensitive data during deployment</li></ul></li><li><p>AWSLambdaBasicExecutionRole</p><ul><li>Grants permissions only for CloudWatch Logs actions to write logs</li><li>Contains<ul><li>logs:CreateLogGroup</li><li>logs:CreateLogStream</li><li>logs:PutLogEvents</li></ul></li></ul></li><li><p>CloudWatch metrics for Lambda</p><ul><li>Dead Letter Error</li><li>Duration</li><li>Invocation</li><li><strong><del>Memory</del></strong></li></ul></li><li><p>Ensuring version of Lambda in the code</p><ul><li>Using Method<ul><li>getFunctionVersion()</li></ul></li><li>Using Environment Variables<ul><li>AWS_LAMBDA_FUNCTION_VERSION</li></ul></li></ul></li><li><p>Errors for the response of Lambda </p><ul><li>Synchronous invocation<ul><li>response header: X-Amz-Function-Error</li><li>The status code is 200 for function error</li></ul></li><li>Asynchronous invocation<ul><li>Stored in DLQ if you specified a DLQ for errors</li></ul></li></ul></li><li><p>Users are charged based on the number of requests and the time it taks for the code to execute.</p></li><li><p>The duration price depends on the amount of memory allocated to the function.</p></li><li><p>Lambda function’s cost will be reduced if the execution duration decreases</p></li></ol><h2 id="CloudWatch"><a href="#CloudWatch" class="headerlink" title="CloudWatch"></a>CloudWatch</h2><ol><li>CloudWatch Metrics<ul><li>Dimension is an attribute of metric (instance ID, environment, etc…)</li><li>Up to 10 dimensions per metric</li><li>Metrics have timestamps</li><li>Metrics belong to namespaces</li></ul></li><li>EC2 instance metrics can monitor “every 5 minutes”, and you can also change to “every 1 minute”</li><li>Using detailed monitoring if you want to prompt scale your ASG in EC2</li><li>EC2 memory usage must be created for Custom Metrics</li><li>Custom Metrics<ul><li>Standard: 1 minutes</li><li>High resolution: up to 1 second, but higher cost</li><li>Use API called PutMetricData and Use exponential back off in case of throttle errors</li></ul></li><li>Dashboards<ul><li>Dashboards are global, can include graphs from different regions</li></ul></li><li>CloudWatch Logs<ul><li>CloudWatch can collect log from<ul><li>Elastic Beanstalk</li><li>ECS</li><li>AWS Lambda</li><li>VPC Flow Logs</li><li>API Gateway</li><li>CloudTrail based on filter</li><li><strong>CloudWatch log agents: ex. EC2</strong></li><li>Route 53: Log DNS queries</li></ul></li><li>can go to<ul><li>to S3 for archive</li><li>to ElasticSearch for further analytics</li></ul></li><li>encrytion using KMS</li></ul></li><li>CloudWatch Logs Insights: can be used to query logs and queries to CloudWatch Dashboards</li><li>CloudWatch Logs Agent vs CloudWatch Unified Agent<ul><li>Logs Agent<ul><li>Old version</li><li>Can only send to CloudWatch Logs</li></ul></li><li>Unified Agent<ul><li>Collect additional system level metrics such as RAM, processes, etc…</li><li>Centralized configuration using SSM Parameter Store</li></ul></li></ul></li><li>CloudWatch Unified Agent - Metrics<ul><li>CPU</li><li>Disk metrics</li><li>RAM</li><li>Netstat</li><li>Processes</li><li>Swap Space</li></ul></li><li>CloudWatch Alarm<ul><li>trigger notification for any metrics</li><li>Alarm can used for<ul><li>ASG</li><li>EC2 Action</li><li>SNS notification</li></ul></li><li>Alarm States<ul><li>OK</li><li>INSUFFICIENT_DATA</li><li>ALARM</li></ul></li><li>Period<ul><li>High resolution custom metrics: can only choose 10 sec or 30 sec</li></ul></li></ul></li><li>CloudWatch Events<ul><li>Schedule: Cron jobs</li><li>Event Pattern: Event rules to react to a service doing something</li><li><strong>Trigger to</strong> <ul><li><strong>Lambda</strong></li><li><strong>SQS</strong></li><li><strong>SNS</strong></li><li><strong>Kinesis Messages</strong></li></ul></li><li>When target is Lambda, the inputs can be<ul><li>Matched event</li><li>Part of the matched event</li><li>Constant (JSON text)</li></ul></li></ul></li></ol><h2 id="CloudTrail"><a href="#CloudTrail" class="headerlink" title="CloudTrail"></a>CloudTrail</h2><ol><li>Provides governance, compliance and audit for your AWS account</li><li><strong>CloudTrail is enable by default</strong></li><li>Get an history of events &#x2F; API calls made within your AWS account by<ul><li>Console</li><li>SDK</li><li>CLI</li><li>AWS Services</li></ul></li><li>To ensure logs have nont tampered with you need to turn on Log File Validation</li><li><strong>CloudTrail can be set to log across all AWS accounts in an organization and all regions in an account</strong></li><li>CloudTrail will deliver log files from all regions to S3 bucket and an optional CloudWatch Logs log group you specified.</li><li>Two types of events<ul><li>Management Events<ul><li>Tracks management operations, turn on by default</li></ul></li><li>Data Events<ul><li>Tracks specific operations for specific AWS services, turn off by default</li></ul></li></ul></li><li><strong>By default, CloudTrail event log files are encrypted using S3 server-side encryption (SSE-S3).</strong> You can also choose to encrypt your log files with an KMS key.</li><li>Log File Integrity<ul><li>After you enable CloudTrail log file integrity, it will create a hash file called digest file, which refers to logs that are generated. <strong>The Digest file can be validated using the public key.</strong> This feature ensures that all the modifications made to CloudTrail log files are recorded.</li></ul></li><li><strong>Global Service Events Logging</strong></li></ol><ul><li>For most services, events are recorded in the region where the action occurred.</li><li>For Global services such as IAM, and CloudFront, events are delivered to any trail that includes global services</li><li>For most global services, events are logged as occurring in US East (N. Virginnia) Region, but some global service events are logged as occurring in other regions, such as US East (Ohio) Region or US West (Oregon) Region.</li><li><strong>If you change the configuration of a trail from logging all regions to logging a single region, global service event logging is turned off automatically for that trail</strong></li><li><strong>For eliminating duplicate logs in all regions, you can disable Global Service Event in all regions and enable them in only one region</strong></li></ul><h2 id="SQS"><a href="#SQS" class="headerlink" title="SQS"></a>SQS</h2><ol><li><p><strong>Unlimited throughputs, unlimited number of messages in queue</strong></p></li><li><p>Default retention of messages: 4days, maximum is 14 days</p></li><li><p>Low latency ( &lt; 10ms on publish and receive)</p></li><li><p>Limitation of 256KB per message sent</p></li><li><p>It helps in horizontal scaling of AWS resources and is used for decoupling systems.</p></li><li><p>SQS Access Policies</p><ul><li>Useful for cross-account access to SQS queues</li><li>Useful for allowing other services (SNS, S3…) to write to an SQS queue</li></ul></li><li><p>Message Visibility Timeout</p><ul><li>After a message is pulled by a consumer, it becomes invisible to other  consumers</li><li>By default, the “message visibility timeout” is 30 seconds, 12 hours maximum</li><li>A consumer could call the “ChangeMessageVisibility” API to get more time</li></ul></li><li><p>Dead Letter Queue</p><ul><li>Make sure to process the messages in the DLQ before they expire (Good to set a retention of 14 days in the DLQ)</li></ul></li><li><p>Delay Queue</p><ul><li>Delay a message up to 15 minutes</li><li>Default is 0 seconds</li><li>Can set a default at queue level</li></ul></li><li><p>Standard Queue</p></li></ol><ul><li>Unlimited number of transactions per second</li></ul><ol start="11"><li><p>FIFO Queue</p><ul><li>Limited throughput: 300 msg&#x2F;s without batching, 3000 msg&#x2F;s with</li><li>Exactly-once send capability</li></ul></li><li><p>Queuing vs Streaming</p><ul><li>Queuing<ul><li>Generally will delete messages once they are consumed</li><li>Not real-time</li><li>have to pull</li></ul></li><li>Streaming<ul><li>Multiple consumers can react to events</li><li>Event live in the stream for long periods of time, so complex operations can be applied</li><li>Real-time</li></ul></li></ul></li><li><p>Amazon SQS Extended Client Library for Java</p><ul><li>Lets you send messages 256KB to 2GB in size</li><li>The message will be stored in S3 and library will reference the S3 object</li></ul></li><li><p>Short Polling vs Long Polling</p><ul><li>Short Polling (default)<ul><li>When you need a message right away, short polling is what you want</li></ul></li><li>Long Polling (most used)<ul><li>Maximum 20 seconds</li><li>reduce cost</li></ul></li><li>Benefits for Long Polling<ul><li>Eliminate empty responses</li><li>Eliminate false responses</li><li>Return messages as soon as they become available</li></ul></li></ul></li><li><p>SQS doesn’t delete messages automatically</p></li><li><p>Permission exist on the Queue level, not on the message level</p></li><li><p>SQS Batch Actions</p><ul><li>To reduce costs and manipulate up to 10 messages with a single action, you can use the following actions<ul><li>SendMessageBatch</li><li>DeleteMessageBatch</li><li>ChangeMessageVisibilityBatch</li></ul></li></ul></li><li><p>DeleteMessage</p><ul><li>Deletes the specifed message from the specified queue. Using the <strong>ReceiptHandle</strong> of the message (not the MessageId which you receive when you send the message)</li></ul></li><li><p>SQS Encryption</p><ul><li>SQS does not encrypt messages by default. You need to enable encryption on the Queue messages.</li></ul></li><li><p>How to process data with priority</p><ul><li>Use two SQS queues, one for high priority messages and the other for default priority. The high priority queue can be polled first.</li></ul></li><li><p>Queue Size Metrics</p><ul><li>ApproximateNumberOfMessagesVisible describes the number of messages available for retrieval. It can be used to decide the queue length.</li></ul></li><li><p>Increasing Throughput</p><ul><li><p>SQS queues can deliver very high throughput.</p></li><li><p>Horizontal Scaling</p><ul><li>To achieve high throughput, you must scale message producers annd consumers horizontally (add more producers and consumers)</li><li>Horizontal scaling involves increasing the number of message producers and consumers in order to increase your overall queue throughput. You can scale horizontally in three ways<ul><li>Increase the number of threads per client</li><li>Add more client</li><li>Increase the number of threads per client and add more clients</li></ul></li></ul></li><li><p>Action batching</p><ul><li>Batching performs more work during each round trip to the service.</li></ul></li></ul></li></ol><h2 id="SNS"><a href="#SNS" class="headerlink" title="SNS"></a>SNS</h2><ol><li><p>All messages published to SNS are stored redundantly across multiple AZs</p></li><li><p><strong>It’s a real-time notification.</strong></p></li><li><p>Integrate with AWS services</p><ul><li>CloudWatch (for alarms)</li><li>ASG notification</li><li>S3 (on bucket events)</li><li>CloudFormation (upon state changes)</li></ul></li><li><p>Publish</p><ul><li>Topic Publish (using SDK)<ul><li>Create a topic</li><li>Create a subscription (or many)</li><li>Publish to the topic</li></ul></li><li>Direct Publish (for mobile apps SDK)<ul><li>Create a platform application</li><li>Create a platform endpoint</li><li>Publish to the platform endpoint</li><li>Works with Google GCM, Apple APNS, Amazon ADM, etc…</li></ul></li></ul></li><li><p>SNS Access Policies</p><ul><li>Useful for cross-account access to SNS topics</li><li>Useful for allowing other services to write to an SNS topic</li></ul></li><li><p>SQS + SNS: Fan Out</p><ul><li>Push once in SNS, receive in all SQS queues</li><li>Fully decoupled, no data loss</li><li>SQS allows<ul><li>data persistence</li><li>delayed processing</li><li>retries of work</li><li>Make sure your SQS queue access policy allows for SNS to write</li><li><strong>SNS cannot send messages to SQS FIFO queues (AWS Limitation)</strong></li></ul></li><li>Use Case<ul><li>S3 Events to multiple queues (if you want to send the same S3 event to many SQS queues, use fan-out)</li></ul></li></ul></li><li><p>Subscribers do not pull for messages (not like SQS)</p></li><li><p>Messages are instead automatically and immediately pushed to subscribers</p></li><li><p>SNS Topic</p><ul><li>Topic allows you to group multiple subscriptions together</li><li>When topic deliver messages to subscribers it will automatically format your message according to the subscriber’s chosen protocol</li><li>A topic is able to deliver to multiple protocol at once<ul><li>HTTP and HTTPS: create web hooks into your web application</li><li>Email</li><li>Email-JSON</li><li>SQS</li><li>Lambda</li><li>SMS</li><li>Platform application endpoint: Mobile Push</li></ul></li></ul></li><li><p>Delivery protocols for receving notification from SNS</p></li></ol><ul><li>HTTP</li><li>HTTPS</li><li>Email</li><li>Email-JSON</li><li>SQS</li><li>Application</li><li>Lambda</li><li>SMS</li></ul><ol start="11"><li>SNS Message Filtering - Using Filter Policy</li></ol><p>   By default, a subscriber of an Amazon SNS topic receives every message published to the topic. A subscriber assigns a filter policy to the topic subscriptionto receive only a subset of the messages. A filter policy is a simple JSON obiert. The policy contains attributes that define which messages the subscriber receives.</p><ol start="12"><li>Message Attribute Items<ul><li>Name</li><li>Type</li><li>Value</li><li><del>MessageId</del></li></ul></li></ol><h2 id="Messaging"><a href="#Messaging" class="headerlink" title="Messaging"></a>Messaging</h2><ol><li>Two patterns of application communication<ul><li>Sync: can be problematic if there are sudden spikes of traffic</li><li>Async<ul><li>SQS: queue model</li><li>SNS: pub&#x2F;sub model</li><li>Kinesis: real-time streaming model</li></ul></li></ul></li></ol><h2 id="KMS"><a href="#KMS" class="headerlink" title="KMS"></a>KMS</h2><ol><li>Integrate with<ul><li>EBS</li><li>S3</li><li>Redshift</li><li>RDS</li><li>SSM - Parameter Store</li></ul></li><li>KMS can help in encrypting up to 4KB of data per call, if data &gt; 4KB, use envelope encryption</li><li>Envelope Encryption<ul><li>Encrypt your data key by your Customer CMK, then delete plain text data key. Keep encrypted data key and encrypted data stored in S3.</li><li>When you need to decrypt data in S3, you first decrypt your data key by your CMK, then decrypt data by decrypted data key.</li><li>If data is more than 4KB, using Envelope Encryption</li></ul></li><li>Three types of CMK<ul><li>AWS Managed Service Default CMK: free</li><li>User Keys created in KMS: $1 &#x2F; month</li><li>User Keys imported (must be 256-bit symmetric key): $1 &#x2F; month</li></ul></li><li><strong>To give access to KMS to someone</strong><ul><li><strong>Make sure the Key Policy allows the user</strong></li><li><strong>Make sure the IAM Policy allows the API calls</strong></li></ul></li><li><strong>KMS is regional specific.</strong> When you copy snapshot over, you need to re-encrypt your snapshot with a new key</li><li><strong>Keys are not transferrable out of the region they were created in.</strong> Keys are also region-specific.</li><li><strong>KMS Key Policies</strong><ul><li><strong>Control access to KMS keys</strong></li><li><strong>You cannot control access without Key Policy</strong></li><li>Default KMS Key Policy<ul><li>Created if you don’t provide a specific KMS Key Policy</li><li>Complete access to the key to the root user &#x3D; entire AWS account</li><li>Give access to the IAM policies to the KMS key</li></ul></li><li>Custom KMS Key Policy<ul><li><strong>Define users, roles that can access the KMS key</strong></li><li><strong>Define who can administer the key</strong></li><li><strong>Useful for cross-account access of your KMS key</strong></li></ul></li></ul></li><li>Unauthorized KMS master key permission error<ul><li>In the KMS key policy, assign the permission to the application to access the key</li></ul></li><li>Key Rotation</li></ol><ul><li>KMS will rotate keys annually and use the appropriate keys to perform cryptographic operations.</li></ul><h2 id="CLI-amp-SDK"><a href="#CLI-amp-SDK" class="headerlink" title="CLI &amp; SDK"></a>CLI &amp; SDK</h2><ol><li><strong>Access Key ID</strong> and <strong>Secret Access Key</strong> are collectively known as AWS Credentials</li></ol><h2 id="CloudFormation"><a href="#CloudFormation" class="headerlink" title="CloudFormation"></a>CloudFormation</h2><ol><li>For reducing cost:<ul><li>You can estimate the costs of your resources using the CloudFormation template</li><li>Saving strategy: In dev, you could automatically delete templates at 5PM and recreate at 8AM, safely</li></ul></li><li>You can many stacks for many apps, and many layers</li><li>Templates have to be uploaded in S3 and then referenced in CloudFormation</li><li>To update a template, you can’t update it, you have to re-create a new version of that template</li><li>Tempaltes Components<ul><li>Resources (Mandatory)</li><li>Parameters: dynamic inputs for your template</li><li>Mappings: the static variables for your template</li><li>Outputs</li><li>Conditionals</li><li>Metadata: additional information about template</li></ul></li><li>When CloudFormation encounters an error, it will rollback with ROLLBACK_IN_PROGRESS</li><li>CreationPolicy<ul><li>CreationPolicy is invoked only when CloudFormation creates the associated resource.</li><li>The resources that support CreationPolicy<ul><li>AppStream::Fleet</li><li>AutoScaling::AutoScalingGroup</li><li>EC2::Instance</li><li>CloudFormation::WaitCondition</li></ul></li></ul></li><li>DeletionPolicy<ul><li>DeletionPolicy attribute<ul><li>With the DeletionPolicy attribute you can preserve, and in some cases, backup a resource when its stack is deleted. You specify a DeleteionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, CloudFormation deletes the resource by default</li><li>If you want to modify resources outside of CloudFormation, use a <strong>retain</strong> policy and then delete the stack. Otherwise, your resources might get out of sync with your CloudFormation template and cause stack errors</li></ul></li><li>DeletionPolicy options<ul><li>Delete<ul><li>the defualt DeletionPolicy of the most services is delete. It means CloudFormation will delete the resources and its content during stack deletion</li><li>But some services are not<ul><li>RDS::DBCluster resources, default is <strong>Snapshot</strong></li><li>RDS::DBInstance resources, default is <strong>Snapshot</strong></li><li>S3 buckets, you must delete all objects in the bucket for deletion to succeed</li></ul></li></ul></li><li>Retain<ul><li>Keeps the resource without deleting the resource or its content when its stack is deleted.</li></ul></li><li>Snapshot<ul><li>CloudFormation will create a snapshot for the resource before deleting it</li><li>Resources that support snapshots<ul><li>EC2::Volume</li><li>ElastiCache::CacheCluster</li><li>ElastiCache::ReplicationGroup</li><li>Neptune::DBCluster</li><li>RDS::DBCluster</li><li>RDS::DBInstance</li><li>Redshift::Cluster</li></ul></li></ul></li></ul></li></ul></li><li>Parameters on Template<ul><li>OnDemandPercentageAboveBaseCapacity</li><li>SpotMaxPrice<ul><li>determine the maximum price that you are wiling to pay for Spot Instances</li></ul></li></ul></li><li>Dirft Detection</li></ol><ul><li>CloudFormation Dirft Detection can be used to detect changes made to AWS resources outside the CloudFormation Templates.</li><li>It does not determine drift for property values that are set by default. To determine drift for these resources, you can explicitly set property values that can be the same as that of the default value.</li><li>Resolving drift helps to ensure configuration consistency and successful stack operations</li></ul><ol start="11"><li>CloudFormation Template</li></ol><ul><li>A tempalte is a JSON- or YAML- formatted text file that describes your AWS infrastructure.</li><li>Items<ul><li>Resources<ul><li>The required Resources section declares the AWS resources that you want to include in the stack, such as EC2 instance or S3 bucket</li></ul></li><li>Parameters<ul><li>Use the optional Parameters section to cutomize your templates. Parameters enable you to input custom values to your template each time you create or update a stack.</li></ul></li><li>Outputs<ul><li>The optional Outputs section declares output values that you can import into other stacks, return inn response, or view on the CloudFormation console. Ex, you can output the S3 bucket name for a stack to make the bucket easier to find.</li></ul></li><li>Mappings<ul><li>The optional Mappings section matches a key to a corresponding set of named values. Ex, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region.</li></ul></li><li>Rules<ul><li>The optional Rules section validates a parameter or a combination of parameters passed to a template during a stack update.</li></ul></li></ul></li></ul><h2 id="Elastic-Beanstalk"><a href="#Elastic-Beanstalk" class="headerlink" title="Elastic Beanstalk"></a>Elastic Beanstalk</h2><ol><li>Choose a platform, upload your code and it runs with little worry for developers about infrastructure knowledge</li><li>No recommended for Production application</li><li>Elastic Beanstalk is powered by a CloudFormation template setups for you<ul><li>ELB</li><li>ASG</li><li>RDS</li><li>EC2 platforms</li><li>Monitoring (CloudWatch, SNS)</li><li>In-Place and Blue&#x2F;Green elopement methodologies</li><li>Security</li><li>Can run Dockerized environments</li></ul></li><li>Beanstalk is free, but you pay the underlying infrastructure</li><li>Enviroemnt Tier<ul><li>Web-Server Tier<ul><li>Serves HTTP requests</li></ul></li><li>Worker Tier<ul><li>Pulls tasks from an SQS queue</li></ul></li></ul></li><li>Environment Types<ul><li>Load-balanced, scalable environemnt<ul><li>ELB + ASG + EC2</li></ul></li><li>Single-instance environment<ul><li>EC2 + Elastic IP</li></ul></li></ul></li><li>Elastic Beanstalk component can create Web Server environments and Worker environments</li><li>The worker environments in Elastic Beanstalk include an ASG and an SQS queue.</li><li><strong>It is not used for serverless applications.</strong></li><li><strong>Terraform is an open-source infrastructure as code software tool to configure the infrastructure.</strong></li><li>Elastic Beanstalk supports the deployment of web applications from Docker containers. With Docker containers, you can define your own runtime environment. You can choose your own platform, programming language, and application dependencies that aren’t supported by other platforms.</li><li><strong>Note: Deploying Docker containers using CloudFormation is not an ideal choice.</strong></li><li>Elastic Beanstalk is an easy-to-use serivce for deploying and scaling web applications and services.</li><li>We can retain full control over the AWS resources used in the application and access the underlying resources at any time</li><li>Elastic Beanstalk vs ECS<ul><li>With ECS, you’ll have to build the infrastructure first before you can start deploying the Dockerfile</li><li>With Elastic Beanstalk, you provide a Dockerfile, and Elastic Beanstalk takes care of scaling your provisioning of the number and size of nodes.</li></ul></li><li>Elastic Beanstalk vs CloudFormtaion<ul><li>Elastic Beanstalk is intended to make developers’ lives easier</li><li>CloudFormation is intended to make systems engineers’ lives easier</li><li>CloudFormation doesn’t automatically do anything.</li></ul></li></ol><h2 id="Cognito"><a href="#Cognito" class="headerlink" title="Cognito"></a>Cognito</h2><ol><li>Cognito User Pools (CUP)<ul><li>Sign in &#x2F; Sign up functionality for app users</li><li>Integrate with API Gateway</li><li>Create a serverless database of user for your mobile apps</li><li>Simple login: Username (or email) &#x2F; password combination</li><li>Possibility to verify emails &#x2F; phone numbers and add MFA</li><li>Can enable Federated Identity (Facebook, Google, SAML, ….)</li><li>Sends back a JSON Web Token (JWT)</li><li>CUP is a IdP</li></ul></li><li>Cognito Identity Pools (Federated Identity)<ul><li>Provide AWS credentials to users so they can access resources directly</li><li>Integrate with User Pools as an identity provider</li><li>get temporary AWS credentials back from the Federated Identity Pool</li><li>These credentials come with a pre-defined IAM policy stating their permissions</li></ul></li><li>Cognito Sync<ul><li>Synchronize data fromm device to Cognito</li><li>Maybe deprecated and replaced by AppSync</li><li>Store preferences, configuration, state of app</li><li>Cross device synchronization</li><li>Requires Federated Identity Pool in Cognito (not User Pool)</li></ul></li><li>SAML: A type of Identity Provider which is used for SSO</li><li>OIDC: A type of Identity Provider which uses OAuth</li><li>tech of IdP<ul><li>SAML</li><li>SSO</li><li>OAuth</li><li>OpenID</li></ul></li><li><strong>Federated identity providers are used to authenticate users. Then the Cognito identity pool provides the temporary token that authorizes users to access AWS resources.</strong></li><li><strong>Identity Providers authenticate users, not authenticate services.</strong></li><li>Cognito supports both authenticated and unauthenticated users.</li><li>Cognito supports more than just social identity providers, including OIDC, SAML, and its own identity pools.</li></ol><h2 id="ECS"><a href="#ECS" class="headerlink" title="ECS"></a>ECS</h2><ol><li>Features of ECS<ul><li>Containers and Images</li><li>Task Definitions</li><li>Tasks and Scheduling</li><li>Clusters</li><li>Container Agent</li></ul></li><li>ECS is a container orchestration service</li><li>ECS helps you run Docker containers on EC2 instances</li><li><strong>ECS is ideal for performing batch processing, and it should scale up or down based on the number of messages in the queue.</strong></li><li>ECS is made of<ul><li>ECS Core: Running ECS on user-provisioned EC2 instances</li><li>Fargate: Running ECS tasks on AWS provisioned compute (serverless)</li><li>EKS: Running ECS on AWS-powered Kubernetes (running on EC2)</li><li>ECR: Docker Container Registery hosted by AWS</li></ul></li><li>Use Cases<ul><li>Microservices<ul><li>Direct integration with ALB</li><li>Auto scaling capability</li><li>Easy service discovery features</li><li><strong>Lambda, ECS, and API Gateway are serverless independent and easily scale up and down</strong></li></ul></li><li>Run batch processing &#x2F; scheduled tasks<ul><li>Scheduled ECS containers to run On-demand &#x2F; Reserved &#x2F; Spot instances</li></ul></li><li>Migrate application to the cloud<ul><li>Dockerize legacy applications running on premises</li><li>Move Docker containers to run on ECS</li></ul></li></ul></li><li>ALB Integration<ul><li>ALB has a direct integration feature with ECS called “port mapping”</li><li>Use Cases<ul><li>Increased resiliency even if running on one EC2 instance</li><li>Maximize utilization of CPU &#x2F; cores</li><li>Ability to perform rolling upgrades without impacting application uptime</li></ul></li></ul></li><li>IAM Task Roles<ul><li>The EC2 instances should have an IAM role allowing it to access the ECS service (for the ECS agent)</li></ul></li><li>An ECS Task is a running Docker container</li><li>ECR is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images</li><li>Service Definition</li></ol><ul><li>defines which task definition to use with your service, how many instantiations of that task to run, and which load balancers associate with your tasks</li><li>Parameters defined in Service Definition<ul><li>loadbalancers</li><li>serviceRegistries</li><li>placementConstrains</li><li>networkConfiguration</li><li>cluster</li><li>taskDefinition</li><li>role</li></ul></li></ul><ol start="12"><li>Task Definitions<ul><li>To prepare your application to run on ECS, you create a task definition. The task definition is a text file, in JSON format, that describes one or more containers, up to a maximum of ten, that form your application. It can be thought of as a blueprint for your application. Task definitions specify various parameters for your definition.</li><li>Some of parameters you can specify in a task definition<ul><li>Docker images</li><li>How much CPU and memory to use with each container</li><li>The launch type to use</li><li>Whether container are linked together in a task</li><li>The Docker networking mode to use for the container in your task</li><li>(Optional) The ports from the container to map to the host container instance</li><li>Whether the task should continue to run if the container finishes or fails</li><li>The command the container should run when it is started</li><li>(Optional) The environment variables that should be passed to the container when it starts</li><li>Any data volumes that should be used with containers in the task</li><li>(Optional) The IAM role that your tasks should use for permissions</li></ul></li></ul></li><li>Container Agent<ul><li>It runs on each infrastructure resource within an ECS cluster. It sends information about the resource’s current running tasks and resource utilization to ECS, and ECS can send start or stop request to agent.</li><li>Container instances needs a public ID address, VPC Endpoints or a NAT Gateway to communicate with ECS Service</li><li>ECS-optimized AMI looks for agent configuration data in the <strong>&#x2F;etc&#x2F;ecs&#x2F;ecs.config</strong> file when the container agent starts. You can specify this configuration data at launch with EC2 user data.</li></ul></li><li>You have root access to the operating system of your container instances</li><li>Container instances must have an IAM role to have sufficient permissions to communicate with ECS<ul><li>ex: ecs:Poll, it provides container instance with agent permission to connect with ECS service to report status and get commands</li><li>ecs: CreateCluster, provided that the cluster you intend to register your container instance into already exists. If the cluster does not already exist, the agent must have permission to create it, or you can create the cluster with the create-cluster command prior to launching your container instance.</li></ul></li><li>ECS Launch Types<ul><li>Fargate Launch Type</li><li>ECS Launch Type</li></ul></li><li>How to use<ol><li>Create a docker image of your batch processing application</li><li>Deploy the image as an ECS task</li></ol></li></ol><h2 id="Snowball-amp-Snowball-Edge-amp-Snowmobile"><a href="#Snowball-amp-Snowball-Edge-amp-Snowmobile" class="headerlink" title="Snowball &amp; Snowball Edge &amp; Snowmobile"></a>Snowball &amp; Snowball Edge &amp; Snowmobile</h2><h3 id="Snowball"><a href="#Snowball" class="headerlink" title="Snowball"></a>Snowball</h3><ol><li><p>Petabyte-scale data transfer service</p></li><li><p>It costs thousands of dollars to transfer 100TB over high speed internet, Snowball can reduce that costs by 1&#x2F;5th</p></li><li><p>It can take 100TB over 100 days to transfer over high speed internet, Snowball can reduce that transfer time by less than a week</p></li><li><p>Data is encrypted end-to-end (256-bit encryption)</p></li><li><p>Two sizes: 50TB &amp; 80TB</p></li><li><p>If it takes more than a week to transfer over the network, use Snowball instead</p><hr><h3 id="Snowball-Edge"><a href="#Snowball-Edge" class="headerlink" title="Snowball Edge"></a>Snowball Edge</h3></li><li><p>Snowball Edge add computational capability to the device</p></li><li><p>100TB capacity with either</p><ul><li>Storage optimized</li><li>Compute optimized</li></ul></li><li><p>Supports a custom EC2 AMI so you can perform processing on the go</p></li><li><p>Supports custom Lambda functions</p></li><li><p>Very useful to pre-process the data while moving</p></li><li><p>Use case</p><ul><li>Data migration</li><li>Image collation</li><li>IoT capture</li><li>Machine learning</li></ul></li><li><p>Two sizes: 100TB &amp; 100TB clustered</p><hr><p>###Snowmobile</p></li><li><p>Transfer 100PB per Snowmobile</p></li><li><p>It is a exabyte-scale migration</p></li></ol><h2 id="Redshift"><a href="#Redshift" class="headerlink" title="Redshift"></a>Redshift</h2><ol><li><p>Fully managed Petabyte-size data warehouse</p></li><li><p>Columnar Store database which can SQL-like queries and is an OLAP</p></li><li><p>load from</p><ul><li>S3</li><li>EMR</li><li>DynamoDB</li><li>multiple data sources on remote hosts</li></ul></li><li><p>Redshift can run via a single node or multi-node (clusters)</p></li><li><p>Pricing</p><ul><li>starts at just $0.25 per hour with no upfront costs or commitments</li><li>scale up to petabytes for $1000 per terabyte per year</li><li>Redshift is price is less than 1&#x2F;10 cost of most similar services</li></ul></li><li><p>Configuration</p><ul><li>Single Node<ul><li>Nodes come in sizes of 160GB.</li></ul></li><li>Multi-Node<ul><li>You can launch a cluster of nodes with Multi-Node mode</li><li>Leader Node: manages client connections and receiving queries</li><li>Compute Node: stores data and performs queries up to 128 compute nodes</li></ul></li></ul></li><li><p>Node Type and Sizes</p><ul><li>Dense Compute (DC): best for high performance, but they have less storage</li><li>Dense Storage (DS): clusters in which you have a lot of data</li></ul></li><li><p>Compression</p><ul><li>Uses multiple compression techs to achieve significant compression to traditional relational data stores</li><li>Similar data is stored sequentially on disk</li><li>Does not require indexes or materialized view, which saves a lot of space compred to traditional databases</li><li>When loading data to an empty table, data is sampled and the most appropriate compression scheme is selected automatically</li></ul></li><li><p>Processing</p><ul><li>Redshift uses Massively Parallel Processing (MPP)</li><li>Automatically distributes data and query loads acrosss all nodes</li></ul></li><li><p>Backups</p><ul><li>Backups are enabled by default with a one day retention period. Retention period can be modified up to 35 days</li><li>maintain at least 3 copies of your data<ul><li>The original copy</li><li>Replica on the compute nodes</li><li>Backup copy in S3</li></ul></li><li>Can asynchronously replicate your snapshots to S3 in a different region</li><li>Manual Snapshot<ul><li>By default, manual snapshots are retained indefinitely, even after you delete you cluster. You can specify retention period when you create a manual snapshot or change the retention period by modifying the snapshot. If you create a snapshot using Redshift console, the default retention period is 365 days.</li></ul></li><li>Automated Snapshot<ul><li>Automated snapshots are automatically deleted within the period of 1(Lest) to 35(Max) days.</li></ul></li></ul></li><li><p>Database Encryption can be applied using</p><ul><li><p>Redshift uses a hierarchy of encryption keys to encrypt the database. You can use either AWS KMS or a HSM to manage the top-level encryption keys in this hierarchy. </p></li><li><p>KMS multi-tenant HSM</p></li><li><p>CloudHSM single-tenant HSM</p></li></ul></li><li><p>Avaialbility</p><ul><li>Redshift is Single-AZ</li><li>To run in multi-AZ you would have to run multiple Redshift Cluster in different AZs with the same inputs</li><li>Snapshots can be restored to a different AZ in the event an outage occurs</li></ul></li><li><p>Database Encryption</p><ul><li>KMS multi-tenant HSM</li><li>CloudHSM single-tenant HSM</li></ul></li><li><p><strong>Redshift Enhanced VPC Routing</strong></p><ul><li>It provides VPC resources access to Redshift.</li><li>Without it, Redshift cannot be able to access the S3 VPC endpoints.</li><li>Without it, NAT instance cannot be reached by Redshift.</li></ul></li><li><p>Cross-Region Snapshots</p><ul><li>Snapshots are available for Redshift clusters enabling them to be <strong>available in different regions</strong></li></ul></li><li><p><strong>If you intend to keep your Redshift cluster running continuously for a prolonged period, you should consider purchasing reserved node offerings. But you need to pay for those nodes for either one-year or three-year duration.</strong></p></li></ol><h2 id="ElastiCache"><a href="#ElastiCache" class="headerlink" title="ElastiCache"></a>ElastiCache</h2><ol><li>Managed caching service which either runs Redis or Memcached</li><li>Helps make your application stateless</li><li>Write Scaling using sharding</li><li>Read Scaling using Read Replicas</li><li>Multi AZ with Failover Capability</li><li>ElastiCache - Redis vs Memcached<ul><li>Redis<ul><li>Multi AZ with Auto-Failover</li><li>Read Replicas to scale reads and have HA</li><li>Data Durability using Append Only File (AOF) persistence</li><li>Backup and restore features</li><li>It’s very good for leaderboards, keep track of unread notification data. It’s very fast, but arguably not as fast as Memcached</li></ul></li><li>Memcached<ul><li>Multi-node for partitioning of data (sharding)</li><li>Non persistent</li><li>No backup and restore</li><li>Multi-threaded architecture</li><li>Memcached is generally preferred for caching HTML fragments.</li></ul></li></ul></li><li>IAM policies on ElastiCache are only used for AWS API-level security</li><li>Redis AUTH<ul><li>You can set a “password&#x2F;token” when you create a Redis cluster</li><li>This is an extra level of security for your cache (on top of SG)</li><li>To use Redis AUTH that will require users to provide a password before accessing Redis Cluster, in-transit encryption needs to be enabled on the cluster while creating the cluster.</li><li><strong>For Redis AUTH, clusters must enabled with in-transit encryption during initial deployment, not at-rest encryption.</strong></li></ul></li><li>Memcached supports SASL-based authentication (advanced)</li><li>Patterns for ElastiCache<ul><li>Lazy Loading: all the read data is cached, data can become state in cache</li><li>Write Through: Adds or update data in the cache when written to a DB (no stale data)</li><li>Session Store: store temporary session data in a cache (using TTL features)</li></ul></li><li>Quote: There are only two hard things in CS<ul><li>Cache Invalidation</li><li>Naming</li></ul></li><li>ElastiCache is only accessible to resource operating with the same VPC to ensure low latency</li></ol><h2 id="Kinesis"><a href="#Kinesis" class="headerlink" title="Kinesis"></a>Kinesis</h2><ol><li>Kinesis is a managed alternative to Apache Kafka</li><li>Great for application logs, metrics, IoT, clickstreams</li><li>Great for “real-time” big data</li><li>Data is automatically replicated to 3 AZs</li><li>Components of Kinesis<ul><li>Kinesis Streams: low latency streaming ingest at scale</li><li>Kinesis Analytics: perform real-time analytics on streams using SQL</li><li>Kinesis Firehose: load streams into S3, Redshift, ElasticSearch…</li></ul></li><li>Kinesis Streams<ul><li>Streams are divided in ordered Shards &#x2F; Partitions</li><li>Data retention is 1 day by default, can go up 7 days</li><li>Ability to reprocess &#x2F; replay data</li><li>Multiple applications can consume the same stream</li><li>Real-time processing with scale of throughput because of shards</li><li>Once data is inserted in Kinesis, it can’t bee deleted (immutability)</li><li>You pay per running shard</li><li><strong>It is an ordered sequence of data records</strong></li><li><strong>stores the records from 24h up to 168h</strong></li></ul></li><li>Kinesis Streams Shards<ul><li>One stream is made of many different shards</li><li>1 MB&#x2F;s or 1000 messages&#x2F;s at write PER SHARD</li><li>2 MB&#x2F;s at read PER SHARD</li><li>Billing is per shard provisioned, can have as many shards as you want</li><li>Batching available or per message calls</li><li>The number of shards can evolve over time  (reshard &#x2F; merge)</li><li>Records are ordered per shard</li></ul></li><li>Kinesis API<ul><li>Put Records<ul><li>PutRecord API + Partition key that gets hashed</li><li>The same key goes to the same partition</li><li>Messages sent get a “sequence number”</li><li>Use Batching with PutRecords to reduce costs and increase throughput</li><li>ProvisionedThroughputExceeded if we go over the limits</li><li>Can use CLI, AWS SDK, or producer libraries from various frameworks</li></ul></li><li>Exceptions<ul><li>ProvisionedThroughputExceeded Exceptions<ul><li>Happens when sending more data (exceeding MB&#x2F;s or TPS for any shard)</li><li>Make sure you don’t have a lot shard (such as your partition key is bad and too much data goes to that partition)</li></ul></li><li>Solution<ul><li>Retries with backoff</li><li>Increase shards(scaling)</li><li>Ensure your partition key is good one</li></ul></li></ul></li><li>Customers<ul><li>Can use a normal consumer (CLI, SDK, etc…)</li><li>Can use Kinesis Client Library (in Java, Node, Python, Ruby, .Net)<ul><li>KCL uses DynamoDB to checkpoint offsets</li><li>KCL uses DynamoDB to track other workers and share the work amongst shards</li></ul></li></ul></li></ul></li><li>Kinesis Data Firehose<ul><li>Fully Managed Service, no administration, automatic scaling, serverless</li><li>Load data into<ul><li>Redshift</li><li>S3</li><li>ElasticSearch</li><li>Splunk</li></ul></li><li>Near Real Time<ul><li>60 seconds latency minimum for non full batches</li><li>Or minimum 32 MB of data at a time</li></ul></li><li>Supports many data formats, conversions, transformations, compression</li><li>Pay for the amount of data going through Firehose</li><li>Data immediatelly disappears once it’s consumed</li><li>You pay only for data that is ingested</li></ul></li><li>Kinesis Data Streams vs Firehose<ul><li>Streams<ul><li>Going to write custom code (producer &#x2F; consumer)</li><li>Real time (~200ms)</li><li>Must manage scaling (shard splitting &#x2F; merging)</li><li>Data Storage for 1 to 7 days, replay capability, multi consumers</li></ul></li><li>Firehose<ul><li>Serverless data transformation with Lambda</li><li>Near real time (lowest buffer time is 1 minute)</li><li>Automated Scaling</li><li>No data storage</li></ul></li></ul></li><li>Kinesis Analytics<ul><li>Perform real-time analytics on Kinesis Streams using SQL</li><li>Kinesis Data Analytics<ul><li>Auto Scaling</li><li>Managed: no servers to provision</li><li>Continuous: real time</li></ul></li><li>You can specify Firehose or Data Streams as an input and an output</li></ul></li><li>Ordering data into SQS<ul><li>For SQS FIFO, if you don’t use Group ID, messages are consumed in the order they are sent, with only one consumer</li><li>You want to scale the number of consumers, but you want messages to be  “grouped” when they are realted to each other, then use Group ID (similar to Partition Key in Kinesis)</li></ul></li><li>Kinesis Data Streams vs SQS FIFO<ul><li>Data Streams<ul><li>data will be ordered within each shard</li><li>The maximum amount of consumers in parallel we can have is 5</li><li>Can receive up to 5 MB&#x2F;s of data</li></ul></li><li>SQS FIFO<ul><li>You only have one SQS FIFO queue</li><li>You will have 100 Group ID</li><li>You can have up to 100 consumers (due to the 100 Group ID)</li><li>You have up to 300 messages per second (or 3000 if using batching)</li></ul></li></ul></li><li>Kinesis Video Analytics<ul><li>Output video data to ML or video processing services</li></ul></li></ol><h2 id="EMR"><a href="#EMR" class="headerlink" title="EMR"></a>EMR</h2><ol><li>EMR stands for Elastic MapReduce</li><li>EMR helps creating Hadoop clusters (Big Data) to analyze and process vast amount of data</li><li>Not for a data streaming service</li><li>The clusters can be made of hundreds of EC2 instances</li><li>Also supports Apache Spark, HBase, Presto, Flink…</li><li>EMR takes care of all the provisioning and configuration</li><li>Auto-scaling and integrated with Spot instances</li><li>Use cases: data processing, ML, web indexing, big data…</li></ol><p>##Glue</p><ol><li>Fully managed ETL (Extract, Transform &amp; Load) service</li><li>Automating time consuming steps of data preparation for analytics</li><li>Serverless, provisions Apache Spark</li><li>Crawl data sources and identifies data formats (schema interface)</li><li>Automated Code Generation</li><li>Sources<ul><li>Aurora</li><li>RDS</li><li>Redshift</li><li>S3</li></ul></li><li>Sinks<ul><li>S3</li><li>Redshift</li></ul></li><li>Glue Data Catalog: Metadata (definition &amp; schema) of the Source Tables</li><li>the table in Glue is used to define the data schema.</li><li>Crawler</li></ol><ul><li>Performing ETL jobs</li></ul><ol start="11"><li>Classifier<ul><li>Generating a schema</li></ul></li></ol><h2 id="Amazon-MQ"><a href="#Amazon-MQ" class="headerlink" title="Amazon MQ"></a>Amazon MQ</h2><ol><li>Amazon MQ &#x3D; managed Aapche Active MQ</li><li>Amazon MQ doesn’t “scale” as much as SQS&#x2F;SNS</li><li>Amazon MQ runs on a dedicated machine, can run in HA with failover</li><li>Amazon MQ both has queue feature (SQS) and topic feature (SNS)</li></ol><h2 id="SQS-vs-SNS-vs-Kinesis"><a href="#SQS-vs-SNS-vs-Kinesis" class="headerlink" title="SQS vs SNS vs Kinesis"></a>SQS vs SNS vs Kinesis</h2><p>###SQS</p><ul><li>Consumer pull data</li><li>Data is deleted after being consumed</li><li>Can have as many consumers as we want</li><li>No need to provision throughput</li><li>No ordering guarantee (except FIFO features)</li><li>Individual message delay capability</li></ul><p>###SNS</p><ul><li>Push data to many subscribers</li><li>Up to 10,000,000 subscribers</li><li>Data is not persisted (lost if not delivered)</li><li>Pub&#x2F;Sub</li><li>Up to 100,000 topics</li><li>No need to provision throughput</li><li>Integrates with SQS for fan-out architecture pattern</li></ul><p>###Kinesis</p><ul><li>Consumers pull data</li><li>As many consumers as we want</li><li>Possibility to replay data</li><li>Meant for real-time big data, analytics and ETL</li><li>Ordering at the shard level</li><li>Data expires after X days</li><li>Must provision throughput</li></ul><h2 id="Resource-Groups-same-with-Application-Manager"><a href="#Resource-Groups-same-with-Application-Manager" class="headerlink" title="Resource Groups (same with Application Manager)"></a>Resource Groups (same with Application Manager)</h2><ol><li>Create, view or manage logical group of resources thanks to tags</li><li>Allows creation of logical groups of resources<ul><li>Applications</li><li>Different layers of an application stack</li><li>Production vs development environments</li></ul></li><li>Regional service</li><li>Works with EC2, S3, DynamoDB, Lambda</li><li>Group Type<ul><li>Tag based</li><li>CloudFormation stack based</li></ul></li></ol><p>##Resource Groups - Tags</p><ol><li>Free naming, common tags are<ul><li>Name</li><li>Environment</li><li>Team</li></ul></li><li>Used for<ul><li>Resources grouping</li><li>Automation</li><li>Cost allocati</li></ul></li><li>Better to have too many tags than too few</li><li>You can easily add tags to define which instances are the production instances and which ones are development instances. These tags can be used while controlling access via an IAM policy</li></ol><p>##Resource Access Manager (RAM)</p><ol><li>Share AWS resources that you own with other AWS accounts</li><li>Share with any account or within your organization</li><li>Avoid resource duplication</li><li>Share<ul><li>VPC Subnets<ul><li>Allow to have all the resources launched in the same subnets</li><li>must be from the same AWS Organization</li><li>Cannot share SG and default VPC</li><li>Participants can manage their own resources in there</li><li>Participants can’t view, modify, delete resources that belong to other participants or the owner</li><li>Share VPC, but not resources among it</li></ul></li><li>AWS Transit Gateway</li><li>Route 53 Resolver Rules</li><li>License Manager Configuration</li></ul></li><li><strong>In RAM, you should directly share the resource to the AWS Organization rather than all the AWS accounts in RAM</strong></li></ol><h2 id="Storage-Gateway"><a href="#Storage-Gateway" class="headerlink" title="Storage Gateway"></a>Storage Gateway</h2><ol><li>Hybrid Cloud for Storage</li><li>AWS Storage Cloud Native Options<ul><li>Block<ul><li>EBS</li><li>EC2 Instance Store</li></ul></li><li>File<ul><li>EFS</li></ul></li><li>Object<ul><li>S3</li><li>Glacier</li></ul></li></ul></li><li>It’s a bridge between on-premise data and cloud data in S3</li><li>Use case: DR, backup &amp; restore, tiered storage</li><li>3 types of Gateway<ul><li>File Gateway (NFS)<ul><li><strong>store your files in S3</strong></li><li><strong>Access your files through a NFS or SMB mount point</strong></li><li>Supports S3 standard, S3 IA, S3 One Zone IA</li><li>Bucket access using IAM roles for each File Gateway</li><li>Most recently used data is cached in the file gateway</li><li>Can be mounted on many servers</li><li>Once a file is transferred to S3, it can be managed as a native S3 object</li><li>Bucket Policies, Versioning, Lifecycle Management, and CRR apply directly to objects stored in your bucket</li></ul></li><li>Volume Gateway (iSCSI)<ul><li>Volume Gateway presents your applications with disk volumes usingn the Internet Small Computer System Interface (iSCSI) block protocol</li><li>Data that is written to volumes can be asynchronously backed up as point-in-time snapshots of the volumes, and stored in the cloud as AWS EBS Snapshots</li><li>All snapshots storage is also compressed to help minimize your storage charges</li><li>store copies of your hard disk drives in S3</li><li>store as EBS</li><li><strong>Stored Volumes</strong><ul><li>Primary data is stored locally, while asychronously backing up that data to AWS</li><li>Provide your on-premises application with low-latency access to their entire datasets, while still providing durable off-site backups</li><li>Stored Volumes can be between 1GB - 16TB in size</li></ul></li><li><strong>Cached Volumes</strong><ul><li>Low latency access to most recent data</li><li>Let you use S3 as your primary data storage, while retaining frequently accessed data locally in your data storage</li><li><strong>Minimize the need to scale your on-premises storage infrastructure, while still providing your applications with low latency data access</strong></li><li>Create storage volumes up to 32TB in size and attach them as iSCSI devices from your on-premises servers</li></ul></li></ul></li><li>Tape Gateway (VTL)<ul><li>Backups virtual tapes to S3 Glacier for long archive storage</li><li>A durable, cost-effective solution to archive your data in the AWS Cloud</li><li>Supported by NetBackup, Backup Exec, and Veeam</li></ul></li></ul></li><li><strong>All three storage gateway patterns are backed by S3</strong></li><li>On-premises data to the cloud &#x3D;&gt; Storage Gateway</li><li>File access &#x2F; NFS &#x3D;&gt; File Gateway (backed by S3)</li><li>Volumes &#x2F; Block Storage &#x2F; iSCSI &#x3D;&gt; Volume Gateway (backed by S3 with EBS snapshots)</li><li>VTL Tape solution &#x2F; Backup with iSCSI &#x3D;&gt; Tape Gateway (backed by S3 and Glacier)</li></ol><h2 id="FSx"><a href="#FSx" class="headerlink" title="FSx"></a>FSx</h2><ol><li><p>Amazon FSx for Windows (File Server)</p><ul><li><strong>EFS is shared POSIX system for Linux systems, FSx for Windows is not POSIX-compliant file system</strong></li><li>POSIX stands for Portable Operating System Interface</li><li>FSx for Windows is a fully managed Windows file system share drive</li><li>Supports SMB protocol &amp; Windows NTFS</li><li>Microsoft Active Directory integration, ACLs, user quotas</li><li>Built on SSD, scale up to 10s of GB&#x2F;s, millions of IOPS, 100s PB of data</li><li>Can be accessed from your on-premise infrastructure</li><li>Can be configured to be Multi-AZ (HA)</li><li>Data is backed-up daily to S3</li><li><strong>for enterprise workloads</strong></li><li><strong>FSx for Windows File Server supports across VPCs, accounts, and Regions via Direct Connect or VPN (on-premises) and VPC Peering or AWS Transit Gateway</strong></li></ul></li><li><p>Amazon FSx for Lustre </p><ul><li><p>FSx for Lustre is a POSIX-compliant file server.</p></li><li><p>Lustre is a type of parallel distributed file system, for large-scale computing</p></li><li><p>The name Lustre is derived from “Linux” andn “Cluster”</p></li><li><p>ML, HPC, Video Processing, Financial Modeling, Electronic Design Automation</p></li><li><p>Scales up to 100s GB&#x2F;s, millions of IOPS, sub-ms latencies</p></li><li><p>Seamless integration with S3</p><ul><li>Can “read S3” as a file system (through FSx)</li><li>Can write the output of the computations back to S3 (through FSx)</li></ul></li><li><p>Can be used from on-premises servers</p></li><li><p><strong>for high-performance workloads</strong></p></li></ul></li></ol><h2 id="Storage-Comparison"><a href="#Storage-Comparison" class="headerlink" title="Storage Comparison"></a>Storage Comparison</h2><ul><li>S3: Object Storage</li><li>Glacier: Object Archival</li><li>EFS: Network File System for Linux instances, POSIX file system</li><li>FSx for Windows: Network File System for Windows servers</li><li>FSx for Lustre: High Performance Computing Linux file system</li><li>EBS volumes: Network storage for one EC2 instance at a time</li><li>Instance Storage: Physical storage for your EC2 instance (high IOPS)</li><li>Storage Gateway: File Gateway, Volume Gateway (cached &amp; stored), Tape Gateway</li><li>Snowball &#x2F; Snowmobile: to move large amount of data to the cloud, physically</li><li>Database: for specific workloads, usually with indexing and querying</li></ul><h2 id="Database-Comparison"><a href="#Database-Comparison" class="headerlink" title="Database Comparison"></a>Database Comparison</h2><p>Database Types</p><ul><li>RDBMS (&#x3D;SQL&#x2F;OLTP): RDS, Aurora - great for joins</li><li>NoSQL database: DynamoDB (~JSON), ElastiCache (key&#x2F;value pairs), Neptune (graphs) - no joins, no SQL</li><li>Object Store: S3 (for big objects) &#x2F; Glacier (for backups &#x2F; archives)</li><li>Data Warehouse (&#x3D;SQL Analytics &#x2F; BI): Redshift (OLAP), Athena</li><li>Search: ElasticSearch (JSON) - free text, unstructured searches</li><li>Graphs: Neptune - displays relationships between data</li></ul><p>RDS</p><ul><li>Must provision an EC2 instance &amp; EBS Volume type and size</li><li>Support for Read Replicas and Multi AZ</li><li>Security through IAM, SG, KMS, SSL in transit</li><li>Backup &#x2F; Snapshot &#x2F; Point in time restore feature</li><li>Managed and Scheduled maintenance</li><li>Monitoring through CloudWatch</li><li>Operations: small downtime when failover happens, when maintenance happens, scaling in read replicas &#x2F; ec2 instance &#x2F; restore EBS implies manual intervention, application changes</li><li>Security: KMS, SG, IAM policies, SSL in transit</li><li>Reliability: Multi AZ, failover in case of failures</li><li>Performance: depends on EC2 instance type, EBS volume type, ability to add Read Replicas, doesn’t auto-scale</li><li>Cost: Pay per hour based on provisioned EC2 and EBS</li></ul><p>Aurora</p><ul><li>Data is held in 6 replicas, across 3 AZ</li><li>Auto healing capability</li><li>Multi AZ, Auto Scaling Read Replicas</li><li>Read Replicas can be Global</li><li>Aurora database can be Global for DR or latency purposes</li><li>Auto scaling of storage from 10GB to 64TB</li><li>Define EC2 instance type for aurora instances</li><li>Aurora Serverless option</li><li>Operations: less operation, auto scaling storage</li><li>Security: same with RDS</li><li>Reliability: Multi AZ, highly available, possibly more than RDS, Aurora Serverless option</li><li>Performance: 5x performance due to architectural optimizations. Up to 15 Read Replicas (only 5 for RDS)</li><li>Cost: Pay per hour based on EC2 and storage usage. Possibly lower costs compared to Enterprise grade databases sucha as Oracle</li></ul><p>ElastiCache</p><ul><li>Managed Redis &#x2F; Memcached </li><li>In-memory data store, sub-millionsecond latency</li><li>Must provision an EC2 instance type</li><li>Support for Clustering (Redis) and Multi AZ, Read Replicas (sharding)</li><li>Security through IAM, SG, KMS, Redis Auth</li><li>Backup &#x2F; Snapshot &#x2F; Point in time restore feature</li><li>Managed and Scheduled maintenance</li><li>Monitoring through CloudWatch</li><li>Operations: same as RDS</li><li>Security: same with RDS, but can also use Redis Auth</li><li>Reliability: Clustering, Multi AZ</li><li>Performance: Sub-millisecond performance, in memory, read replicas for sharing, very popular cache option</li><li>Cost: Pay per hour based on EC2 and storage usage</li></ul><p>DynamoDB</p><ul><li>NoSQL database</li><li>Serverless, provisioned capability, auto scaling, on demand capability</li><li>Can replace ElastiCache as a key&#x2F;value store (storing session data for example)</li><li>HA, Multi AZ by default, Read and Writes are decoupled, DAX (DynamoDB Accelerator) for read cache</li><li>Reads can be eventually consistent or strongly consistent</li><li>DynamoDB Streams to integrate with AWS Lambda</li><li>Backup&#x2F;Restore feature, Global Table feature</li><li>Monitoring through CloudWatch</li><li>Can only query on primary key, sort key, or indexes</li><li>Operations: no operations needed, auto scaling capability, serverless</li><li>Security: full security through IAM policies, KMS encryption, SSL in flight</li><li>Reliability: Multi AZ, Backups</li><li>Performance: single digit millisecond performance, DAX for caching reads, performance doesn’t degrade if your application scales</li><li>Cost: Pay per provisioned capability and storage usage (no need to guess in advance any capacity - can use auto scaling)</li></ul><p>S3</p><ul><li>Great for big objects</li><li>Serverless, scales infinitely, max object size is 5TB</li><li>Eventually consistency for overwrites and deletes</li><li>Tiers: S3 Standard, S3 IA, S3 One Zone IA, Glacier for backups</li><li>Features: Versioning, Encryption, CRR</li><li>Encryption: SSE-S3, SSE-KMS, SSE-C, client side encryption, SSL in transit</li><li>Operations: no operation needed</li><li>Security: IAM, Bucket Policies, ACL, Encryption, SSL</li><li>Reliability, 11 9’s durability and 4 9’s availability, Multi AZ, CRR</li><li>Performance: scales to thousands of reads&#x2F;writes per second, transfer acceleration &#x2F; multipart for big files</li><li>Cost: pay per storage usage, network cost, requests number</li></ul><p>Athena</p><ul><li>Fully Serverless database with SQL capabilities</li><li>Used to query data in S3</li><li>Pay per query</li><li>Output results back to S3</li><li>Secured through IAM</li><li>Operations: no operations needed, serverless</li><li>Security: IAM + S3 security</li><li>Reliability: mannaged service, uses Presto engine, highly available</li><li>Performance: queries scale based on data size</li><li>Cost: pay per query &#x2F; per TB of data scanned, serverless</li></ul><p>Redshift</p><ul><li>Redshift is based on PosgreSQL, but it’s not used for OLTP, it’s OLAP</li><li>10x better performance than other data warehouses, scale to PBs of data</li><li>Columnar storage of data</li><li>MPP, highly available</li><li>Pay as you go based on the instances provisioned</li><li>Has a SQL interface for performing the queries</li><li>BI tools such as AWS Quicksight or Tableau integrate with it</li><li>From 1 node to 128 nodes, up to 160GB of space per node</li><li>Two type of nodes<ul><li>Leader Node: for query planning, results aggregation</li><li>Compute Node: for performing the queries, send results to leader</li></ul></li><li>Redshift Spectrum: perform queries directly against S3 (no need to load)</li><li>Backup &amp; Restore, Security VPC &#x2F; IAM &#x2F; KMS, Monitoring</li><li>Redshift Enhanced VPC Routing: COPY &#x2F; UNLOAD goes through VPC</li><li>Redshift - Snapshots &amp; DR<ul><li>Snapshots are point-in-time backups of a cluster, stored internally in S3</li><li>Snapshots are incremental (only what has changed is saved)</li><li>You can restore a snapshots into a new cluster</li><li>Automated: every 8 hours, every 5 GB, or on a schedule. set retention</li><li>Manual: snapshot is retained untile you delete it</li><li>You can configure Amazon Redshift to automatically copy snapshots of a cluster to another AWS region</li></ul></li><li>Redshift Spectrum<ul><li>Query data that is already in S3 without loading it</li><li>Must have a Redshift cluster available to start the query</li><li>The query is then submitted to thousands of Redshift Spectrum nodes</li></ul></li><li>Operations: similar to RDS</li><li>Security: IAM, VPC, KMS, SSL</li><li>Reliability: highly available, auto healing features</li><li>Performance: 10x performance vs other data warehousing, compression</li><li>Cost: pay per node provisioned, 1&#x2F;10th of the cost vs other warehouses</li><li>Remember: Redshift &#x3D; Analytics &#x2F; BI &#x2F; Data Warehouse</li></ul><p>Neptune</p><ul><li>fully managed graph database</li><li>When do we use Graph?<ul><li>High relationship data</li><li>Social Networking</li><li>Knowledge graphs</li></ul></li><li>High available across 3 AZ, with up to 15 read replicas</li><li>Point-in-time recovery, continuous backup to S3</li><li>Support for KMS encryption at rest + HTTPS</li><li>Operations: similar to RDS</li><li>Security: IAM, VPC, KMS, SSL + IAM Authentication</li><li>Reliability: Multi AZ, clustering</li><li>Performance: best suited for graphs, clustering to improve performance</li><li>Cost: pay per node provisioned</li><li>Remember: Neptune &#x3D; Graphs</li></ul><p>ElasticSearch</p><ul><li>Search any field, even partially matches</li><li>It’s common to use ElasticSearch as a complement to another database</li><li>You can provision a cluster of instances</li><li>Built-in integrations: Amazon Kinesis Data Firehose, AWS IoT, and Amazon CloudWatch Logs for data ingestion</li><li>Comes with ELK stack</li><li>Operations: similar to RDS</li><li>Security: Cognito, IAM, VPC, KMS, SSL</li><li>Reliability: Multi AZ, clustering</li><li>Performance: based on ElasticSearch project, petabyte scale</li><li>Cost: pay per node provisioned</li><li>Remember: ElasticSearch &#x3D; Search &#x2F; Indexing</li></ul><h2 id="Config"><a href="#Config" class="headerlink" title="Config"></a>Config</h2><ol><li>Helps with auditing and recording compliance of your AWS resources</li><li>Helps record configurations and changes over time</li><li>Possibility of storing the configuration data in S3 (analyzed by Athena)</li><li>Questions that can be solved by AWS Config<ul><li>Is there unrestricted SSH access to my SG?</li><li>Do my buckets have any public access?</li><li>How has my ALB configuration changed over time?</li></ul></li><li>You can receive alerts for any changes</li><li>AWS Config is a region service</li><li>Can be aggregated across regions and accounts</li><li>AWS Config Resource<ul><li>View compliance of a resource over time</li><li>View configuration of a resource over time</li><li>View CloudTrail API calls if enabled</li></ul></li><li>AWS Config Rules<ul><li>Can use AWS managed config rules</li><li>Can make custom config rules (must be defined in AWS Lambda)<ul><li>Evaluate if each EBS disk is of type gp2</li><li>Evaluate if each EC2 instance is t2.micro</li></ul></li><li>Rules can be evaluated &#x2F; triggered<ul><li>For each config change</li><li>And &#x2F; Or: at regular time intervals</li><li>Can trigger CloudWatch Events if the rule is non-compliant (and chain with Lambda)</li></ul></li><li>Rules can have auto remediations<ul><li>If a resource is not compliant, you can trigger an auto remediation</li><li>Ex: stop instances with non-approved tags</li></ul></li><li>AWS Config Rules does not prevent actions from happening (no deny)</li><li>Pricing: no free tier, $2 per active rule per region per month</li></ul></li></ol><h2 id="CloudWatch-vs-CloudTrail-vs-Config"><a href="#CloudWatch-vs-CloudTrail-vs-Config" class="headerlink" title="CloudWatch vs CloudTrail vs Config"></a>CloudWatch vs CloudTrail vs Config</h2><p>###CloudWatch</p><ul><li>Performance monitoring (metrics, CPU, network, etc…) &amp; dashboards</li><li>Events &amp; Alerting</li><li>Log Aggregation &amp; Analysis</li></ul><p>###CloudTrail</p><ul><li>Record API calls made within your Account by everyone</li><li>Can define trails for specific resources</li><li><strong>Global Service</strong></li></ul><p>###Config</p><ul><li>Record configuration changes</li><li>Evaluate resources against compliance rules</li><li>Get timeline of changes and compliance</li></ul><p>ex: For an ELB</p><ul><li>CloudWatch<ul><li>Monitoring incoming connection metric</li><li>Visulize error codes as a % over time</li><li>Make a dashboard to get an idea of your load balancer performance</li></ul></li><li>Config<ul><li>Track SG rules for the LB</li><li>Track configuration changes for the LB</li><li>Ensure an SSL certificate is always assigned to the LB (compliance)</li></ul></li><li>CloudTrail<ul><li>Tack who made any changes to the LB with API calls</li></ul></li></ul><h2 id="SSM-Parameter-Store"><a href="#SSM-Parameter-Store" class="headerlink" title="SSM - Parameter Store"></a>SSM - Parameter Store</h2><ol><li>Secure storage for configuration and secrets</li><li>Optional Seamless Encryption using KMS</li><li>Serverless, scalable, durable, easy SDK</li><li>Version tracking of configuration &#x2F; secrets</li><li>Configuration management using path &amp; IAM</li><li>Notification with CloudWatch Events</li><li>Integration with CloudFormation</li><li>Parameter Policies (only for advanced parameters)<ul><li>Allow to assign a TTL to a parameter (expiration date) to force updating or deleting sensitive data such as password</li><li>Can assign multiple policies at a time</li></ul></li><li>Using CLI&#x2F;SDK or Lambda to require a parameter from Parameter Store</li></ol><h2 id="Secrets-Manager"><a href="#Secrets-Manager" class="headerlink" title="Secrets Manager"></a>Secrets Manager</h2><ol><li>Newer service, meant for storing secrets</li><li>Capability to force rotation of secrets every X days</li><li>Automate generation of secrets on rotation (use Lambda)</li><li>Integration with Amazon RDS (MySQL, PostgreSQL, Aurora)</li><li>supports the key rotation for database credentials, third-party services, etc.</li><li>Natively knows how to rotate secrets for supported databases such as RDS. For other secret types, such as API keys, users need to <strong>cutomize the Lambda rotation function.</strong></li><li>Secrets are encrypted using KMS</li><li>Mostly meant for RDS integration</li><li><strong>In each application, only one secret in Secrets Manager is required, and the application should always get the latest version of the secret.</strong></li><li>there is no configuration to enable rotation for all secrets. The rotation is managed in each secret.</li><li>CloudWatch must use Lambda function to check if rotation is enabled</li><li>AWS Config rule “secretsmanager-rotation-enabled-check” checks whether AWS Secrets Manager secret has rotation enabled. Users need to add the rule in AWS Config and set up a notification.</li></ol><h2 id="CloudHSM"><a href="#CloudHSM" class="headerlink" title="CloudHSM"></a>CloudHSM</h2><ol><li>KMS &#x3D; AWS manages the software for encryption</li><li>CloudHSM &#x3D; AWS provisions encryption hardware</li><li>Dedicated Hardware (HSM &#x3D; Hardware Security Module)</li><li>You manage your own encryption keys entirely (not AWS)</li><li>HSM device is tamper resistant</li><li>CloudHSM clusters are spread across Multi AZ (HA) - must setup</li><li>Supports both symmetric and asymmetric encryption (SSL&#x2F;TLS keys)</li><li>No free tier available</li><li>Must use the CloudHSM Client Software</li><li>Redshift supports CloudHSM for database encryption and key management</li><li>Good option to use with SSE-C encryption</li><li>CloudHSM Software is not within the AWS console</li><li>Backup<ul><li>To back up the CloudHSM data to S3 buckets in the same region, CloudHSM generates a unique Ephemeral Backup Key (EBK) to encrypt all data using AES 256-bit encryption key. This Ephemeral Backup Key is further encrypted using Persistent Backup Key (PBK), which is also an AES 256-bit encryption key.</li><li>backup CloudHSM data -&gt; using EBK encrypt data -&gt; using PBK encrypt EBK key</li></ul></li></ol><h2 id="Shield"><a href="#Shield" class="headerlink" title="Shield"></a>Shield</h2><ul><li>It is a service to protect web applications against DDoS attacks</li></ul><p>###AWS Shield Standard</p><ul><li>Free service that is activated for every AWS customer</li><li>Provides protection from attacks such as SYN&#x2F;UDP Floods, Reflection attacks and other layer 3 &#x2F; layer 4 attacks</li></ul><p>###AWS Shield Advanced</p><ul><li>Optional DDoS mitigation service ($3,000 per month per organization)</li><li>Protect against more sophisticated attack on Amazon EC2, ELB, CloudFront, Global Accelerator, and Route 53</li><li>24&#x2F;7 access to AWS DDoS response team (DRP)</li><li>Protect against higher fees during usage spikes due to DDoS</li></ul><h2 id="WAF"><a href="#WAF" class="headerlink" title="WAF"></a>WAF</h2><ol><li>WAF stands for Web Application Firewall</li><li>Protect your web application from common web exploits (Layer 7)</li><li>Deploy on ALB, API Gateway, CloudFront</li><li>Define Web ACL (Web Access Control List)<ul><li>Rules can include: IP, HTTP header, HTTP body, or URI strings</li><li>Protects from common attack - SQL injection and Cross-Site Scripting (XSS)</li><li>Size constraints, geo-match (block countries)</li><li>Rate-based rules (to count occurrences of events) - for DDoS protection</li></ul></li><li>AWS Firewall Manager<ul><li>Manage rules in all accounts of an AWS Organization</li><li>Common set of security rules</li><li>WAF rules (ALB, API Gateways, CloudFront)</li><li>AWS Shield Advanced (ALB, CLB, Elastic IP, CloudFront)</li><li>SG for EC2 and ENI resources in VPC</li><li>AWS provides the PHP protection rule in WAF. Users can add the rule in Web ACL.</li></ul></li></ol><h2 id="Encryption"><a href="#Encryption" class="headerlink" title="Encryption"></a>Encryption</h2><ul><li>Encryption in flight (SSL)<ul><li>SSL certificates help with encryption (HTTPS)</li><li>Encryption in flight ensures no MITM (man in the middle attack)</li></ul></li><li>Server side encryption at rest</li><li>Client side encryption<ul><li>Data is encrypted by the client and never decrypted by the server</li><li>Data will be decrypted by a receiving client</li><li>The server should not be able to decrypt the data</li><li>Could leverage Envelope Encryption</li></ul></li></ul><h2 id="STS"><a href="#STS" class="headerlink" title="STS"></a>STS</h2><ol><li><p>STS stands for Security Token Service</p></li><li><p>Allows you to grant limited and temporary access to AWS resources</p></li><li><p>Token is valid for up to one hour (must be refreshed)</p></li><li><p>APIs</p><ul><li><p>AssumeRole</p><ul><li>Within your own account: for enhanced security</li><li>Cross Account Access: assume role in target account to perform actions there</li></ul></li><li><p>AssumeRoleWithSAML</p><ul><li>return credentials for users logged with SAML</li></ul></li><li><p>AssumeRoleWithWebIdentity</p><ul><li>return credentials for users logged with an IdP (Facebook Login, Google Login, OIDC compatible…)</li></ul></li><li><p>GetFederationToken</p><ul><li>For IAM user or AWS account root user</li><li>the permission of GetFederationToken<ul><li>AWS allows the federated user’s request only when both the attacheed policy and the IAM user policy explicitly allow the federated user to perform the requested action.</li><li>You can generated FederatedUser credentials using an IAM User, not using an IAM Role</li><li>You must call the GetFederationToken operation using the long-term security credentials of an IAM user.</li></ul></li></ul></li><li><p>GetSessionToken</p><ul><li><strong>for MFA, from a user or AWS account root user</strong></li></ul></li></ul></li><li><p>Using STS to Assume a Role</p><ul><li>Define an IAM Role within your account or cross-account</li><li>Define which principals can access this IAM Role</li><li>User AWS STS to retrieve credentials and impersonate the IAM Role you have access to (AssumeRole API)</li><li>Temporary credentials can be valid between 15 minutes to 1 hour</li></ul></li><li><p>Identity Federation in AWS</p><ul><li>Federation lets users outside of AWS to assume temporary role for accessing AWS resources</li><li>These users assume identity provided access role</li><li>Federation can have many flavors<ul><li>SAML 2.0<ul><li>To integrate Active Directory &#x2F; ADFS with AWS (or any SAML 2.0)</li><li>Provides access to AWS Console or CLI (through temporary credentials)</li><li>No need to create an IAM user for each of your employees</li><li>Needs to setup a trust between AWS IAM and SAML (both ways)</li><li>SAML 2.0 enables web-based, cross domain SSO</li><li><strong>Uses the STS API: AssumeRoleWithSAML</strong></li><li>Note federation through SAML is the “old way” of doing things</li><li>Amazon SSO Federation is the new managed and simpler way</li></ul></li><li>Custom Identity Broker<ul><li>Use only identity provider is not compatible with SAML 2.0</li><li><strong>The identity broker must determine the appropriate IAM policy</strong></li><li><strong>Uses STS API: AssumeRole or GetFederationToken</strong></li><li>Not our application talks with STS, but the IdP does</li></ul></li><li>Web Identity Federation with Cognito<ul><li>Goal<ul><li>Provide direct access to AWS Resources from the Client Side (mobile, web app)</li></ul></li><li>Example<ul><li>Provide temporary access to write to S3 bucket using Facebook Login</li></ul></li><li>Problem<ul><li>We don’t want to create IAM users for our app users</li></ul></li><li>How<ul><li>Log into federation identity provider - or remain anonymous</li><li><strong>Get temporary AWS credential back from the Federation Identity Pool</strong></li><li>These credentials come with pre-defined IAM Policy stating their permissions</li></ul></li></ul></li><li>Web Identity Federation without Cognito<ul><li><strong>Using AssumeRoleWithWebIdentity</strong></li><li>Not recommended by AWS - use Cognito instead (allows for anonymous users, data synchronization, MFA)</li></ul></li><li>SSO</li><li>Non-SAML with AWS Microsoft AD<ul><li>Found on any Windows Server with AD Domain Service</li><li>Database of objects: User Accounts, Computers, Printer, File  Shares, Security Groups</li><li>Centralized security management, create account, assign permissions</li><li>Objects are organized in trees</li><li>A group of trees is a forest</li><li><strong>AWS Directory Services</strong></li></ul></li></ul></li><li>Using federation, you don’t need to create IAM users</li></ul></li></ol><h2 id="Directory-Service"><a href="#Directory-Service" class="headerlink" title="Directory Service"></a>Directory Service</h2><ol><li>It enables your end-users to use their existing corporate credentials while accessing AWS applications. Once you’ve been able to consolidate services to AWS, you won’t have to create new credentials. Instead, you’ll be able to allow the users to use their existing username&#x2F;password.</li><li><strong>for users who wants to use existing Microsoft AD or LDAP-aware applications in the cloud.</strong></li><li><strong>It can also be used to support Linux workloads that need LDAP service.</strong></li><li>A way to create AD on AWS</li><li>3 types<ul><li><strong>AWS Managed Microsoft AD</strong><ul><li>Create your own AD in AWS, manage users locally, supports MFA</li><li>Establish “trust” connection with your on-premise AD</li><li><strong>it can be used for both cloud and on-premises environments (must implement VPN or Direct Connect)</strong></li></ul></li><li><strong>AD Connector</strong><ul><li>Directory Gateway (proxy) to redirect to on-premise AD</li><li>Users are managed on the on-premise AD</li></ul></li><li><strong>Simple AD</strong><ul><li>AD-compatible managed directory on AWS</li><li>Cannot be joined with on-premise AD</li><li>don’t have on-premise AD stuff</li></ul></li></ul></li><li>LDAP<ul><li>LDAP (Lightweight Directory Access Protocol) is an open and cross platform protocol used for directory services authentication.</li></ul></li><li>LDAP vs Active Directory<ul><li><strong>LDAP is a protocol</strong> that many different directory services and access management solutions can understand.</li><li><strong>Active Directory is a directory server that uses the LDAP protocol</strong></li></ul></li></ol><h2 id="Single-Sign-On-Servicee-SSO"><a href="#Single-Sign-On-Servicee-SSO" class="headerlink" title="Single Sign On Servicee (SSO)"></a>Single Sign On Servicee (SSO)</h2><ol><li>Centrally manage Single Sign-On to access multiple accounts and 3rd-party business application</li><li>Integrated with AWS Organizations</li><li>Supports SAML 2.0 markup</li><li>Integration with on-premise Active Directory</li><li>Centralized permission management</li><li>Centralized auditing with CloudTrail</li><li>AWS SSO provides login portal</li><li>SSO vs AssumeRoleWithSAML<ul><li>AssumeRoleWithSAML<ul><li>You need to create a portal site integrated with Identity Store</li><li>You need to connect to STS for requiring token</li></ul></li><li>SSO<ul><li>You don’t need to create a portal site that already exists in SSO</li><li>You don’t need to connect to STS for requiring token</li></ul></li></ul></li></ol><h2 id="Data-Migration-Service-DMS"><a href="#Data-Migration-Service-DMS" class="headerlink" title="Data Migration Service (DMS)"></a>Data Migration Service (DMS)</h2><ol start="2"><li>Quickly and securely migrate databases to AWS, resilient, self healing</li><li>The source database remains available during the migration</li><li>Supports<ul><li>Homogeneous migration: Oracle to Oracle</li><li>Heterogeneous migration: Microsoft SQL Server to Aurora</li></ul></li><li>Continuous Data Replication usingn CDC</li><li>You must create an EC2 instance to perform the replication tasks</li><li>DMS Sources and Targets<ul><li>Sources<ul><li>On-premise and EC2 instance databases</li><li>Azure</li><li>Amazon RDS</li><li>Amazon S3</li></ul></li><li>Targetes<ul><li>On-premise and EC2 instance databases</li><li>Amazon RDS</li><li>Amazon Redshift</li><li>Amazon DynamoDB</li><li>Amazon S3</li><li>ElasticSearch </li><li>Kinesis Data Streams</li><li>DocumentDB</li></ul></li></ul></li><li>Schema Conversion Tool &amp; Engine Conversion Tool<ul><li>Schema Conversion Tool (SCT)<ul><li>For heterogenous conversion</li><li>Convert your database’s schema from one engine to another</li><li>You don’t need to use SCT if you are migrating the same DB engine</li></ul></li><li>Engine Conversion Tool<ul><li>For homogenous database migration</li></ul></li></ul></li></ol><h2 id="DataSync"><a href="#DataSync" class="headerlink" title="DataSync"></a>DataSync</h2><ol><li>Move large amount of data from on-premise to AWS</li><li>Can synchroninze to: <strong>S3, EFS, FSx for Windows</strong></li><li><strong>It is not supported FSx for Lustre</strong></li><li>Move data from your NAS or file system via NFS or SMB</li><li>Replication tasks can be scheduled hourly, daily, weekly</li><li>Leverage the DataSync agent to connect to your system</li><li><strong>Transferring a constantly changing dataset between on-premise servers &amp; EFS using AWS DataSync, you could initially uncheck Enable verification, because files at the source are slightly different from files at the destination. You can enable the verification during the final cut-over from on-premises to AWS</strong></li></ol><h2 id="AppSync"><a href="#AppSync" class="headerlink" title="AppSync"></a>AppSync</h2><ol><li>Store and sync data across mobile and web apps in real-time</li><li>Makes use of GraphQL (mobile technology from Facebook)</li><li>Client Code can be generated automatically</li><li>Integrations with DynamoDB &#x2F; Lambda</li><li>Real-time subscriptions</li><li>Offline data synchronization (replaces Cognito Sync)</li><li>Fine Grained Security</li></ol><h2 id="Transferring-large-amount-of-data-into-AWS"><a href="#Transferring-large-amount-of-data-into-AWS" class="headerlink" title="Transferring large amount of data into AWS"></a>Transferring large amount of data into AWS</h2><p>Example: transfer 200TB of data in the cloud. We have a 100Mbps internet connection.</p><ul><li>Over the internet &#x2F; Site-to-Site VPN<ul><li>Immediate to setup</li><li>Will take 200(TB)*1000(GB)*1000(MB)*8(Mb)&#x2F;100Mbps &#x3D; 16,000,000s &#x3D; 185d</li></ul></li><li>Over Direct Connect 1Gbps<ul><li>Long for one-time setup (over one month)</li><li>Will take 200(TB)*1000(GB)*8(Mb)&#x2F;1 Gbps &#x3D; 1,600,000s &#x3D; 18.5d</li></ul></li><li>Over Snowball<ul><li>Will take 2 to 3 snowball in parallel</li><li>Takes about 1 week for the end-to-end transfer</li><li>Can be combined with DMS</li></ul></li><li>For on-goning replication &#x2F; transfers: Site-to-Site VPN or DX with DMS or DataSync</li></ul><h2 id="AWS-SAM"><a href="#AWS-SAM" class="headerlink" title="AWS SAM"></a>AWS SAM</h2><ol><li>SAM &#x3D; Serverless Application Model</li><li>Framwork for developing and developing serverless applications</li><li>All the configuration is YAML code<ul><li>Lambda Functions</li><li>DynamoDB tables</li><li>API Gateway</li><li>Cognito User Pools</li></ul></li><li>SAM can help you to run Lambda, API Gateway, DynamoDB locally</li><li>SAM can use CodeDeploy to deploy Lambda functions</li></ol><h2 id="Step-Functions"><a href="#Step-Functions" class="headerlink" title="Step Functions"></a>Step Functions</h2><ol><li>Build serverless visual workflow to orchestrate your Lambda functions</li><li>Represent flow as a JSON state machine</li><li>Features: sequence, parallel, conditions, timeouts, error handling</li><li>Can also integrate with EC2, ECS, On premise servers, API Gateway</li><li>Maximum execution time of 1 year</li><li>Possibility to implement human approval feature</li><li>Use Case<ul><li>Order fullfillment</li><li>Data processing</li><li>Web applications</li><li>Any workflow</li></ul></li></ol><h2 id="Simple-Workflow-Service"><a href="#Simple-Workflow-Service" class="headerlink" title="Simple Workflow Service"></a>Simple Workflow Service</h2><ol><li>Coordinate work amongst applications</li><li>Code runs on EC2 (not serverless)</li><li>1 year maximum runtime</li><li>Concept of “activity step” and “decision step”</li><li>Has built-in “human intervention” step</li><li>Example: order fulfillment from web to warehouse to delivery</li><li>Step functions is recommended to be used for new applications, except<ul><li>If you need external signals to intervene in the processes</li><li>If you need child processes that return values to parent processes</li></ul></li><li>SWF is older tha Step Functions</li></ol><h2 id="Opsworks"><a href="#Opsworks" class="headerlink" title="Opsworks"></a>Opsworks</h2><ol><li>Chef &amp; Puppet help you perform server configuration automatically, or repetitive actions</li><li><strong>They work great with EC2 &amp; On premise VM</strong></li><li>AWS Opsworks &#x3D; Managed Chef &amp; Puppet</li><li>It’s an alternative to AWS SSM</li><li>In the exam: Chef &amp; Puppet needed &#x3D;&gt; AWS Opsworks</li><li>Quick work on Chef &amp; Puppet<ul><li>They help with managing configuration as code</li><li>Helps in having consistent delopyments</li><li>Works with Linus &#x2F; Windows</li><li>Can automate: user accounts, cron, ntp, packages, services…</li><li>They leverage “Recipes” or “Manifests”</li><li>Chef &#x2F; Puppet have similarities with SSM &#x2F; Benstalk &#x2F; CloudFormation but they’re open-source tools that works cross-cloud</li></ul></li><li>A stack is basically a collection of instances that are managed together for serving a common task.</li></ol><h2 id="Elastic-Transcoder"><a href="#Elastic-Transcoder" class="headerlink" title="Elastic Transcoder"></a>Elastic Transcoder</h2><ol><li>Convert media files (video + music) stored in S3 into vairous formats for tablets, PC, Smartphone, TV, etc</li><li>Features: bit rate optimization, thumbnail, watermarks, captions, DRM, progressive download, encyption</li><li>4 components<ul><li>Jobs: what does the work of the transcoder</li><li>Pipeline: Queue that manages the transcoding job</li><li>Presets: Template for converting media from one format to another</li><li>Notification: SNS for example</li></ul></li><li>Pay for what you use, scales automatically, fully managed</li></ol><h2 id="WorkSpaces"><a href="#WorkSpaces" class="headerlink" title="WorkSpaces"></a>WorkSpaces</h2><ol><li>Managed, Secure Cloud Desktop</li><li>Great to eliminate management of no-premise VDI (Virtual Desktop Infrasturcture)</li><li>On Demand, pay per by usage</li><li>Secure, Encrypted, Network Isolation</li><li>Integrated with Microsoft Active Directory</li></ol><h2 id="WorkMail"><a href="#WorkMail" class="headerlink" title="WorkMail"></a>WorkMail</h2><ol><li>WorkMail is a managed email and calendar service that offers strong security controls and support for existing desktop and mobile clients</li></ol><h2 id="WorkDocs"><a href="#WorkDocs" class="headerlink" title="WorkDocs"></a>WorkDocs</h2><ol><li>WorkDocs is a fully managed, secure enterprise storage and sharing service with strong administrative controls and feedback capabilities that improve user productivity.</li><li>Your user’s files are only visible to them, and their designated contributors and viewers. Other members of your organization do not have access to other user’s files unless they are specifically granted access.</li><li>S3 vs WorkDocs<ul><li>S3 can’t be used like “Dropbox and Google drive”</li><li>S3 is a bucket storage, not a syncing service</li></ul></li><li><strong>To restrict all users to invite external users and share WorkDocs links publicly</strong>, you can create a <strong>Power user</strong> responsible for performing this activity.</li></ol><h2 id="Organizations"><a href="#Organizations" class="headerlink" title="Organizations"></a>Organizations</h2><ol><li>Global service</li><li>Allows to manage multiple AWS accounts</li><li>The main account is the master account - you can’t change it</li><li>Other accounts are member accounts</li><li>Member accounts can only be part of one organization</li><li>Consolidated Billing across all accounts - single payment method</li><li>Pricing benefits from aggregated usage (volume discount for EC2, S3…)</li><li>API is available to automate AWS account creation</li><li>Multi Account Strategies<ul><li>Create accounts per department, per cost center, per dev&#x2F;test&#x2F;prod, based on regulatory restrictions (using SCP), for better resource isolation (ex: VPC), to have separate per-account service limits, isolated account for logging</li><li>Multi Account vs One Account Multi VPC<ul><li>Multi Account: all resources separate</li><li>One Account Multi VPC: all resources can have chance talk with each other</li></ul></li><li>Using tagging standard for billing purposes</li><li>Enable CloudTrail on all accounts, send logs to central S3 account</li><li>Establish Cross Account Roles for Admin purposes</li></ul></li><li><strong>Service Control Policies (SCP)</strong><ul><li>Whitelist or blacklist IAM actions</li><li>Applied at the OU or Account level</li><li><strong>Does not apply to the Master Account</strong>, but all other accounts, including root accounts of individual accounts in an AWS Organization.</li><li>SCP is applied to all the Users and Roles of the Account, including Root</li><li>The SCP does not affect service-linked roles<ul><li>Service-linked roles enable other AWS services to integrate with AWS Organizations and can’t be restricted by SCPs</li></ul></li><li>SCP must have an explicit Allow (does not allow anything by default)</li><li>Use Case<ul><li>Restrict access to certain service (for example: can’t use EMR)</li><li>Enforce PCI compliance by explicit disabling services</li></ul></li></ul></li><li>SCP Hierachy<ul><li>Account acquire SCP from all its parent OU</li><li>Although one account have a permission for accessing, this account still cannot access the resource since its parent OU denying that</li></ul></li><li>AWS Organization - Moving Accounts<ul><li>To migrate accounts from one organization to another<ol><li>Remove the member account from the old organization</li><li>Send an invite to the new organization</li><li>Accept the invite to the new organization from the member account</li></ol></li><li>If you want the master account of the old organization to also join the new organization, do the following<ol><li>Remove all member accounts from the old organization</li><li>Delete the old organization</li><li>Invite the master account of the old organization to be a member account of the new organization</li></ol></li></ul></li><li><strong>Resource Sharing</strong><ul><li>For accounts that are part of Organization, <strong>Resource sharing can be done on an individual account basis if resource sharing is not enabled at the Organization level</strong>. With this, resources are shared within accounts as external accounts &amp; an <strong>invitation</strong> needs to be accepted between these accounts to start resource sharing.</li></ul></li><li>Avaialble feature sets<ul><li>All features<ul><li>The default feature set that is available to AWS Organization. It includes all the functionality of consolidated billing, plus advanced features that give you more control over accounts in your organization.</li></ul></li><li>Consolidated billing<ul><li>This feature set provides shared billing functionality, but does not include the more advanced features of AWS Organizations</li></ul></li></ul></li><li><strong>Consolidated Billing</strong><ul><li>benefits<ul><li>One bill</li><li>Easy tracking</li><li>Combined usage<ul><li>You can combine the usage across all accounts in the organization to share the volume pricing discounts</li></ul></li><li>No extra fee</li></ul></li></ul></li></ol><h2 id="Disaster-Recovery"><a href="#Disaster-Recovery" class="headerlink" title="Disaster Recovery"></a>Disaster Recovery</h2><ol><li><p>Any event that has a negative impact on a company’s business continuity or finances is a disaster</p></li><li><p>DR is about perparing for and recovering from a disaster</p></li><li><p>When we discuss a Disaster Recovery scenario, we assume that the entire region is affected due to the some disaster. We need that service to be provided from another region</p></li><li><p>Creating an AMI of the EC2 instance and copy it to another region. It’s a Disaster Recovery Solution.</p></li><li><p>What kind of DR?</p><ul><li>On-premise &#x3D;&gt; On-premise: traditional DR, and very expensive</li><li>On-premise &#x3D;&gt; AWS Cloud: hybrid recovery</li><li>AWS Cloud Region A &#x3D;&gt; AWS Cloud Region B</li></ul></li><li><p>Need to define two terms</p><ul><li>RPO: Recovery Point Objective</li><li>RTO: Recovery Time Objective</li></ul><p><img src="/posts/a60a7db0/image-20210529133605147.png" alt="image-20210529133605147"></p></li><li><p>Disaster Recovery Strategies</p><ul><li>Backup and Restore<ul><li><img src="/posts/a60a7db0/image-20210529134022829.png" alt="image-20210529134022829"></li><li>Easy, cheap, high RPO, high RTO</li></ul></li><li>Pilot Light<ul><li><img src="/posts/a60a7db0/image-20210529134216840.png" alt="image-20210529134216840"></li><li>A small version of the app is always running in the cloud</li><li>Useful for the critical core (pilot light)</li><li>Very similar to Backup and Restore</li><li>Faster than Backup and Restore as critical systems are already up</li><li>Lower RPO, lower RTO, and we still manage costs</li></ul></li><li>Warm Standby<ul><li><img src="/posts/a60a7db0/image-20210529134335039.png" alt="image-20210529134335039"></li><li>Full system is up and running, but at minimum size</li><li>Upon disaster, we can scale to production load</li></ul></li><li>Hot Site &#x2F; Multi Site Approach<ul><li><img src="/posts/a60a7db0/image-20210529134517347.png" alt="image-20210529134517347"></li><li>Very low RTO (minutes or seconds) - very expensive</li><li>Full production Scale is running AWS and On-premise</li></ul></li><li>All AWS Multi Region<ul><li><img src="/posts/a60a7db0/image-20210529134601180.png" alt="image-20210529134601180"></li></ul></li></ul><p><img src="/posts/a60a7db0/image-20210529133717204.png" alt="image-20210529133717204"></p></li><li><p>DR Tips</p><ul><li>Backup<ul><li>EBS Snapshots, RDS automated backups &#x2F; Snapshots, etc…</li><li>Regular pushes to S3 &#x2F; S3 IA &#x2F; Glacier, Lifecycle Policy, CRR</li><li>From On-premise: Snowball or Storage Gateway</li></ul></li><li>HA<ul><li>Use Route 53 to migrate DNS over from Region to Region</li><li>RDS Multi-AZ, ElastiCache Multi-AZ, EFS, S3</li><li>Site-to-Site VPN as a recovery from Direct Connect</li></ul></li><li>Replication<ul><li>RDS Replication (Cross Region), AWS Aurora + Global Databases</li><li>Database replication from on-premise to RDS</li><li>Storage Gateway</li></ul></li><li>Automation<ul><li>CloudFormation &#x2F; Elastic Beanstalk to recreate a whole new environment</li><li>Recover &#x2F; Reboot EC2 instance with CloudWatch if alarms fail</li><li>AWS Lambda functions for customized automations</li></ul></li><li>Chaos<ul><li>Netflix has a “simian-army” randomly terminating EC2 instances</li></ul></li></ul></li></ol><h2 id="On-Premise-Strategy-with-AWS"><a href="#On-Premise-Strategy-with-AWS" class="headerlink" title="On-Premise Strategy with AWS"></a>On-Premise Strategy with AWS</h2><ol><li>Ability to download Linux 2 AMI as a VM (.iso format)<ul><li>VMWare, KVM, VirtualBox (Oracle VM), Microsoft Hyper-V</li></ul></li><li>VM Import &#x2F; Export<ul><li>Migrate existing applications into EC2</li><li>Create a DR repository strategy for your on-premise VMs</li><li>Can export back the VMs from EC2 to on-premise</li></ul></li><li>AWS Application Discovery Service<ul><li>Gather information about your on-premise servers to plan a migration</li><li>Server utilization and dependency mappings</li><li>Track with AWS Migration Hub</li></ul></li><li>AWS Database Migration Service (DMS)<ul><li>replicate On-premise &#x3D;&gt; AWS, AWS &#x3D;&gt; AWS, AWS &#x3D;&gt; On-premise</li><li>Works with various database technologies (Oracle, MySQL, DynamoDB, etc…)</li></ul></li><li>AWS Server Migration Service (SMS)<ul><li>Incremental replication of on-premise live servers to AWS</li></ul></li></ol><h2 id="CI-x2F-CD"><a href="#CI-x2F-CD" class="headerlink" title="CI&#x2F;CD"></a>CI&#x2F;CD</h2><ul><li><p>Continuous Integration</p><ul><li>Developers push the code to a code repository often</li><li>A testing &#x2F; build server checks the code as soon as it’s pushed (CodeBuild &#x2F; Jenkins CI)</li><li>The developers gets feedback about the tests and checks that have passed &#x2F; failed</li><li>Find bugs early, fix bugs</li><li>Deliver faster as the code is tested</li><li>Deploy often</li></ul></li><li><p>Continuous Delivery</p><ul><li>Ensure that the software can be released reliably whenever needed</li><li>Ensure deployments happen often and are quick</li><li>Shift away from “one release every 3 months” to “5 releases a day”</li><li>That usually means automated deployment<ul><li>CodeDeploy</li><li>Jenkins CD</li><li>Spinnaker</li></ul></li></ul></li><li><p>Tech Stack for CI&#x2F;CD</p><p><img src="/posts/a60a7db0/image-20210529140801950.png" alt="image-20210529140801950"></p></li></ul><p>##Classic Solution Architecture</p><ol><li><p>Stateless Web App</p><ul><li><img src="/posts/a60a7db0/image-20210529142846543.png" alt="image-20210529142846543"></li><li>Progressively<ol><li>Public EC2 + Elastic IP</li><li>Multi Public EC2s + Elastic IP for each one</li><li>Route 53 + Public EC2s (without Elastic IP)<ul><li>visiting same hostname</li></ul></li><li>Route 53 + ELB (with Health Checks) + Private EC2s in one AZ</li><li>Route 53 + ELB (with Health Checks) + Private EC2s in different AZs</li><li>Minimum 2 AZ for cost saving</li></ol></li></ul></li><li><p>Stateful Web App</p><ul><li><img src="/posts/a60a7db0/image-20210529144252708.png" alt="image-20210529144252708"></li><li>Progressively<ol><li>Route 53 + ELB + Private EC2s in different AZs</li><li>Route 53 + ELB (with Stickness) + Private EC2s in diff AZs</li><li>Using ElasticCache for Storing &#x2F; Retrieving session data</li><li>Storing session data into a database (DynamoDB)</li><li>Scaling reads of DynamoDB (Read Replicas)</li><li>Read from ElastiCache</li><li>Multi AZ for ElastiCache and DynamoDB</li><li>Restrict traffic only from EC2 to ElastiCache or DynamoDB</li></ol></li></ul></li><li><p>Summary</p><ul><li>ELB sticky sessions</li><li>Web clients for storing cookies and making our web app stateless</li><li>ElastiCache<ul><li>For storing sessions (alternative: DynamoDB)</li><li>For caching data from DB</li><li>Multi AZ</li></ul></li><li>RDS<ul><li>For storing user data</li><li>Read replicas for scaling reads</li><li>Multi AZ for DR</li></ul></li><li>Tight Security with SGs referencing each other</li></ul></li><li><p>WordPress Blog Website</p><ul><li>Progressively<ol><li>Route 53 + ELB + EC2s in diff AZs + RDS with Multi AZ</li><li>Route 53 + ELB + EC2s in diff AZs + Aurora MySQL with Multi AZ and Read Replicas</li><li>Storing images with EBS</li><li>Storing images with EFS</li></ol></li></ul></li><li><p>Typical Architecture</p><p><img src="/posts/a60a7db0/image-20210529145000881.png" alt="image-20210529145000881"></p></li><li><p>Application Shards</p><ul><li>Sharding is a common concept to split data across multiple tables in a database</li><li>To make future growth easier, we make use of application shards</li><li>By using this, we can distribute the load to best suit our needs</li></ul></li></ol><h2 id="Serverless-Solution-Architecture"><a href="#Serverless-Solution-Architecture" class="headerlink" title="Serverless Solution Architecture"></a>Serverless Solution Architecture</h2><ol><li>Mobile App<ul><li><img src="/posts/a60a7db0/image-20210531111508777.png" alt="image-20210531111508777"></li><li>Progressively<ol><li>API Gateway + Cognito + Lambda + DynamoDB</li><li>S3 + STS (giving users access to S3, using Cognito to generate temporary credentials with STS to access S3 bucket with restricted policy)</li><li>DAX (using DAX before DynamoDB for high read throughput)</li><li>Caching at the API Gateway</li></ol></li></ul></li><li>Serverless hosted website<ul><li>Serving static content, globally<ul><li><img src="/posts/a60a7db0/image-20210531112930778.png" alt="image-20210531112930778"></li></ul></li><li>Serving static content, globally, securely<ul><li><img src="/posts/a60a7db0/image-20210531113011355.png" alt="image-20210531113011355"></li></ul></li><li>Adding a public serverless REST API<ul><li><img src="/posts/a60a7db0/image-20210531113047249.png" alt="image-20210531113047249"></li></ul></li><li>Leveraging DynamoDB Global Tables<ul><li><img src="/posts/a60a7db0/image-20210531113116352.png" alt="image-20210531113116352"></li></ul></li><li>User Welcome email flow<ul><li><img src="/posts/a60a7db0/image-20210531113151894.png" alt="image-20210531113151894"></li></ul></li><li>Thumnail generation flow<ul><li><img src="/posts/a60a7db0/image-20210531113237277.png" alt="image-20210531113237277"></li></ul></li></ul></li><li>Microservices <ul><li><img src="/posts/a60a7db0/image-20210531113409185.png" alt="image-20210531113409185"></li><li>Synchronous patterns: API Gateway, ELB</li><li>Asynchronous patterns: SQS, Kinesis, SNS, Lambda</li></ul></li><li>Distributing Paid Content<ul><li>Simple, premium user service<ul><li><img src="/posts/a60a7db0/image-20210531113736231.png" alt="image-20210531113736231"></li></ul></li><li>Add authentication<ul><li><img src="/posts/a60a7db0/image-20210531113753060.png" alt="image-20210531113753060"></li></ul></li><li>Add Video Storage Service<ul><li><img src="/posts/a60a7db0/image-20210531113814850.png" alt="image-20210531113814850"></li></ul></li><li>Distribute Globally and Securely<ul><li><img src="/posts/a60a7db0/image-20210531113846532.png" alt="image-20210531113846532"></li></ul></li><li>Distribute Content only to Premium Users<ul><li><img src="/posts/a60a7db0/image-20210531113939305.png" alt="image-20210531113939305"></li></ul></li><li>Summary<ul><li>Cognito for authentication</li><li>DynamoDB for storing users that are premium</li><li>2 Serverless applications<ul><li>Premium User registration</li><li>CloudFront Signed URL generator</li></ul></li><li>Content is stored in S3 (Serverless and scalable)</li><li>Integrated with CloudFront with OAI for security</li><li>CloudFront can only be used using Signed URLs to prevent unauthorized users</li></ul></li></ul></li><li>Software updates offloading<ul><li>Current Architecture<ul><li><img src="/posts/a60a7db0/image-20210531114544031.png" alt="image-20210531114544031"></li></ul></li><li>Using CloudFront to fix<ul><li><img src="/posts/a60a7db0/image-20210531114609126.png" alt="image-20210531114609126"></li></ul></li><li>Why CloudFront?<ul><li>No changes to architecture</li><li>Will cache software update files at the edge</li><li>Software update files are not dynamic, they’re static (never changing)</li><li>Our EC2 instance aren’t serverless</li><li>But CloudFront is, and will scale for us</li><li>Our ASG will not scale as much, and we’ll save tremendously in EC2</li><li>We’ll also save in availability, network bandwidth cost, etc…</li><li>Easy way to make an existing application more scalable and cheaper</li></ul></li></ul></li><li>Big Data Ingestion Pipeline<ul><li><img src="/posts/a60a7db0/image-20210531114914272.png" alt="image-20210531114914272"></li><li>Summary<ul><li>Kinesis is great for real-time data collection</li><li>Firehose helps with data delivery to S3 in near real-time (1 min)</li><li>Lambda can help Firehose with data transformation</li><li>S3 can trigger notification to SQS</li><li>Lambda can subscribe to SQS</li><li>Athena is a serverless SQL service and results are stored in S3</li><li>The reporting bucket contains analyzed data and can be used by reporting tool such as QuickSight, Redshift, etc…</li></ul></li></ul></li></ol><h2 id="Other-Architectures"><a href="#Other-Architectures" class="headerlink" title="Other Architectures"></a>Other Architectures</h2><p><strong>Serverless: S3, Lambda, DynamoDB, CloudFront, API Gateway</strong></p><ul><li>Event Processing<ul><li>Lambda, SNS &amp; SQS<ul><li>SQS + Lambda + SQS DLQ</li><li>SQS FIFO + Lambda + SQS DLQ</li><li>SNS + Lambda + SQS DLQ</li></ul></li><li>Fan Out Pattern (deliver to multiple SQS)<ul><li><img src="/posts/a60a7db0/image-20210531120517296.png" alt="image-20210531120517296"></li></ul></li><li>S3 Event<ul><li><img src="/posts/a60a7db0/image-20210531120538426.png" alt="image-20210531120538426"></li><li>if you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket</li></ul></li></ul></li><li>Caching Strategies<ul><li><img src="/posts/a60a7db0/image-20210531120748355.png" alt="image-20210531120748355"></li><li>Using TTL for renewing the cache of CloudFront</li><li>API Gateway also have some caching capability</li><li>Since API Gateway is a regional service, so the cache of API Gateway is also regional</li><li>Apps can do cache in Redis, Memcached, or DAX and reduce the pressure of database</li><li><strong>Note: there is no caching capability in S3 and Databases</strong></li></ul></li><li>Blocking an IP address<ol><li>Client -&gt; EC2<ul><li>Using NACLs deny rule for blocking</li></ul></li><li>Client -&gt; ALB -&gt; EC2<ul><li>Usig ALB connection termination</li><li>The SG of EC2 must only allows the traffic from the SG of ALB, not from the client</li></ul></li><li>Client -&gt; NLB -&gt; EC2<ul><li>NLB cannot do connection termination</li><li>No SG for NLB</li><li>all traffic go through NLB to EC2 without any obstacle</li></ul></li><li>Client -&gt; ALB + WAF -&gt; EC2<ul><li><img src="/posts/a60a7db0/image-20210531121512438.png" alt="image-20210531121512438"></li></ul></li><li>Client -&gt; CloudFront + WAF -&gt; ALB -&gt; EC2<ul><li><img src="/posts/a60a7db0/image-20210531121538192.png" alt="image-20210531121538192"></li><li>If the CloudFront stands before ALB, the NACL cannot get the IP address of the client. So, using NACL to block IP address will fail</li></ul></li></ol></li><li>HPC<ul><li>Data Management &amp; Transfer<ul><li>Direct Connect<ul><li>Move GB&#x2F;s of data to the cloud, over a private secure network</li></ul></li><li>Snowball &amp; Snowmobile<ul><li>Move PB of data to the cloud</li></ul></li><li>DataSync<ul><li>Move large amount of data between on-premise and S3, EFS, FSx for Windows</li></ul></li></ul></li><li>Compute &amp; Networking<ul><li>EC2 Instances<ul><li>CPU Optimized, GPU Optimized</li><li>Spot Instances &#x2F; Spot Fleets for cost savings + Auto Scaling</li></ul></li><li><strong>EC2 Placement Groups: Cluster for good network performance</strong></li><li>EC2 Enhanced Networking (SR-IOV)<ul><li>Higher bandwidth, higher PPS (packet per second), low latency</li><li>Option 1: Elastic Network Adapter (ENA) up to 100Gbps<ul><li>ENA is a custom network interface optimized to deliver <strong>high throughput and low latency</strong> on specific supported EC2 instance</li><li>It doesn’t support high-performance requirements</li><li>Not all instance types are supported for using ENA interface</li><li><strong>user can enable it with “-ena-support”</strong></li></ul></li><li>Option 2: Intel 82599 VF up to 10Gbps - Legacy</li></ul></li><li>Elastic Fabric Adapter (EFA)<ul><li><strong>Imporved ENA for HPC and machine learning, only works for Linux</strong></li><li>EFA supports low latency and high throughput with high-performance with the scalability, flexibility, and elasticity provided by AWS.</li><li><strong>Great for inter-node communications</strong>, tightly coupled workloads</li><li>Leverages Message Passing Interface (MPI) standard</li><li>Bypasses the underlyingn Linux OS to provide low-latency reliable transport</li><li><strong>EFA cannot be moved into another subnet once created.</strong></li><li><strong>Amazon CloudWatch metrics are required to monitor EFA in Real-Time</strong></li><li><strong>EFA OS-bypass subnet is limited to only one subnet.</strong></li><li>To enable OS-bypass functionality, the EFA must be a member of security group that <strong>allows inbound and outbound traffic and from the security group itself</strong>.</li><li>Attach &amp; Detach<ul><li><strong>Attach: Only one EFA can be attached to an EC2 instance (stopped state, not running state)</strong></li><li><strong>Detach: You must stop the instance first.</strong> You cannot detach an EFA from a running instance</li></ul></li><li>You can change the IP addresses associated with an EFA. If you have an Elastic Ip address, you can associate it with an EFA</li></ul></li></ul></li><li>Storage<ul><li>Instance-attached storage<ul><li>EBS: scale up to 64000 IOPS with io1 Provisioned IOPS</li><li>Instance Store: scale to millions of IOPS, linked to EC2 instance, low latency</li></ul></li><li>Network storage<ul><li>S3: large blob, not a file system</li><li>EFS: scale IOPS based on total size, or use Provisioned IOPS</li><li>FSx for Lustre<ul><li>HPC optimized distributed system, millions of IOPS</li><li>Backed by S3</li></ul></li></ul></li></ul></li><li>Automation &amp; Orchestration<ul><li>AWS Batch<ul><li>Batch supports multi-node parallel jobs, which enables you to run single jobs that span multiple EC2 instances</li><li>Easily schedule jobs and launch EC2 instances accordingly</li></ul></li><li>AWS ParallelCluster<ul><li>Open source cluster management tool to deploy HPC on AWS</li><li>Configure with text files</li><li>Automate creation of VPC, Subnet, cluster type and instance types</li></ul></li></ul></li></ul></li><li>Creating a highly available EC2 instance<ol><li>Using CloudWatch Event to trigger<ul><li><img src="/posts/a60a7db0/image-20210531123136227.png" alt="image-20210531123136227"></li><li>Using CloudWatch to monitoring the metrics of EC2 instances. After that CloudWatch Event can trigger a Lambda Function to do whatever you want</li></ul></li><li>ASG<ul><li><img src="/posts/a60a7db0/image-20210531123231048.png" alt="image-20210531123231048"></li><li>EC2 instance attaching User Data which contains scripts for attaching Elastic IP to that instance</li><li>When one instance is terminated, other one will be created by ASG, and this new instance will run User Data automatically</li></ul></li><li>ASG + EBS<ul><li><img src="/posts/a60a7db0/image-20210531123341341.png" alt="image-20210531123341341"></li><li>EBS is AZ specific service</li><li>Using ASG lifecycle hooks for creating a new EBS volume in another AZ</li></ul></li></ol></li><li>HA for Bastion Host<ul><li><img src="/posts/a60a7db0/image-20210531123542632.png" alt="image-20210531123542632"></li><li>HA Options for the bastion host<ul><li>Run 2 across 2 AZ</li><li>Run 1 across 2 AZ with 1 ASG 1:1:1</li></ul></li><li>Routing to the bastion host<ul><li>if 1 bastion host, use an Elastic IP with EC2 User Data script to access it</li><li>if 2 bastion host, use NLB(Layer 4) deployed in multiple AZ</li><li>if NLB, the bastion hosts can live in the private subnet directly</li></ul></li><li>Note: Can’t use ALB as the ALB is in layer 7</li></ul></li></ul><p>##Well Architected Tool</p><ol><li>Operation Excellence<ul><li>Includes the ability to run and monitor systems to deliver business value and to continually supporting processes and procedures</li><li>Design Principles<ul><li>Perform operations as code</li><li>Annotate documentation - Automate the creation of annotated documentation after every build</li><li>Make frequent, small, reversible changes - So that is case of any failure, you can reverse it</li><li>Refine operations procedures frequently - And ensure that team member are familiar with it</li><li>Anticipate failure</li><li>Learn from all operational failures</li></ul></li><li>AWS Services<ul><li>Prepare<ul><li>CloudFormation</li><li>Config</li></ul></li><li>Operate<ul><li>CloudFormation</li><li>Config</li><li>CloudTrail</li><li>CloudWatch</li><li>X-Ray</li></ul></li><li>Evolve<ul><li>CloudFormation</li><li>CodeBuild</li><li>CodeCommit</li><li>CodeDeploy</li><li>CodePipeline</li></ul></li></ul></li></ul></li><li>Security<ul><li>Includes the ability to protect information, systems, and assets while  delivering business value through risk assessments and mitigation strategies</li><li>Design Principles<ul><li>Implement a strong identity foundation - Centralize privilege management and reduce (or even eliminate) reliance on long-term credentials - Principle of least privilege - IAM</li><li>Enable traceability - Integrate logs and metrics with systems to automatcially respond and take action</li><li>Apply security at all layers - Like edge network, VPC, subnet, load balancer, every instance, operating system, and application</li><li>Automate security best practices</li><li>Protect data in transit and at rest - Encryption, tokenization, and access control</li><li>Keep people away from data - Reduce or eliminate the need for direct access or manual processing of data</li><li>Prepare for security events - Run incident response simulations and use tools with automation to increase your speed for deletion, investigation, and recovery</li></ul></li><li>AWS Services<ul><li>Identity and Access Management<ul><li>IAM</li><li>STS</li><li>MFA token</li><li>Organizations</li></ul></li><li>Detective Controls<ul><li>Config</li><li>CloudTrail</li><li>CloudWatch</li></ul></li><li>Infrastructure Protection<ul><li>CloudFront</li><li>VPC</li><li>Shield</li><li>WAF</li><li>Inspector</li></ul></li><li>Data Protection<ul><li>KMS</li><li>S3</li><li>ELB</li><li>EBS</li><li>RDS</li></ul></li><li>Incident Response<ul><li>IAM</li><li>CloudFormation</li><li>CloudWatch Events</li></ul></li></ul></li></ul></li><li>Reliability<ul><li>Ability of a system to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues</li><li>Design Principles<ul><li>Test recovery procedures - Use automation to simulate different failures or to recreate scenarios that led to failures before</li><li>Automatically recover from failure - Anticipate and remediate failures before they occur</li><li>Scale horizontally to increase aggregate system availability - Distribute requests across multiple, smaller resources to ensure that they don’t share a common point of failure</li><li>Stop guessing capability - Maintain the optimal level to satisfy demand without over or under provisioning - Use Auto Scaling</li><li>Manage change in automation - Use automation to make changes to infrasturcture</li></ul></li><li>AWS Services<ul><li>Foundations<ul><li>IAM</li><li>VPC</li><li>Service Limits</li><li>AWS Trusted Advisor</li></ul></li><li>Change Management<ul><li>AWS Auto Scaling</li><li>CloudWatch</li><li>CloudTrail</li><li>Config</li></ul></li><li>Failure Management<ul><li>Backups</li><li>CloudFormation</li><li>S3</li><li>S3 Glacier</li><li>Route 53</li></ul></li></ul></li></ul></li><li>Performance Efficiency<ul><li>Includes the ability to use computing resources efficiently to meet system requirements, and to maintain that efficiency as demand changes and technologies evolve</li><li>Design Principles<ul><li>Democratize advanced technologies - Advance technologies become services and hence you can focus more on product development</li><li>Go global in minutes - Easy deployment in multiple regions</li><li>Use serverless architectures - Avoid burden of managing servers</li><li>Experiment more often - Easy to carry out comparative testing</li><li>Mechanical sympathy - Be aware of all AWS services</li></ul></li><li>AWS Services<ul><li>Selection<ul><li>AWS Auto Scaling</li><li>Lambda</li><li>EBS</li><li>S3</li><li>RDS</li></ul></li><li>Review<ul><li>CloudFormation</li><li>AWS News Blog</li></ul></li><li>Monitoring<ul><li>CloudWatch</li><li>Lambda</li></ul></li><li>Tradeoffs<ul><li>RDS</li><li>ElastiCache</li><li>Snowball</li><li>CloudFront</li></ul></li></ul></li></ul></li><li>Cost Optimization<ul><li>Includes the ability to run systems to deliver business value at the lowest price point</li><li>Design Principles<ul><li>Adopt a consumption mode - Pay only for what you want</li><li>Measure overall efficiency - Use CloudWatch</li><li>Stop spending money on data center operations - AWS does the infrastructure part and enables customer to focus on organization projects</li><li>Analyze and attribute expenditure - Accurate identification of system usage and costs, helps measure return on investment (ROI) - Make sure to use tags</li><li>Use managed and application level services to reduce cost of ownership - As managed services at cloud scale, they can offer a lower cost per transaction or service</li></ul></li><li>AWS Services<ul><li>Expenditure Awareness<ul><li>AWS Budgets</li><li>AWS Cost and Usage Report</li><li>AWS Cost Explorer</li><li>Reserved Instance Reporting</li></ul></li><li>Cost-Effective Resources<ul><li>Spot Instance</li><li>Reserved Instance</li><li>S3 Glacier</li></ul></li><li>Matching supply and demand<ul><li>AWS Auto Scaling</li><li>Lambda</li></ul></li><li>Optimizing Over Time<ul><li>AWS Trusted Advisor</li><li>AWS Cost and Usage Report</li><li>AWS News Blog</li></ul></li></ul></li></ul></li></ol><h2 id="Trusted-Advisor"><a href="#Trusted-Advisor" class="headerlink" title="Trusted Advisor"></a>Trusted Advisor</h2><ul><li>No need to install anything - high level AWS account assessment</li><li>Analyze your AWS accounts and provides recommendation<ul><li>Cost Optimization</li><li>Performance</li><li>Security</li><li>Fault Tolerance</li><li>Service Limits</li></ul></li><li>Full Trusted Advisor - Available for Business &amp; Enterprise support plans<ul><li>Ability to set CloudWatch alarms when reaching limits</li></ul></li><li>It is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices.</li></ul><h2 id="Savings-Plans"><a href="#Savings-Plans" class="headerlink" title="Savings Plans"></a>Savings Plans</h2><ol><li>Savings Plans is a flexible pricing model that provides low prices in exchange for commitment</li><li>Supported services<ul><li>EC2</li><li>Fargate</li><li>Lambda</li><li>SageMaker</li></ul></li><li>Types of Savings Plans<ul><li>Compute Savings Plans<ul><li><strong>reduce your costs by up to 66%</strong></li><li>automatically apply to EC2 instance usage <strong>regardless of instance family, size, AZ, region, OS or tenancy, and also apply to Fargate or Lambda usage</strong></li></ul></li><li>EC2 Instance Savings Plans<ul><li>provides the lowest prices</li><li><strong>savings up to 72%</strong></li><li>EC2 Instance Savings Plans apply to usage <strong>regardless of size, OS, or tenancy within the spcified family</strong></li></ul></li></ul></li><li>Note:<ul><li>AWS Reserved Instance is only for EC2</li><li>AWS Spot Instance is only for EC2</li><li>Savings Plans don’t support RDS</li></ul></li></ol><h2 id="SageMaker"><a href="#SageMaker" class="headerlink" title="SageMaker"></a>SageMaker</h2><ol><li>is a fully managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment</li></ol><h2 id="ParallelCluster"><a href="#ParallelCluster" class="headerlink" title="ParallelCluster"></a>ParallelCluster</h2><ol><li>ParallelCluster is an AWS-supported open source cluster management tool that makes it easy for you to deploy and manage HPC clusters on AWS.</li><li>It is built on the popular open source CfnCluster project is released via the Python Package Index (PyPI).</li><li>ParallelCluster is available at no additional charge, and you only pay for the AWS resources needed to run your applications.</li><li>ParallelCluster supports EFA, which can get OS-bypass capabilities (kernel-bypass networking), which is possible only in specific instance types and limited to a single subnet. Also, you cannot attach an EFA to an instance that is in the running state.</li><li>EFA support is not enabled by default and is not supported in any EC2 instance type.</li></ol><h2 id="Certificate-Manager"><a href="#Certificate-Manager" class="headerlink" title="Certificate Manager"></a>Certificate Manager</h2><ol><li>Certificate Manager can be used to generate SSL certicates to encrypt traffic in transit, but not at rest.</li></ol><h2 id="Lake-Formation"><a href="#Lake-Formation" class="headerlink" title="Lake Formation"></a>Lake Formation</h2><ol><li>AWS Lake Formation is a service that makes it easy to set up a secure data lake in days.</li><li>A data lake enables you to break down data silos and combine different types of analytics to gain insights and guide better business decisions.</li></ol><h2 id="Control-Tower"><a href="#Control-Tower" class="headerlink" title="Control Tower"></a>Control Tower</h2><ol><li>The easiest way to set up and govern a secure multi-account AWS environment</li><li>Control Tower is more suitable to automate a deployment in multi-account environments</li></ol><h2 id="Cost-Explorer"><a href="#Cost-Explorer" class="headerlink" title="Cost Explorer"></a>Cost Explorer</h2><ol><li>Visualize, understand, and manage your AWS costs and usage over time</li><li>In Cost Explorer, you can analyze and explore your bills and service usage in the account.</li></ol><h2 id="Server-Migration-Service-SMS"><a href="#Server-Migration-Service-SMS" class="headerlink" title="Server Migration Service (SMS)"></a>Server Migration Service (SMS)</h2><ol><li>Server Migration is used to migrate on-premises workloads to EC2</li></ol><h2 id="Migration-Hub"><a href="#Migration-Hub" class="headerlink" title="Migration Hub"></a>Migration Hub</h2><ol><li>Migration Hub is used to track the progress of migrations in AWS.</li></ol><h2 id="Transfer"><a href="#Transfer" class="headerlink" title="Transfer"></a>Transfer</h2><ol><li>AWS Transfer is a better choice for <strong>transferring SFTP data between on-premises &amp; S3</strong></li></ol><h2 id="Servcie-Catalog"><a href="#Servcie-Catalog" class="headerlink" title="Servcie Catalog"></a>Servcie Catalog</h2><ol><li>Service Catalog is used to manage catalogs and cannot share resources with others.</li><li>Allow organizations to create and manage catalogs of IT services that are approved for use on AWS. These IT services can include everything from virtual machine images, servers, software, and databases to complete multi-tier application architectures.</li></ol><h2 id="Polly"><a href="#Polly" class="headerlink" title="Polly"></a>Polly</h2><ol><li>Amazon Polly is a cloud service that converts text into lifelike speech</li><li>Managing Lexicons<ul><li>Pronunciation lexicons enable you to customize the pronunciation of words. Polly provides API operations that you can use to store lexicons in an AWS region. <strong>Those lexicons are then specific to that particular region</strong>. You can use one or more of the lexicons from that region when syntheizing the text by using the SynthesizeSpeech operation.</li></ul></li></ol><h2 id="CloudSearch"><a href="#CloudSearch" class="headerlink" title="CloudSearch"></a>CloudSearch</h2><ol><li>With CloudSearch, you can quickly add rich search capabilities to your website or application. You don’t need to become a search expert or worry about hardware provisioning, setup, and maintenance.</li></ol><h2 id="X-Ray"><a href="#X-Ray" class="headerlink" title="X-Ray"></a>X-Ray</h2><ol><li><strong>X-Ray collects data, analysis and debug of microservice application.</strong></li><li>X-Ray helps to analyze and debug modern applications. It will also collect the trace about the request from each of the applications. It also records the traces. After recording, <strong>it can create a view service map that can be seen to trace data latency and analyze the issues.</strong> This can help to find any unusual behavior to identify any root cause.</li></ol><p>##AWS Batch</p><ol><li>Batch helps you to run batch computing workloads on the AWS Cloud. </li><li><strong>Batch simplifies running batch jobs across multiple AZs within a Region.</strong> You can create Batch compute environments within a new or existing VPC. After a compute environment is up and associated with a job queue, you can define job definitions that specify which Docker container images to run your jobs. Container images are stored in and pulled from container registries, which may exist within or outside of your AWS infrastructure.</li><li>Components<ul><li>Jobs<ul><li>A unit of work (such as a shell script, a Linux executable, or a Docker container image) that you submit to AWS Batch.</li></ul></li><li>Job Definitions<ul><li>A job definition specifies how jobs are to be run. You can supply your job with an IAM role to provide access to other AWS resources. You also specify both memory and CPU requirements. The job definition can also control container properties, environment variables, and mount points for persistent storage.</li></ul></li><li>Job Queues<ul><li>When you submit an AWS Batch job, you submit it to a particular job queue, where the job resides until it’s scheduled onto a compute environment. You associate one or more compute environments with a job queue.</li></ul></li></ul></li><li>Batch supports both customized AMI and ECS-optimized AMI</li><li>Job States<ul><li>SUMITTED<ul><li>A job that has been submitted to the queue, and has not yet been evaluated by the scheduler</li></ul></li><li>PENDING<ul><li>A job that resides in the queue and isn’t yet able to run due to a dependency on another job or resource. After the dependencies are satisfied, the job is moved to RUNNABLE.</li></ul></li><li>RUNNABLE<ul><li>A job that resides in the queue, has no outstanding dependencies, and is therefore ready to be scheduled to a host. Jobs can remain in this state indefinitely when sufficient resources are unavailable.</li></ul></li><li>STARTING<ul><li>These jobs have been scheduled to a host and the relevant container initiation operations are underway. After the container image is pulled and the container is up and running, the job transactions to RUNNING.</li></ul></li><li>RUNNING<ul><li>The job is running as a container job on ECS container instance within a compute environment. <strong>If the job associated with a failed attempt has any remaining attempts left in its optional retry strategy configuration, the job is moved to RUNNABLE again.</strong></li></ul></li><li>SUCCEEDED<ul><li>The job has successfully completed with an exit code of 0. The job state for SUCCEEDED jobs is persisted in Batch for at least 24 hours.</li></ul></li><li>FAILED<ul><li>The job has failed all available attempts. The job state for FAILED jobs is persisted in Batch for at least 24 hours.</li></ul></li></ul></li><li>Job Stuck in RUNNABLE Status<ul><li><strong>The AWS logs log drivers isn’t configured on your compute resources</strong><ul><li>AWS Batch jobs send their log information to CloudWatch Logs. To enable this, you must configure your compute resources to use the AWS logs log driver.</li></ul></li><li><strong>Insufficient resources</strong><ul><li>If your job definitions specify more CPU or memory resources than your compute resources can allocate, then your jobs is never placed.</li></ul></li><li><strong>No internet access for compute resources</strong><ul><li>Compute resources need access to communicate with ECS service endpoint. This can be through an interface VPC endpoint or through your compute resources having public IP addresses.</li></ul></li><li><strong>Amazon EC2 instance limit reached</strong><ul><li>The number of Amazon EC2 instances that your account can launch in an AWS Region is determined by your EC2 instance limit. Certain instance types have a per-instance-type limit as well.</li></ul></li></ul></li><li>Priority<ul><li>You can create multiple Job queues with different priority and mapped Compute environments to each Job queue. When Job queues are mapped to the same compute environment, queues with higher priority are evaluated first.</li></ul></li><li>Job Queue Parameters<ul><li>Job queue name</li><li>State<ul><li>The state of the job queue. If the job queue state is ENABLED (the default value), it can accept jobs.</li></ul></li><li>Priority<ul><li>The priority of the job queue. Job queues with a higher priority (or a higher integer value for the priority parameter) are evaluated first when associated with same compute environment.</li><li>Priority is determined in descending order. </li><li>All the compute environments must be either EC2 (EC2 or SPOT) or Fargate (FARGATE or FARGATE_SPOT)</li><li><strong>EC2 and Fargate compute environments can’t be mixed</strong></li></ul></li></ul></li></ol><p>##CodePipeline</p><ol><li>To automatically trigger pipeline with changes in the source S3 bucket, CloudWatch Events rule &amp; CloudTrail trail must be applied. When there is a change in the S3 bucket, events are filtered using CloudTrail &amp; then CloudWatch events are used to trigger the start of the pipeline. This default method is faster and periodic checks should be disabled to have events-based triggering of CodePipeline.</li><li>Webhooks are used to trigger pipeline the source is GitHub repository</li><li>Periodic checks are not a faster way to trigger CodePipeline</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/a60a7db0/image-20210919194848724.png&quot; alt=&quot;image-20210919194848724&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;IAM&quot;&gt;&lt;a href=&quot;#IAM&quot; class=&quot;headerlink&quot; title=&quot;IAM&quot;&gt;&lt;/a&gt;IAM&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;IAM role policy only define &lt;strong&gt;which API actions&lt;/strong&gt; can be made to that role.&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Cloud" scheme="https://liuninglin.github.io/categories/Cloud/"/>
    
    <category term="AWS" scheme="https://liuninglin.github.io/categories/Cloud/AWS/"/>
    
    
    <category term="AWS" scheme="https://liuninglin.github.io/tags/AWS/"/>
    
    <category term="AWS-SAA" scheme="https://liuninglin.github.io/tags/AWS-SAA/"/>
    
    <category term="Architect" scheme="https://liuninglin.github.io/tags/Architect/"/>
    
  </entry>
  
  <entry>
    <title>Several Pivotal Conceptions about Hive</title>
    <link href="https://liuninglin.github.io/posts/449d41fb.html"/>
    <id>https://liuninglin.github.io/posts/449d41fb.html</id>
    <published>2019-12-06T22:50:29.000Z</published>
    <updated>2022-10-01T20:38:34.639Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/449d41fb/Hive-feature-2.jpg" alt="Image result for Hive feature"></p><h3 id="What-is-Apache-Hive"><a href="#What-is-Apache-Hive" class="headerlink" title="What is Apache Hive?"></a>What is Apache Hive?</h3><p>As the data grew in size, there was also the scarcity of Java developers who can write complex MapReduce jobs for Hadoop. Hence the advent of Hive comes which is created on top of Hadoop itself. Hive provides a SQL like a language termed HiveQL interface for users to extract data from a Hadoop system. With the simplicity provided by Hive to transform simple SQL queries into Hadoop’s MapReduce jobs, and runs them against a Hadoop cluster.</p><span id="more"></span><p>Apache Hive is well suited for Data warehousing applications in which case the data is structured, static and also formatted. As there are certain design constraints on Hive, it does not provide row-wise updates and inserts (which is coined as the biggest disadvantage of using Hive). As most Hive queries turn out into Map to Reduce jobs these queries will have higher latency due to start up overhead.</p><p>Based on these details, Hive is <strong>NOT</strong> a</p><ul><li>Relational database</li><li>Design for OLTP (stands for Online Transaction Processing)</li><li>Language for real-time queries and row-level updates</li></ul><h3 id="Features-of-Hive"><a href="#Features-of-Hive" class="headerlink" title="Features of Hive"></a>Features of Hive</h3><p>With the basic understanding of what Apache Hive is, let us now take a look at all the features that are provided with this component of the Hadoop ecosystem:</p><ul><li>Hive stores the schema details in a database and processes the data into HDFS</li><li>Hive is designed for OLAP (stands for Online Analytics Processing)</li><li>Hive provides SQL like language for querying data, named as HiveQL or HQL (do not misinterpret this with HQL from Hibernate, which stands for Hibernate Query Language).</li><li>Hive is a very fast, scalable and extensible component within the Hadoop ecosystem.</li></ul><h3 id="External-Table"><a href="#External-Table" class="headerlink" title="External Table"></a>External Table</h3><p>If there is data that is already existing in the HDFS cluster of Hadoop then an external Hive table is created to describe the data. These tables are called External tables, because they are going to be residing in the path specified by the LOCATION properties instead of the default warehouse directory (as described in the above paragraph).</p><p>When the data is stored in the external tables and when it is dropped, the metadata table is deleted but then the data is kept as is. This means that Hive evidently ignores the data that is presently residing in the path specified by LOCATION property and is left untouched forever. If you want to delete such data, then use the command to achieve the same:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs –rmr ‘tablename’</span><br></pre></td></tr></table></figure><p>For example, we can add <strong>EXTERNAL</strong> keyword for specifying the external table.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> table_name (</span><br><span class="line">  id                <span class="type">int</span>,</span><br><span class="line">  dtDontQuery       string,</span><br><span class="line">  name              string</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="Internal-or-Managed-Table"><a href="#Internal-or-Managed-Table" class="headerlink" title="Internal or Managed Table"></a>Internal or Managed Table</h3><p>The tables that are created with the Hadoop Hive’s context, is very much similar to tables that are created on any of the RDBMS systems. Each of the tables that get created is associated with a directory configured within the ${HIVE_HOME}&#x2F;conf&#x2F;hive-site.xml in the Hadoop HDFS cluster.</p><p>By default, on a Linux machine, it is this path <strong>&#x2F;user&#x2F;hive&#x2F;warehouse</strong> in HDFS. If there is a <strong>&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;match</strong> created by Hive in HDFS for a match table. All the data for the table is recorded in the same folder as mentioned above and hence such tables are called <strong>INTERNAL</strong> or <strong>MANAGED</strong> tables.</p><p>When the data resides in the internal tables, then Hive takes the full responsibility of maintaining the life-cycle of the data and the table in itself. Hence it is evident that the data is removed the moment when the internal tables are dropped.</p><p>By default Hive creates managed tables such as SQL below.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> table_name (</span><br><span class="line">  id                <span class="type">int</span>,</span><br><span class="line">  dtDontQuery       string,</span><br><span class="line">  name              string</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="Partitioning-of-Table"><a href="#Partitioning-of-Table" class="headerlink" title="Partitioning of Table"></a>Partitioning of Table</h3><p>Hive stores tables in partitions. Partitions are used to divide the table into related parts. Partitions make data querying more efficient.</p><p>For example, using SQL below.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> table_name (</span><br><span class="line">  id                <span class="type">int</span>,</span><br><span class="line">  dtDontQuery       string,</span><br><span class="line">  name              string</span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="type">date</span> string)</span><br></pre></td></tr></table></figure><h3 id="Bucketing-of-Table"><a href="#Bucketing-of-Table" class="headerlink" title="Bucketing of Table"></a>Bucketing of Table</h3><p>As we all know, Partition helps in increasing efficiency when performing a query on a table. Instead of scanning the whole table, it will only scan for the partitioned set and does not scan or operate on the unpartitioned sets, which helps us to provide results in lesser time and the details will be displayed very quickly because of Hive Partition.</p><p>Now, let’s assume a condition that there is a huge dataset. At times, even after partitioning on a particular field or fields, the partitioned file size doesn’t match with the actual expectation and remains huge and we want to manage the partition results into different parts. To overcome this problem of partitioning, Hive provides Bucketing concept, which allows the user to divide table data sets into more manageable parts.</p><p>The Bucketing concept is based on Hash function, which depends on the type of the bucketing column. Records which are bucketed by the same column will always be saved in the same bucket.</p><p>In Hive Partition, each partition will be created as directory. But in Hive Buckets, each bucket will be created as file. Bucketing can also be done even without partitioning on Hive tables.</p><p>With the help of <strong>CLUSTERED BY</strong> clause and optional <strong>SORTED BY</strong> clause in <strong>CREATE TABLE</strong> statement we can create bucketed tables. </p><p>We can create a bucketed_user table with above-given requirement with the help of the below HiveQL.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> bucketed_user(</span><br><span class="line">firstname <span class="type">VARCHAR</span>(<span class="number">64</span>),</span><br><span class="line">  lastname  <span class="type">VARCHAR</span>(<span class="number">64</span>),</span><br><span class="line">  address   STRING,</span><br><span class="line">  city <span class="type">VARCHAR</span>(<span class="number">64</span>),</span><br><span class="line">state  <span class="type">VARCHAR</span>(<span class="number">64</span>),</span><br><span class="line">  post      STRING,</span><br><span class="line">  phone1    <span class="type">VARCHAR</span>(<span class="number">64</span>),</span><br><span class="line">  phone2    STRING,</span><br><span class="line">  email     STRING,</span><br><span class="line">  web       STRING</span><br><span class="line">  )</span><br><span class="line">COMMENT ‘A bucketed sorted <span class="keyword">user</span> <span class="keyword">table</span>’</span><br><span class="line">  PARTITIONED <span class="keyword">BY</span> (country <span class="type">VARCHAR</span>(<span class="number">64</span>))</span><br><span class="line">CLUSTERED <span class="keyword">BY</span> (state) SORTED <span class="keyword">BY</span> (city) <span class="keyword">INTO</span> <span class="number">32</span> BUCKETS</span><br><span class="line">  STORED <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure><h3 id="Row-Format-Delimited-x2F-SerDe"><a href="#Row-Format-Delimited-x2F-SerDe" class="headerlink" title="Row Format (Delimited &#x2F; SerDe)"></a>Row Format (Delimited &#x2F; SerDe)</h3><h4 id="SerDe"><a href="#SerDe" class="headerlink" title="SerDe"></a>SerDe</h4><p>Basically, SerDe is a short name for Serializer&#x2F;Deserializer.</p><ul><li><p><strong>Deserializer</strong></p><p>The Hive deserializer converts record (string or binary) into a Java object that Hive can process.</p><p>HDFS files –&gt; InputFileFormat –&gt; &lt;key, value&gt; –&gt; Deserializer –&gt; Row object</p></li><li><p><strong>Serializer</strong></p><p>Now, the Hive serializer will take this Java object, convert it into suitable format that can be stored into HDFS.</p><p>Row object –&gt; Serializer –&gt; &lt;key, value&gt; –&gt; OutputFileFormat –&gt; HDFS files</p></li></ul><p>So, basically, a SerDe is responsible for converting the record bytes into something that can be used by Hive. Hive comes with several SerDe like JSon SerDe for JSon files, CSV SerDe for CSV files, etc.</p><p>For example, using JsonSerDe:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ROW</span> FORMAT SERDE</span><br><span class="line"><span class="string">&#x27;org.apache.hive.hcatalog.data.JsonSerDe&#x27;</span></span><br></pre></td></tr></table></figure><p>For more specific SerDes, we can check the official site: </p><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-RowFormats&amp;SerDe">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-RowFormats&amp;SerDe</a></p><h4 id="Row-Format"><a href="#Row-Format" class="headerlink" title="Row Format"></a>Row Format</h4><p>You can create tables with a custom SerDe or using a native SerDe. A native SerDe is used if <strong>ROW FORMAT</strong> is not specified or <strong>ROW FORMAT DELIMITED</strong> is specified.<br>Use the <strong>SERDE</strong> clause to create a table with a custom SerDe. </p><p>Here is an example. </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> table_name</span><br><span class="line"></span><br><span class="line">(id <span class="type">INT</span>, name STRING, published_year <span class="type">INT</span>)</span><br><span class="line"></span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line"></span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> ‘,’</span><br><span class="line"></span><br><span class="line">STORED <span class="keyword">AS</span> TEXTFILE</span><br></pre></td></tr></table></figure><p><strong>ROW FORMAT DELIMITED</strong>: This line is telling Hive to expect the file to contain one row per line. So basically, we are telling Hive that when it finds a newline character that means is a new record.</p><h3 id="Fields-Terminated"><a href="#Fields-Terminated" class="headerlink" title="Fields Terminated"></a>Fields Terminated</h3><p>For example:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> table_name</span><br><span class="line"></span><br><span class="line">(id <span class="type">INT</span>, name STRING, published_year <span class="type">INT</span>)</span><br><span class="line"></span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line"></span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> ‘,’</span><br><span class="line"></span><br><span class="line">STORED <span class="keyword">AS</span> TEXTFILE</span><br></pre></td></tr></table></figure><p><strong>FIELDS TERMINATED BY ‘,’</strong>: This is really similar to the one above, but instead of meaning rows this one means columns, this way Hive knows what delimiter you are using in your files to separate each column. If none is set the default will be used which is ctrl-A.</p><h3 id="Storage-Format"><a href="#Storage-Format" class="headerlink" title="Storage Format"></a>Storage Format</h3><p>Hive supports built-in and custom-developed file formats. </p><ul><li><p><strong>STORED AS TEXTFILE</strong></p><p>Stored as plain text files. TEXTFILE is the default file format, unless the configuration parameter <strong>hive.default.fileformat</strong> has a different setting.</p></li><li><p><strong>STORED AS SEQUENCEFILE</strong></p><p>Stored as compressed Sequence File.</p></li><li><p><strong>STORED AS AVRO</strong></p><p>Stored as Avro format.</p></li><li><p><strong>STORED AS JSONFILE</strong></p><p>Stored as Json file format.</p></li><li><p><strong>STORED AS ORC</strong></p><p>Stored as Optimized Row Columnar (ORC) file format.</p></li></ul><p>For example:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> table_name</span><br><span class="line"></span><br><span class="line">(id <span class="type">INT</span>, name STRING, published_year <span class="type">INT</span>)</span><br><span class="line"></span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line"></span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> ‘,’</span><br><span class="line"></span><br><span class="line">STORED <span class="keyword">AS</span> TEXTFILE</span><br></pre></td></tr></table></figure><p><strong>STORED AS TEXTFILE:</strong> This is to tell Hive what type of file to expect.</p><h3 id="Import-CSV-File"><a href="#Import-CSV-File" class="headerlink" title="Import CSV File"></a>Import CSV File</h3><ul><li><p><strong>Step 1</strong> : </p><p>If we want to import data from external CSV file which using comma to separate columns, we can use the <strong>CREATE TABLE</strong> statement SQL below.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> table_name</span><br><span class="line">(</span><br><span class="line">    id string,</span><br><span class="line">    name string,</span><br><span class="line">    age string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format serde</span><br><span class="line"><span class="string">&#x27;org.apache.hadoop.hive.serde2.OpenCSVSerde&#x27;</span></span><br><span class="line"><span class="keyword">with</span></span><br><span class="line">SERDEPROPERTIES</span><br><span class="line">(&quot;separatorChar&quot;<span class="operator">=</span>&quot;,&quot;,&quot;quotechar&quot;<span class="operator">=</span>&quot;\&quot;&quot;)</span><br><span class="line">STORED AS TEXTFILE</span><br></pre></td></tr></table></figure></li><li><p><strong>Step 2</strong> :  </p><p>Loading data from CSV file of HDFS.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &#x27;/home/hive/table_name.csv&#x27; overwrite into table table_name;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/449d41fb/Hive-feature-2.jpg&quot; alt=&quot;Image result for Hive feature&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;What-is-Apache-Hive&quot;&gt;&lt;a href=&quot;#What-is-Apache-Hive&quot; class=&quot;headerlink&quot; title=&quot;What is Apache Hive?&quot;&gt;&lt;/a&gt;What is Apache Hive?&lt;/h3&gt;&lt;p&gt;As the data grew in size, there was also the scarcity of Java developers who can write complex MapReduce jobs for Hadoop. Hence the advent of Hive comes which is created on top of Hadoop itself. Hive provides a SQL like a language termed HiveQL interface for users to extract data from a Hadoop system. With the simplicity provided by Hive to transform simple SQL queries into Hadoop’s MapReduce jobs, and runs them against a Hadoop cluster.&lt;/p&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="Hive" scheme="https://liuninglin.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Introduction of NiFi</title>
    <link href="https://liuninglin.github.io/posts/60c88280.html"/>
    <id>https://liuninglin.github.io/posts/60c88280.html</id>
    <published>2019-12-06T18:31:42.000Z</published>
    <updated>2022-10-01T20:38:34.624Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/60c88280/nifi-logo-horizontal.png" alt="img"></p><h3 id="Brief-of-NiFi"><a href="#Brief-of-NiFi" class="headerlink" title="Brief of NiFi"></a>Brief of NiFi</h3><p>Apache NiFi is a software project from the Apache Software Foundation designed to automate the flow of data between software systems. </p><p>Software development and the commercial support is currently offered by Hortonworks (now merged into Cloudera), who acquired NiFi’s originator, Onyara Inc.</p><p>Apache NiFi supports powerful and scalable directed graphs of data routing, transformation, and system mediation logic. Some of the high-level capabilities and objectives of Apache NiFi include:</p><span id="more"></span><ul><li>Web-based user interface<ul><li>Seamless experience between design, control, feedback, and monitoring</li></ul></li><li>Highly configurable<ul><li>Loss tolerant vs guaranteed delivery</li><li>Low latency vs high throughput</li><li>Dynamic prioritization</li><li>Flow can be modified at runtime</li><li>Back pressure</li></ul></li><li>Data Provenance<ul><li>Track dataflow from beginning to end</li></ul></li><li>Designed for extension<ul><li>Build your own processors and more</li><li>Enables rapid development and effective testing</li></ul></li><li>Secure<ul><li>SSL, SSH, HTTPS, encrypted content, etc…</li><li>Multi-tenant authorization and internal authorization&#x2F;policy management</li></ul></li></ul><p>For more information about the usage of NiFi: </p><p><a href="https://nifi.apache.org/docs/nifi-docs/html/getting-started.html">https://nifi.apache.org/docs/nifi-docs/html/getting-started.html</a></p><h3 id="Components-of-NiFi"><a href="#Components-of-NiFi" class="headerlink" title="Components of NiFi"></a>Components of NiFi</h3><ol><li><p>Processor</p><p>Providing abundant processors for processing distinct data types, connecting processors or other data processing.</p><p>Different purposes of the processor:</p><ul><li>Data Ingestion Processor such as GetXXX</li><li>Data Transformation Processor</li><li>Data Egress&#x2F;Sending Processor such as PutXXX</li><li>Routing and Mediation Processor</li><li>Databases Access Processor</li><li>Attribute Extraction Processor</li><li>System Integration Processor</li><li>Splitting and Aggregation Processor</li><li>Http and UDP Processor</li><li>AWS Processor</li></ul></li><li><p>Processor Group</p><p>Combining disparate processors into one group for easy management.</p><p>![2019-12-06 at 13.49](Introduction-of-NiFi&#x2F;2019-12-06 at 13.49.png)</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/60c88280/nifi-logo-horizontal.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Brief-of-NiFi&quot;&gt;&lt;a href=&quot;#Brief-of-NiFi&quot; class=&quot;headerlink&quot; title=&quot;Brief of NiFi&quot;&gt;&lt;/a&gt;Brief of NiFi&lt;/h3&gt;&lt;p&gt;Apache NiFi is a software project from the Apache Software Foundation designed to automate the flow of data between software systems. &lt;/p&gt;
&lt;p&gt;Software development and the commercial support is currently offered by Hortonworks (now merged into Cloudera), who acquired NiFi’s originator, Onyara Inc.&lt;/p&gt;
&lt;p&gt;Apache NiFi supports powerful and scalable directed graphs of data routing, transformation, and system mediation logic. Some of the high-level capabilities and objectives of Apache NiFi include:&lt;/p&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="NiFi" scheme="https://liuninglin.github.io/tags/NiFi/"/>
    
  </entry>
  
  <entry>
    <title>Hive-based Enterprise Data Warehouse(EDW)</title>
    <link href="https://liuninglin.github.io/posts/3465d938.html"/>
    <id>https://liuninglin.github.io/posts/3465d938.html</id>
    <published>2019-12-05T22:41:55.000Z</published>
    <updated>2022-10-01T20:38:34.618Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/3465d938/810x430-hadoop-vs-data-warehouse.jpg" alt="Image result for Hive-based EDW"></p><h2 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h2><ol><li><p>Establishing a complete Hadoop ecosystem such as using Ambari.</p><p>For how to establish a complete Hadoop ecosystem by Ambari, you can check the posts - “Step by Step Tutorial for Ambari Installation” or “Tutorial for Latest Ambari(2.7.1)”.</p></li><li><p>Installing Hive for enterprise data warehouse (EDW).</p></li><li><p>Offering visual Hive query tools such as Hue.</p></li><li><p>Installing NiFi for processing data flow itinerantly.</p></li></ol><span id="more"></span><h2 id="Technical-Architecture"><a href="#Technical-Architecture" class="headerlink" title="Technical Architecture"></a>Technical Architecture</h2><p>![Cloud-based EDW](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;Cloud-based EDW.jpg)</p><h2 id="Business-Requirements"><a href="#Business-Requirements" class="headerlink" title="Business Requirements"></a>Business Requirements</h2><ol><li>Sync order data from external MySQL database into Hive daily</li><li>Compute order amount by orderID daily</li><li>Compute total order amount daily</li><li>Sync total order amount to external MySQL database daily</li></ol><h2 id="Preparations"><a href="#Preparations" class="headerlink" title="Preparations"></a>Preparations</h2><ol><li>Creating databases and tables for Hive of Hadoop ecosystem and MySQL of the external database separately.</li><li>Inserting several order data into the external MySQL database for syncing.</li></ol><h2 id="Design-of-a-series-of-NiFi-flow"><a href="#Design-of-a-series-of-NiFi-flow" class="headerlink" title="Design of a series of NiFi flow"></a>Design of a series of NiFi flow</h2><p>![2019-12-06 at 13.27](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 13.27.png)</p><p>Firstly, we can create a processor group to package overall processors for easy management.</p><p>![2019-12-06 at 10.00](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 10.00.png)</p><p>Based on business requirements, I split requirements into main three tasks - syncing data from the external database, computing and analyzing data and syncing data to the external database.  </p><p>![2019-12-06 at 10.01](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 10.01.png)</p><p>The image above shows the main four NiFi processor groups which contain a series of NiFi processor and I will illustrate all four NiFi processor groups separately.</p><h4 id="NiFi-Group-1-Group-SynOrdersFromMySQL"><a href="#NiFi-Group-1-Group-SynOrdersFromMySQL" class="headerlink" title="NiFi Group 1 : Group_SynOrdersFromMySQL"></a>NiFi Group 1 : Group_SynOrdersFromMySQL</h4><p>![2019-12-06 at 10.07](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 10.07.png)</p><p>This flow which consists of processor ReplaceText, processor PutHive3QL, processor ExecuteSQL, and processor PutHive3Streaming shows how to grab data from the external MySQL database. Before showing the configuration of each processor, I want to shows a chart to explain the tasks of this flow.</p><p>![2019-12-06 at 10.21](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 10.21.png)</p><p>Next, I will state the configuration of each processor in each task step by step.</p><ol><li><p>Dropping Hive table partition which belongs to that day.</p><p>This task contains two processors - ReplaceText and PutHive3QL.</p><ol><li><p>Processor ReplaceText</p><p>This processor can replace the identified text segment to an alternative one. But, why we need this processor in task 1? Because the next processor does not provide a SQL input source, so we need a container for loading SQL to the Hive processor.</p><p>Adding drop SQL to blank “Replacement Value”.</p><p>![2019-12-06 at 10.30](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 10.30.png)</p><p>Drop SQL: </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> s_shop_sephora_order_delta_orc <span class="keyword">DROP</span> IF <span class="keyword">EXISTS</span> <span class="keyword">PARTITION</span>(dt<span class="operator">=</span><span class="string">&#x27;$&#123;now():format(&#x27;</span>yyyyMMdd<span class="string">&#x27;)&#125;&#x27;</span>);</span><br></pre></td></tr></table></figure><p>Using built-in code <strong>${now():format(‘yyyyMMdd’)}</strong> to get current day code.</p></li><li><p>Processor PutHive3QL </p><p>PutHive3QL can connect to Hive and execute SQL generated from the previous processor.</p><p>![2019-12-06 at 10.38](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 10.38.png)</p><p>Clicking the right arrow in the pic above to configure the Hive3ConnectionPool.</p><p>If you do not have that connection pool, you need to create a new one.</p><p>![2019-12-06 at 10.40](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 10.40.png)</p><p>The configuration of this connection pool shows blow.</p><p>![2019-12-06 at 10.40](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 10.40-5600059.png)</p></li></ol></li><li><p>Querying order data from the external database.</p><p>Using processor ExecuteSQL and configuration shows below.</p><p>![2019-12-06 at 10.51](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 10.51.png)</p><p>And DBConnectionPool shows below.</p><p>![2019-12-06 at 10.54](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 10.54.png)</p><p>Querying SQL: </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> test.sephora_order <span class="keyword">where</span> <span class="number">1</span><span class="operator">=</span><span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p>Saving data to a specific Hive table.</p><p>Using processor PutHive3Streaming for inserting dataset to a specific Hive table.</p><p>![2019-12-06 at 10.56](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 10.56.png)</p><p>Configuring AvroReader because of the type of the previous dataset.</p><p>![2019-12-06 at 10.58](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 10.58.png)</p></li></ol><p>Until now, we can test the whole flow for accuracy.</p><h4 id="NiFi-Group-2-Group-ComputePaymentNumberByOrder"><a href="#NiFi-Group-2-Group-ComputePaymentNumberByOrder" class="headerlink" title="NiFi Group 2 : Group_ComputePaymentNumberByOrder"></a>NiFi Group 2 : Group_ComputePaymentNumberByOrder</h4><p>![2019-12-06 at 11.03](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 11.03.png)</p><p>In this flow, we just use the processor to execute Hive SQL for computing data or analyzing data.</p><ol><li><p>Processor ReplaceText</p><p>The same with the previous processor in the last task, this processor just used for storing SQL.</p><p>![2019-12-06 at 11.08](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 11.08.png)</p><p>Computing SQL:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> default.temp_order_result_orc <span class="keyword">partition</span>(dt<span class="operator">=</span><span class="string">&#x27;$&#123;now():format(&#x27;</span>yyyyMMdd<span class="string">&#x27;)&#125;&#x27;</span>)</span><br><span class="line"><span class="keyword">select</span> order_num,province,city,region,address,(<span class="built_in">cast</span>(order_amount <span class="keyword">AS</span> <span class="type">FLOAT</span>) <span class="operator">+</span> <span class="number">1</span>) <span class="keyword">from</span> default.s_shop_sephora_order_delta_orc <span class="keyword">where</span> dt<span class="operator">=</span><span class="string">&#x27;$&#123;now():format(&#x27;</span>yyyyMMdd<span class="string">&#x27;)&#125;&#x27;</span>;</span><br></pre></td></tr></table></figure></li><li><p>Processor PutHive3QL</p><p>![2019-12-06 at 11.11](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 11.11.png)</p></li></ol><h4 id="NiFi-Group-3-Group-ConvergePaymentNumByOrder"><a href="#NiFi-Group-3-Group-ConvergePaymentNumByOrder" class="headerlink" title="NiFi Group 3 : Group_ConvergePaymentNumByOrder"></a>NiFi Group 3 : Group_ConvergePaymentNumByOrder</h4><p>![2019-12-06 at 11.12](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 11.12.png)</p><p>In this step, we need to compute the total order amount.</p><ol><li><p>Processor ReplaceText</p><p>The Configuration shows below.</p><p>![2019-12-06 at 11.14](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 11.14.png)</p><p>Computing SQL:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t_order_convergence_orc <span class="keyword">partition</span>(dt<span class="operator">=</span><span class="string">&#x27;$&#123;now():format(&#x27;</span>yyyyMMdd<span class="string">&#x27;)&#125;&#x27;</span>)</span><br><span class="line"><span class="keyword">select</span> order_num,<span class="built_in">sum</span>(total) <span class="keyword">from</span> temp_order_result_orc <span class="keyword">where</span> dt<span class="operator">=</span><span class="string">&#x27;$&#123;now():format(&#x27;</span>yyyyMMdd<span class="string">&#x27;)&#125;&#x27;</span> <span class="keyword">group</span> <span class="keyword">by</span> order_num;</span><br></pre></td></tr></table></figure></li><li><p>Processor PutHive3QL</p><p>The configuration shows below.</p><p>![2019-12-06 at 11.15](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 11.15.png)</p></li></ol><h4 id="NiFi-Group-4-Group-SynTotalPaymentNum2MySQL"><a href="#NiFi-Group-4-Group-SynTotalPaymentNum2MySQL" class="headerlink" title="NiFi Group 4 : Group_SynTotalPaymentNum2MySQL"></a>NiFi Group 4 : Group_SynTotalPaymentNum2MySQL</h4><p>This is the last step for this whole POC and the overview flow chart shows below.</p><p>![2019-12-06 at 11.18](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 11.18.png)</p><p>In this step, I split into five processors for querying result data from the Hive table and saving them to the specific external MySQL table. Next, I will state the function and the configuration of each processor respectively. </p><ol><li><p>Processor SelectHive3QL</p><p>Querying analysis result data from the Hive table and qualifying output format “Avro”.</p><p>![2019-12-06 at 11.30](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 11.30.png)</p><p>![2019-12-06 at 11.30](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 11.30-5603039.png)</p><p>Since we only have two output formats - Avro and CSV, we can convert Avro to SQL successively.</p><p>Querying Data:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> order_num_str,total,<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> update_time <span class="keyword">from</span> default.t_order_convergence_orc <span class="keyword">where</span> dt <span class="operator">=</span> <span class="string">&#x27;$&#123;now():format(&#x27;</span>yyyyMMdd<span class="string">&#x27;)&#125;&#x27;</span></span><br></pre></td></tr></table></figure></li><li><p>Processor SplitAvro</p><p>In this processor, we need to split the Avro dataset because the next processors cannot process the dataset.</p><p>![2019-12-06 at 11.45](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 11.45.png)</p></li><li><p>Processor ConvertAvroToJSON</p><p>Before converting to SQL, we need to convert to JSON firstly.</p><p>![2019-12-06 at 11.47](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 11.47.png)</p></li><li><p>Processor ConvertJSONToSQL</p><p>Specifying table name of the external MySQL database in this processor.</p><p>![2019-12-06 at 11.47](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 11.47-5604065.png)</p></li><li><p>Processor PutSQL</p></li></ol><p>![2019-12-06 at 12.46](Hive-based-Enterprise-Data-Warehouse-EDW&#x2F;2019-12-06 at 12.46.png)</p><h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>To sum up, we can use NiFi as a scheduling tool to process data itinerantly since NiFi provides abundant processors for distinct processing. </p><p>For now, we can run and test this whole processor group. </p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/3465d938/810x430-hadoop-vs-data-warehouse.jpg&quot; alt=&quot;Image result for Hive-based EDW&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Prerequisite&quot;&gt;&lt;a href=&quot;#Prerequisite&quot; class=&quot;headerlink&quot; title=&quot;Prerequisite&quot;&gt;&lt;/a&gt;Prerequisite&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Establishing a complete Hadoop ecosystem such as using Ambari.&lt;/p&gt;
&lt;p&gt;For how to establish a complete Hadoop ecosystem by Ambari, you can check the posts - “Step by Step Tutorial for Ambari Installation” or “Tutorial for Latest Ambari(2.7.1)”.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Installing Hive for enterprise data warehouse (EDW).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Offering visual Hive query tools such as Hue.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Installing NiFi for processing data flow itinerantly.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="Ambari" scheme="https://liuninglin.github.io/tags/Ambari/"/>
    
    <category term="Hive" scheme="https://liuninglin.github.io/tags/Hive/"/>
    
    <category term="NiFi" scheme="https://liuninglin.github.io/tags/NiFi/"/>
    
  </entry>
  
  <entry>
    <title>Installing HDF on Ambari 2.7.1 + HDP 3.0.1</title>
    <link href="https://liuninglin.github.io/posts/8c49992b.html"/>
    <id>https://liuninglin.github.io/posts/8c49992b.html</id>
    <published>2019-09-29T21:15:31.000Z</published>
    <updated>2022-10-01T20:38:34.622Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/8c49992b/hdfandhdp.png" alt="hdfandhdp"></p><p>Installed Ambari and HDP are prerequisites for installing HDF on HDP clusters. So, after that, you can follow the steps below to complete installing HDF.</p><span id="more"></span><ol><li><p>Downloading the installation package.</p><p>Accessing the URL “<a href="https://docs.cloudera.com/HDPDocuments/HDF3/HDF-3.3.1/release-notes/content/hdf_repository_locations.html&quot;">https://docs.cloudera.com/HDPDocuments/HDF3/HDF-3.3.1/release-notes/content/hdf_repository_locations.html&quot;</a> to acquire the download location for your specific OS type.</p><p>Copy the location for “HDF Management Pack” and “HDF Repo” and download these resources on the host where you installed Ambari.</p></li><li><p>Installing the HDF Management Pack.</p><ol><li><p>Stop all services on your Ambari.</p></li><li><p>Stop your Ambari by command.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ambari-server stop</span><br></pre></td></tr></table></figure></li><li><p>Run the command below on the node where the Ambari locates.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ambari-server install-mpack \</span><br><span class="line">--mpack=/tmp/hdf-ambari-mpack-&lt;version&gt;.tar.gz \</span><br><span class="line">--verbose</span><br></pre></td></tr></table></figure></li><li><p>Start your Ambari.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ambari-server start</span><br></pre></td></tr></table></figure></li></ol></li><li><p>Adding HDF service to your HDP cluster.</p><ol><li><p>Unzipping your HDF installation package to your HTTP server and ensure that you can access the HDF installation resources.</p></li><li><p>Signing in your Ambari by user admin and registering HDF-3.2.</p><p><img src="/posts/8c49992b/registerhdf.png" alt="registerhdf"></p></li><li><p>After that, you will notice a brand new stack component named “HDF-3.2.0.0” appearing on the page “Versions”.</p><p><img src="/posts/8c49992b/appearhdf.png" alt="appearhdf"></p></li></ol></li><li><p>Following steps on Ambari Service Wizard to install NiFi.</p><p>So now, you can install the component NiFi contained in HDF by following steps on Ambari Service Wizard.</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/8c49992b/hdfandhdp.png&quot; alt=&quot;hdfandhdp&quot;&gt;&lt;/p&gt;
&lt;p&gt;Installed Ambari and HDP are prerequisites for installing HDF on HDP clusters. So, after that, you can follow the steps below to complete installing HDF.&lt;/p&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="Ambari" scheme="https://liuninglin.github.io/tags/Ambari/"/>
    
  </entry>
  
  <entry>
    <title>Tutorial for Latest Ambari(2.7.1)</title>
    <link href="https://liuninglin.github.io/posts/937f0590.html"/>
    <id>https://liuninglin.github.io/posts/937f0590.html</id>
    <published>2019-03-15T21:10:17.000Z</published>
    <updated>2022-10-01T20:38:34.741Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/937f0590/ambari-2.7-12.png" alt="ambari-2.7-12"></p><span id="more"></span><p>Before I start to install Ambari 2.7.1, I wrote some key information that I need to use later.</p><h2 id="Key-Information"><a href="#Key-Information" class="headerlink" title="Key Information"></a>Key Information</h2><table><thead><tr><th align="center">hostname</th><th align="center">ip</th><th align="center">operating system</th><th align="center">RAM</th><th align="center">disk space</th><th align="center">cores of CPU</th><th>package</th></tr></thead><tbody><tr><td align="center">master01.ambari.com</td><td align="center">192.168.110.210</td><td align="center">CentOS-7-x86_64-Minimal-1611.iso</td><td align="center">20g</td><td align="center">100g</td><td align="center">16</td><td>ambari-server</td></tr><tr><td align="center">slave01.ambari.com</td><td align="center">192.168.110.211</td><td align="center">CentOS-7-x86_64-Minimal-1611.iso</td><td align="center">20g</td><td align="center">100g</td><td align="center">16</td><td>ambari-agent</td></tr><tr><td align="center">slave02.ambari.com</td><td align="center">192.168.110.212</td><td align="center">CentOS-7-x86_64-Minimal-1611.iso</td><td align="center">20g</td><td align="center">100g</td><td align="center">16</td><td>ambari-agent</td></tr></tbody></table><h2 id="VM-Setting-and-CentOS7-Installation"><a href="#VM-Setting-and-CentOS7-Installation" class="headerlink" title="VM Setting and CentOS7 Installation"></a>VM Setting and CentOS7 Installation</h2><h3 id="VM-Setting"><a href="#VM-Setting" class="headerlink" title="VM Setting"></a>VM Setting</h3><h4 id="Upload-the-IOS-file"><a href="#Upload-the-IOS-file" class="headerlink" title="Upload the IOS file"></a>Upload the IOS file</h4><p>Before we start to create VMs and install CentOS7, we ought to upload CentOS7 installation file (using minimal version) to Datacenter in VMware Vsphere. </p><ol><li><p>Choose a specific data node in your Datacenter, then upload files.</p><p><img src="/posts/937f0590/10.png" alt="10"></p></li></ol><h4 id="Create-VMs"><a href="#Create-VMs" class="headerlink" title="Create VMs"></a>Create VMs</h4><ol><li><p>Visit and login vsphere Web Client.</p><p><img src="/posts/937f0590/1.png" alt="1"></p></li><li><p>Create a folder for collecting all nodes.</p><p><img src="/posts/937f0590/2.png" alt="2"></p></li><li><p>Create a VM node in a specific folder that created in the last step.</p><p><img src="/posts/937f0590/3.png" alt="3"></p></li><li><p>Input the name of VM</p><p><img src="/posts/937f0590/4.png" alt="4"></p></li><li><p>Choose computing resources.</p><p><img src="/posts/937f0590/5.png" alt="5"></p></li><li><p>Choose the storage node.</p><p><img src="/posts/937f0590/6.png" alt="6"></p></li><li><p>Choose the compatibility of your VM.</p><p><img src="/posts/937f0590/7.png" alt="7"></p></li><li><p>Choose the version of your VM.</p><p><img src="/posts/937f0590/8.png" alt="8"></p></li><li><p>Input the number of your CPU cores, the memory size, the hard-drive size and the IOS file path of your CentOS7.</p><p><img src="/posts/937f0590/9.png" alt="9"></p></li></ol><h3 id="CentOS7-Installation"><a href="#CentOS7-Installation" class="headerlink" title="CentOS7 Installation"></a>CentOS7 Installation</h3><ol><li><p>Power on the VM.</p><p><img src="/posts/937f0590/11.png" alt="11"></p></li><li><p>Choose the language, then click the button “Continue”.</p><p><img src="/posts/937f0590/12.png" alt="12"></p></li><li><p>Start to update setting(changing date&amp;time, specifying installation destination, turning on the network).</p><p><img src="/posts/937f0590/13.png" alt="13"></p></li><li><p>Specify the installation destination.</p><p><img src="/posts/937f0590/14.png" alt="14"></p><p><img src="/posts/937f0590/15.png" alt="15"></p></li><li><p>Turn on the network or update the configuration of the network.</p><p><img src="/posts/937f0590/16.png" alt="16"></p></li><li><p>Finally, update the password of the user root.</p><p><img src="/posts/937f0590/17.png" alt="17"></p></li></ol><h2 id="File-Preparation"><a href="#File-Preparation" class="headerlink" title="File Preparation"></a>File Preparation</h2><p>We can download the requisite installation file from the official website below.</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#Ambari</span><br><span class="line">https://docs.hortonworks.com/HDPDocuments/Ambari-2.7.1.0/bk_ambari-installation/content/ambari_repositories.html</span><br><span class="line"></span><br><span class="line">#HDP</span><br><span class="line">https://docs.hortonworks.com/HDPDocuments/Ambari-2.7.1.0/bk_ambari-installation/content/hdp_30_repositories.html</span><br><span class="line"></span><br><span class="line">#HDF</span><br><span class="line">https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.2.0/release-notes/content/hdf_repository_locations.html</span><br></pre></td></tr></table></figure><ol><li><p>Ambari installation file.</p><p>ambari-2.7.1.0-centos7.tar.gz.tar</p></li><li><p>HDP installation file.</p><p>HDP-3.0.1.0-centos7-rpm.tar.gz.tar</p><p>HDP-UTILS-1.1.0.22-centos7.tar.gz.tar</p></li><li><p>HDF installation file.</p><p>hdf-ambari-mpack-3.2.0.0-520.tar.gz.tar</p><p>HDF-3.2.0.0-centos7-rpm.tar.gz.tar</p></li></ol><h2 id="System-Preparation"><a href="#System-Preparation" class="headerlink" title="System Preparation"></a>System Preparation</h2><p>Before we start to install Ambari 2.7.1 cluster, some important steps we need to complete.</p><ol><li><p>Install some basic tools.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum -y install unzip</span><br><span class="line">yum -y install wget</span><br></pre></td></tr></table></figure></li><li><p>Change the Hostname on <strong>all cluster hosts</strong>.</p><ol><li><p>Edit the Host File on <strong>all cluster hosts</strong>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/hosts</span><br></pre></td></tr></table></figure><p>Then append the content below to the Host File (Do <strong>NOT</strong> delete the original contents).</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.110.210 master01.ambari.com master01</span><br><span class="line">192.168.110.211 slave01.ambari.com slave01</span><br><span class="line">192.168.110.212 slave02.ambari.com slave02</span><br></pre></td></tr></table></figure></li><li><p>Set the Hostname on <strong>all cluster hosts</strong>.</p><p>master01.ambari.com:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname master01.ambari.com</span><br></pre></td></tr></table></figure><p>slave01.ambari.com:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname slave01.ambari.com</span><br></pre></td></tr></table></figure><p>slave02.ambari.com:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname slave02.ambari.com</span><br></pre></td></tr></table></figure></li><li><p>Edit the Network Configuration File on <strong>all cluster hosts</strong>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/sysconfig/network</span><br></pre></td></tr></table></figure><p>Then append the contents below to the file.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NETWORKING=<span class="built_in">yes</span></span><br><span class="line">HOSTNAME=&lt;fully.qualified.domain.name&gt;</span><br></pre></td></tr></table></figure></li><li><p>Test the hostname on <strong>all cluster hosts</strong>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hostname</span><br><span class="line">hostname -f</span><br></pre></td></tr></table></figure><p>If the result above is incorrect, you need to repair it until the result is what you expect it.</p></li></ol></li><li><p>Set up Password-less SSH</p><p>To have Ambari Server automatically install Ambari Agent on all your cluster hosts, you must set up a password-less SSH connection between the Ambari Server host and all other hosts in the cluster.</p><ol><li><p>Generate public and private SSH keys on <strong>the Ambari Server host</strong>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure></li><li><p>Using the ssh command to copy a   master public key file to slave nodes.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub root@slave01.ambari.com</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub root@slave02.ambari.com</span><br></pre></td></tr></table></figure></li><li><p>Test connections between the Ambari Server and the other cluster hosts.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh root@master01.ambari.com</span><br><span class="line">ssh root@slave01.ambari.com</span><br><span class="line">ssh root@slave02.ambari.com</span><br></pre></td></tr></table></figure><p>It is successful when you do not need to input any passwords.</p></li></ol></li><li><p>Enable NTP on <strong>all cluster hosts</strong>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y ntp</span><br><span class="line">systemctl <span class="built_in">enable</span> ntpd</span><br></pre></td></tr></table></figure></li><li><p>Disable SELinux on <strong>all cluster hosts</strong>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/selinux/config</span><br></pre></td></tr></table></figure><p>Then change the value of SELINUX.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure></li><li><p>Disable iptables on <strong>all cluster hosts</strong>.</p><p>For Ambari to communicate during installation, certain ports must be open and available. So, the easiest way to do this is to temporarily disable iptables.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">systemctl stop firewalld</span><br></pre></td></tr></table></figure></li><li><p>Install <strong>Oracle JDK8 (Not JRE)</strong> and add <strong>JCE</strong> extension package on <strong>all cluster hosts</strong>.</p><p>You can also install JDK8 including JCE automatically on the step that runs command “amber-server setup”</p><ol><li><p>Download the Oracle JDK8 installation file from Oracle.</p></li><li><p>Download the JCE installation file.</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html</span><br></pre></td></tr></table></figure></li><li><p>Install JDK8 firstly on <strong>all cluster hosts</strong>. </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install [your jdk8 yum installation file]</span><br></pre></td></tr></table></figure></li><li><p>Unzip JCE file to “$JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;security&#x2F;“ on <strong>all cluster hosts</strong>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip -o -j -q jce_policy-8.zip -d /usr/java/jdk1.8.0_201-amd64/jre/lib/security</span><br></pre></td></tr></table></figure></li><li><p>Reboot.</p></li><li><p>JDK HOME</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/java/jdk1.8.0_201-amd64/</span><br></pre></td></tr></table></figure></li></ol></li><li><p>Install Mysql Server on <strong>the Ambari Server host</strong>.</p><ol><li><p>Download and add the repository, then update.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</span><br><span class="line">rpm -ivh mysql-community-release-el7-5.noarch.rpm</span><br><span class="line">yum update</span><br></pre></td></tr></table></figure></li><li><p>Install mysql-server and start the service.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install mysql-server</span><br><span class="line">systemctl start mysqld</span><br></pre></td></tr></table></figure></li><li><p>Change the password of the user ROOT (the default password is blank).</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> password <span class="keyword">for</span> <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> <span class="operator">=</span> password(<span class="string">&#x27;root&#x27;</span>);</span><br><span class="line"><span class="keyword">set</span> password <span class="keyword">for</span> <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;master01.ambari.com&#x27;</span> <span class="operator">=</span> password(<span class="string">&#x27;root&#x27;</span>);</span><br></pre></td></tr></table></figure></li><li><p>Create the database “ambari” and the user “ambari”.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database ambari;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;ambari&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;ambari&#x27;</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;ambari&#x27;</span>@<span class="string">&#x27;master01.ambari.com&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;ambari&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">on</span> ambari.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;ambari&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span>;</span><br><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">on</span> ambari.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;ambari&#x27;</span>@<span class="string">&#x27;master01.ambari.com&#x27;</span>;</span><br></pre></td></tr></table></figure></li></ol></li><li><p>Download mysql-connector-java.jar and put this jar to a specific folder on <strong>the Ambari Server host</strong>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">wget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar</span><br><span class="line"></span><br><span class="line"><span class="built_in">mkdir</span> /usr/share/java</span><br><span class="line"></span><br><span class="line"><span class="built_in">cp</span> /root/mysql-connector-java-5.1.47.jar /usr/share/java/</span><br><span class="line"></span><br><span class="line"><span class="comment">#JDBC Driver Path</span></span><br><span class="line">/usr/share/java/mysql-connector-java-5.1.47.jar</span><br></pre></td></tr></table></figure></li><li><p>Setting up a local repository with <strong>NO</strong> Internet access on <strong>the Ambari Server host</strong>.</p><ol><li><p>Install and start httpd.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum -y install httpd</span><br><span class="line">systemctl start httpd</span><br></pre></td></tr></table></figure></li><li><p>Untar installation file to the web server folder.</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/www/html/hdf</span><br><span class="line">mkdir /var/www/html/hdp</span><br><span class="line"></span><br><span class="line">tar -xvf /root/ambari-2.7.1.0-centos7.tar.gz.tar -C /var/www/html</span><br><span class="line">tar -xvf /root/HDP-3.0.1.0-centos7-rpm.tar.gz.tar -C /var/www/html/hdp</span><br><span class="line">tar -xvf /root/HDP-UTILS-1.1.0.22-centos7.tar.gz.tar -C /var/www/html/hdp</span><br><span class="line">tar -xvf /root/HDF-3.2.0.0-centos7-rpm.tar.gz.tar -C /var/www/html/hdf</span><br></pre></td></tr></table></figure></li><li><p>Confirm that you can browse to the newly created local repository.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#Ambari BaseURL</span><br><span class="line">http://192.168.110.210/ambari/centos7/2.7.1.0-169/</span><br><span class="line"></span><br><span class="line">#HDP BaseURL</span><br><span class="line">http://192.168.110.210/hdp/HDP/centos7/3.0.1.0-187/</span><br><span class="line"></span><br><span class="line">#HDP-UTIL BaseURL</span><br><span class="line">http://192.168.110.210/hdp/HDP-UTILS/centos7/1.1.0.22/</span><br><span class="line"></span><br><span class="line">#HDF BaseURL</span><br><span class="line">http://192.168.110.210/hdf/HDF/centos7/3.2.0.0-520/</span><br></pre></td></tr></table></figure></li><li><p>Downloading and Editing the Ambari Repository Configuration File to Use the Local Repository.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget -nv http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.1.0/ambari.repo -O /etc/yum.repos.d/ambari.repo</span><br><span class="line"></span><br><span class="line">vi /etc/yum.repos.d/ambari.repo</span><br></pre></td></tr></table></figure><p>Replace the Ambari baseurl to your local repository.</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#VERSION_NUMBER=2.7.1.0-169</span></span><br><span class="line"><span class="attr">[ambari-2.7.1.0]</span></span><br><span class="line"><span class="comment">#json.url = http://public-repo-1.hortonworks.com/HDP/hdp_urlinfo.json</span></span><br><span class="line"><span class="attr">name</span>=<span class="string">ambari Version - ambari-2.7.1.0</span></span><br><span class="line"><span class="attr">baseurl</span>=<span class="string">http://192.168.110.210/ambari/centos7/2.7.1.0-169/</span></span><br><span class="line"><span class="attr">gpgcheck</span>=<span class="string">1</span></span><br><span class="line"><span class="attr">gpgkey</span>=<span class="string">http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.1.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins</span></span><br><span class="line"><span class="attr">enabled</span>=<span class="string">1</span></span><br><span class="line"><span class="attr">priority</span>=<span class="string">1</span></span><br></pre></td></tr></table></figure></li></ol></li><li><p>Install and set up the Ambari server on <strong>the Ambari Server host</strong>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install ambari-server</span><br></pre></td></tr></table></figure><p>Then, start to set up ambari-server.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ambari-server setup</span><br></pre></td></tr></table></figure><p><img src="/posts/937f0590/18.png" alt="18"></p></li><li><p>Run the SQL script.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql <span class="operator">-</span>uambari <span class="operator">-</span>p</span><br><span class="line"></span><br><span class="line">use ambari;</span><br><span class="line"></span><br><span class="line">source <span class="operator">/</span>var<span class="operator">/</span>lib<span class="operator">/</span>ambari<span class="operator">-</span>server<span class="operator">/</span>resources<span class="operator">/</span>Ambari<span class="operator">-</span>DDL<span class="operator">-</span>MySQL<span class="operator">-</span>CREATE.sql</span><br></pre></td></tr></table></figure></li><li><p>Start Ambari-server.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ambari-server start</span><br></pre></td></tr></table></figure></li></ol><p><img src="/posts/937f0590/19.png" alt="19"></p><h2 id="Installing-Ambari-Cluster"><a href="#Installing-Ambari-Cluster" class="headerlink" title="Installing Ambari Cluster"></a>Installing Ambari Cluster</h2><ol><li><p>Browse Ambari WebUI, then use account admin to log in.</p><p><img src="/posts/937f0590/ambari-2.7-1.png" alt="ambari-2.7-1"></p></li><li><p>Launch Install Wizard.</p><p><img src="/posts/937f0590/ambari-2.7-2.png" alt="ambari-2.7-2"></p></li><li><p>Step 0: Name your cluster.</p><p><img src="/posts/937f0590/ambari-2.7-3.png" alt="ambari-2.7-3"></p></li><li><p>Step 1: Edit BaseURL to your local repository.</p><p><img src="/posts/937f0590/ambari-2.7-4.png" alt="ambari-2.7-4"></p></li><li><p>Step 2: Input a list of hosts using the hostname and input your ssh private key of <strong>the Ambari server host</strong>.</p></li></ol><p><img src="/posts/937f0590/ambari-2.7-5.png" alt="ambari-2.7-5"></p><ol start="6"><li><p>Step 3: Confirm Hosts.</p><p><img src="/posts/937f0590/ambari-2.7-6.png" alt="ambari-2.7-6"></p></li><li><p>Step 4: Choose Services.</p><p><img src="/posts/937f0590/ambari-2.7-7.png" alt="ambari-2.7-7"></p></li><li><p>Step 5: Assign Masters.</p><p><img src="/posts/937f0590/ambari-2.7-8.png" alt="ambari-2.7-8"></p></li><li><p>Step 6: Assign Slaves and Clients.</p><p><img src="/posts/937f0590/ambari-2.7-9.png" alt="ambari-2.7-9"></p></li><li><p>Step 7-1: Input usernames and passwords for your services.</p><p><img src="/posts/937f0590/ambari-2.7-10-1.png" alt="ambari-2.7-10-1"></p></li><li><p>Step 7-2: Create Databases and users for services, then input information for login.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#Ranger</span><br><span class="line">create database ranger;</span><br><span class="line">create user &#x27;ranger&#x27;@&#x27;localhost&#x27; identified by &#x27;ranger&#x27;;</span><br><span class="line">create user &#x27;ranger&#x27;@&#x27;master01.ambari.com&#x27; identified by &#x27;ranger&#x27;;</span><br><span class="line">grant all on ranger.* to &#x27;ranger&#x27;@&#x27;localhost&#x27;;</span><br><span class="line">grant all on ranger.* to &#x27;ranger&#x27;@&#x27;master01.ambari.com&#x27;;</span><br><span class="line"></span><br><span class="line">#RangerKMS</span><br><span class="line">create database rangerkms;</span><br><span class="line">create user &#x27;rangerkms&#x27;@&#x27;localhost&#x27; identified by &#x27;rangerkms&#x27;;</span><br><span class="line">create user &#x27;rangerkms&#x27;@&#x27;master01.ambari.com&#x27; identified by &#x27;rangerkms&#x27;;</span><br><span class="line">grant all on rangerkms.* to &#x27;rangerkms&#x27;@&#x27;localhost&#x27;;</span><br><span class="line">grant all on rangerkms.* to &#x27;rangerkms&#x27;@&#x27;master01.ambari.com&#x27;;</span><br><span class="line"></span><br><span class="line">#Oozie</span><br><span class="line">create database oozie;</span><br><span class="line">create user &#x27;oozie&#x27;@&#x27;localhost&#x27; identified by &#x27;oozie&#x27;;</span><br><span class="line">create user &#x27;oozie&#x27;@&#x27;master01.ambari.com&#x27; identified by &#x27;oozie&#x27;;</span><br><span class="line">grant all on oozie.* to &#x27;oozie&#x27;@&#x27;localhost&#x27;;</span><br><span class="line">grant all on oozie.* to &#x27;oozie&#x27;@&#x27;master01.ambari.com&#x27;;</span><br><span class="line"></span><br><span class="line">#Hive</span><br><span class="line">create database hive;</span><br><span class="line">create user &#x27;hive&#x27;@&#x27;localhost&#x27; identified by &#x27;hive&#x27;;</span><br><span class="line">create user &#x27;hive&#x27;@&#x27;master01.ambari.com&#x27; identified by &#x27;hive&#x27;;</span><br><span class="line">grant all on hive.* to &#x27;hive&#x27;@&#x27;localhost&#x27;;</span><br><span class="line">grant all on hive.* to &#x27;hive&#x27;@&#x27;master01.ambari.com&#x27;;</span><br></pre></td></tr></table></figure><p><img src="/posts/937f0590/ambari-2.7-10-2.png" alt="ambari-2.7-10-2"></p></li><li><p>Step 7-3: Follow other steps to complete.</p><p><img src="/posts/937f0590/ambari-2.7-10-3-2989180.png" alt="ambari-2.7-10-3"></p><p><img src="/posts/937f0590/ambari-2.7-10-4.png" alt="ambari-2.7-10-4"></p></li></ol><p><img src="/posts/937f0590/ambari-2.7-10-5.png" alt="ambari-2.7-10-5"></p><ol start="13"><li><p>Step 8: Review</p><p><img src="/posts/937f0590/ambari-2.7-11.png" alt="ambari-2.7-11"></p></li><li><p>Step 9: Install, Start, and Test.</p></li><li><p>Summary.</p></li><li><p>Completion</p><p><img src="/posts/937f0590/ambari-2.7-12.png" alt="ambari-2.7-12"></p></li></ol><h2 id="Installing-HDF-on-Ambari-2-7-1-HDP-3-0-1"><a href="#Installing-HDF-on-Ambari-2-7-1-HDP-3-0-1" class="headerlink" title="Installing HDF on Ambari 2.7.1 + HDP 3.0.1"></a>Installing HDF on Ambari 2.7.1 + HDP 3.0.1</h2><p>Regarding the HDF installation, I will write another post to describe specifically.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/937f0590/ambari-2.7-12.png&quot; alt=&quot;ambari-2.7-12&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="Ambari" scheme="https://liuninglin.github.io/tags/Ambari/"/>
    
  </entry>
  
  <entry>
    <title>Meeting 401 Http Status Code when Visting Oozie UI by a browser in a Kerberos environment</title>
    <link href="https://liuninglin.github.io/posts/3f9aa226.html"/>
    <id>https://liuninglin.github.io/posts/3f9aa226.html</id>
    <published>2019-03-11T15:38:06.000Z</published>
    <updated>2022-10-01T20:38:34.630Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/3f9aa226/401_webui_1.png" alt="401 webui"></p><p>Using your browser such as Firefox or Chrome to access your component services in Ambari with Kerberos Authorization may confront the error message above.</p><span id="more"></span><p>The Http status code 401 showing that lacking the authorization to access specific resources. So solving this error is to assign specific authorization for the resource that you need to access.</p><blockquote><p><a href="https://community.hortonworks.com/questions/212654/nifi-processor-connection-to-schema-registry-with.html">https://community.hortonworks.com/questions/212654/nifi-processor-connection-to-schema-registry-with.html</a></p></blockquote><h2 id="Firefox"><a href="#Firefox" class="headerlink" title="Firefox"></a>Firefox</h2><ol><li><p>Configuring Firefox</p><p> Open Firefox and type the “about:config” in the address bar.</p><p> <img src="/posts/3f9aa226/401_webui_2.png" alt="401 webui"></p><p> Specify the hostname of your Ambari cluster to the properties(“network.negotiate-auth.delegation-uris” and “network.negotiate-auth.trusted-uris”)</p></li><li><p>Typing Command “kinit”<br> Assigning authorization to specific service which you want to access.</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kinit -kt [the file path of specific keytab file] [the principle of specific service]</span><br></pre></td></tr></table></figure></li></ol><h2 id="Other-Browsers"><a href="#Other-Browsers" class="headerlink" title="Other Browsers"></a>Other Browsers</h2><p>Accessing the website to get concrete solutions for specific browsers.</p><blockquote><p><a href="https://ping.force.com/Support/PingFederate/Integrations/How-to-configure-supported-browsers-for-Kerberos-NTLM">https://ping.force.com/Support/PingFederate/Integrations/How-to-configure-supported-browsers-for-Kerberos-NTLM</a></p></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/3f9aa226/401_webui_1.png&quot; alt=&quot;401 webui&quot;&gt;&lt;/p&gt;
&lt;p&gt;Using your browser such as Firefox or Chrome to access your component services in Ambari with Kerberos Authorization may confront the error message above.&lt;/p&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="Ambari" scheme="https://liuninglin.github.io/tags/Ambari/"/>
    
  </entry>
  
  <entry>
    <title>Comparison between Hortonworks and Alibaba</title>
    <link href="https://liuninglin.github.io/posts/720bb0c8.html"/>
    <id>https://liuninglin.github.io/posts/720bb0c8.html</id>
    <published>2019-03-11T15:25:52.000Z</published>
    <updated>2022-10-01T20:38:34.568Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/720bb0c8/comparison.png" alt="comparison"></p><span id="more"></span><table><thead><tr><th>公司</th><th>Hortonworks</th><th>Alibaba</th></tr></thead><tbody><tr><td>大数据产品</td><td>Ambari (提供Hadoop生态的管理、运维、升级等功能)<br>HDP (提供大部分Hadoop相关开源组件)<br>HDF (提供数据流相关的开源组件)</td><td>Dataworks (大数据开发平台，提供在线编辑工具)<br>Dataphin (新一代大数据平台，为阿里的数据中台业务中台铺路)<br>QuickBI (提供全面的报表展示服务，为Martech助力)</td></tr><tr><td>市场占有率</td><td>国际市场的三驾马车之一（Cloudera, MapR），18年底完成与Cloudera公司的合并</td><td>国际市场占有率低（有一部分Flink的原因，Flink16年出现）</td></tr><tr><td>收费模式</td><td>100%开源，提供收费技术支持及培训</td><td>具体需要联系售前</td></tr><tr><td>产品特点</td><td>100%开源，使用不受任何限制</td><td>基础架构及上层服务均需要全套阿里产品，同时数据留存在阿里</td></tr><tr><td>技术栈</td><td>Hadoop<br>HDFS<br>Yarn<br>Spark2(DataFrame + SQL)<br>Nifi<br>Kafka<br>Kerberos<br>ZooKeeper<br>Ranger<br>Oozie<br>….</td><td>MaxCompute(阿里大数据处理引擎，基于Hadoop早期版本定制)<br>Blink(基于Flink的定制版，做batch及streaming处理)<br></td></tr><tr><td>主要开发语言</td><td>Java, Scala, SQL, 部分可视化拖拽（Nifi）</td><td>可视化拖拽, SQL</td></tr><tr><td>开发成本</td><td>高(需要部署开发环境，同时需要IDE开发工具)</td><td>低(配置+SQL，所有开发均在阿里在线开发环境完成)</td></tr><tr><td>运维成本</td><td>高(对Hadoop生态的优化需要自己完成)</td><td>低</td></tr><tr><td>第三方服务&#x2F;系统对接</td><td>容易实现</td><td>需要看阿里是否支持</td></tr><tr><td>技术支持</td><td>有成熟的社区及非常完善的文档，基本上遇到的问题都能解决<br>同时Ambari还提供智能支持的收费服务，也有收费培训等</td><td>具体暂不清楚，应该会有对应的技术工程师提供完备的服务，省去了花时间自己找解决方案</td></tr><tr><td>数据安全</td><td>Kerberos+Ranger</td><td>阿里产品</td></tr><tr><td>部署方式</td><td>基于虚机&#x2F;Docker的部署，微软Azure提供深度支持</td><td>独立部署（需要有千万级数据的体量）<br>多租户方式 (适于小体量同时价格便宜)</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/720bb0c8/comparison.png&quot; alt=&quot;comparison&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
  </entry>
  
  <entry>
    <title>Cannot Start Namenode with Safemode Error</title>
    <link href="https://liuninglin.github.io/posts/ab80d8c6.html"/>
    <id>https://liuninglin.github.io/posts/ab80d8c6.html</id>
    <published>2019-03-11T15:10:19.000Z</published>
    <updated>2022-10-01T20:38:34.527Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/ab80d8c6/namenodestarterror_1.png" alt="cannot start namenode"></p><span id="more"></span><p>It’s so annoying when you see the namenode error above, and always showing the same error after restarting several times.</p><p>Fortunately, I found <a href="Cannot-start-Namenode-with-Safemode-Error/https://community.hortonworks.com/questions/14058/safe-mode-is-off.html">an answer</a> online writen by a hortonworks staff.</p><p><img src="/posts/ab80d8c6/namenodestarterror_2.png" alt="cannot start namenode"></p><p>So, we don’t need to worry about this kind of error, and we can use the command below to ignore this error.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop dfsadmin -safemode leave </span><br></pre></td></tr></table></figure><p>Using the command below to get the state of the namenode</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -safemode get </span><br></pre></td></tr></table></figure><p>Incorrect steps for stopping your Ambari cluster may casue this kind of error.<br>Following steps below can avoid.</p><ol><li><p>Clicking the button “Stop All”.</p></li><li><p>Running the commands below.  </p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ambari-agent stop</span><br><span class="line"></span><br><span class="line">ambari-server stop</span><br></pre></td></tr></table></figure></li><li><p>Powering off VMs.</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/ab80d8c6/namenodestarterror_1.png&quot; alt=&quot;cannot start namenode&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="Ambari" scheme="https://liuninglin.github.io/tags/Ambari/"/>
    
  </entry>
  
  <entry>
    <title>Cannot start ambari services with 400 status code</title>
    <link href="https://liuninglin.github.io/posts/7449623f.html"/>
    <id>https://liuninglin.github.io/posts/7449623f.html</id>
    <published>2019-03-11T14:52:10.000Z</published>
    <updated>2022-10-01T20:38:34.530Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Error-Message"><a href="#Error-Message" class="headerlink" title="Error Message"></a>Error Message</h2><p>You maybe meet this kind of error message showing below when you click the button “Start All”.</p><p><img src="/posts/7449623f/cannotstartambari_1.png" alt="cannotstartambari"></p><span id="more"></span><blockquote><p>Error message: java.lang.IllegalArgumentException: Invalid transition for servicecomponenthost, clusterName&#x3D;ambari, clusterId&#x3D;2, serviceName&#x3D;SMARTSENSE, componentName&#x3D;ACTIVITY_EXPLORER, hostname&#x3D;ambari.com, currentState&#x3D;STOPPING, newDesiredState&#x3D;STARTED</p></blockquote><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>It’s so annoying I cannot find any great solutions online until I saw <a href="Cannot-start-ambari-services-with-400-status-code/https://community.hortonworks.com/questions/215008/cannot-start-secondary-namenode-currentstatestoppi.html">this article</a> in the hortonworks community.</p><p><img src="/posts/7449623f/cannotstartambari_2.png" alt="cannotstartambari"></p><h3 id="Step1-Update-the-state-of-components-in-Ambari"><a href="#Step1-Update-the-state-of-components-in-Ambari" class="headerlink" title="Step1: Update the state of components in Ambari"></a>Step1: Update the state of components in Ambari</h3><p><img src="/posts/7449623f/cannotstartambari_3.png" alt="cannotstartambari"></p><p>So, update the table hostcomponentstate.</p><h3 id="Step2-Restart-Ambari-agents-and-server"><a href="#Step2-Restart-Ambari-agents-and-server" class="headerlink" title="Step2: Restart Ambari agents and server"></a>Step2: Restart Ambari agents and server</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ambari-agent restart</span><br><span class="line"></span><br><span class="line">ambari-server restart</span><br></pre></td></tr></table></figure><h2 id="Reason"><a href="#Reason" class="headerlink" title="Reason"></a>Reason</h2><p>Stopping your Ambari cluster in an exceptional steps will cause this kind of message such as exceptional poweroff.</p><p>So, we ought to follow the correct steps blow to avoid this kind of unexpectable error.</p><ol><li><p>Clicking the button “Stop All”.</p></li><li><p>Running the commands below.  </p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ambari-agent stop</span><br><span class="line"></span><br><span class="line">ambari-server stop</span><br></pre></td></tr></table></figure></li><li><p>Powering off VMs.</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Error-Message&quot;&gt;&lt;a href=&quot;#Error-Message&quot; class=&quot;headerlink&quot; title=&quot;Error Message&quot;&gt;&lt;/a&gt;Error Message&lt;/h2&gt;&lt;p&gt;You maybe meet this kind of error message showing below when you click the button “Start All”.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/posts/7449623f/cannotstartambari_1.png&quot; alt=&quot;cannotstartambari&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="Ambari" scheme="https://liuninglin.github.io/tags/Ambari/"/>
    
  </entry>
  
  <entry>
    <title>Kerberos Authentication Explained</title>
    <link href="https://liuninglin.github.io/posts/bfaac82a.html"/>
    <id>https://liuninglin.github.io/posts/bfaac82a.html</id>
    <published>2019-03-05T23:02:24.000Z</published>
    <updated>2022-10-01T20:38:34.628Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/bfaac82a/kerberos.png" alt="kerberos"></p><h2 id="What-is-Kerberos"><a href="#What-is-Kerberos" class="headerlink" title="What is Kerberos"></a>What is Kerberos</h2><p>In Greek mythology, Kerberos is a creature that is a three-headed dog that guards the gate toward the underworld. In a nutshell, it’s a security guardian.<br>Back to our network world, Kerberos is a computer network authorization protocol that allows clients and servers to communicate in a secure manner (symmetric key cryptography). It is designed by MIT. </p><span id="more"></span><h2 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h2><p>Kerberos is a protocol which uses within the client-server model. Before we explore the authorization process, we ought to know some terms in Kerberos.</p><ul><li><p>KDC (Key Distribution Center)<br>  Which issues the TGT to the client and contains TGS and AS.</p></li><li><p>AS (Authorization Server)<br>  Which forwards the username to a KDC.</p></li><li><p>TGS (Ticket-granting Service&#x2F;Server)<br>  A service that can generate TGT.</p></li><li><p>TGT (Ticket-granting Ticket)<br>  A data segment that is time-stamped and encrypted by using the TGS secret key and returns to the client.</p></li><li><p>principal<br>  A service node which guards by Kerberos.</p></li><li><p>SS (Service Server)<br>  Also called a principal that provides specific services for clients.</p></li><li><p>SPN (Service Principal Name)<br>  The literal meaning of this word.</p></li><li><p>Realm<br>  Which encompasses all services that you can access.</p></li></ul><h2 id="Principal"><a href="#Principal" class="headerlink" title="Principal"></a>Principal</h2><p>When requiring access to a service or host, there are four components that you need to keep back in your mind.</p><ul><li>AS</li><li>TGS</li><li>SS</li><li>Client</li></ul><p>So, let’s begin to cross this mysterious jungle.</p><p><img src="/posts/bfaac82a/Kerberos-Authentication-Explained%5Ckerberos_authentication_process.png" alt="kerberos_authentication_process"></p><h3 id="Process-1-Getting-TGT"><a href="#Process-1-Getting-TGT" class="headerlink" title="Process 1: Getting TGT"></a>Process 1: Getting TGT</h3><ol><li><p>The client sends the plaintext to AS.<br> The plaintext contains:</p><ul><li>name&#x2F;ID</li><li>timestamp</li></ul></li><li><p>AS verifies timestamp, then lookups client by client’s username to ensure that the client is a legal principle.<br> If the time gap between the client and AS is upper than 5min, the client will be refused by  AS.</p></li><li><p>AS generates a TGT and a TGS session key which used for communicating with TGS.</p></li><li><p>AS sends two messages (TGT and TGS session key) back to the client.<br> The client secret key is created by the client within the registration process (Using command “addprinc”).</p><p> TGT which encrypted by TGS secret key (client cannot decrypt TGT) contains:</p><ul><li>your name&#x2F;ID</li><li>the TGS name&#x2F;ID</li><li>timestamp</li><li>your network address</li><li>lifetime of TGT</li><li>TGT session key</li></ul><p> Info package which encrypted by client secret key which KDC owning it contains:</p><ul><li>TGS name&#x2F;ID</li><li>timestamp</li><li>lifetime</li><li>TGS session key</li></ul></li><li><p>The client fetches TGT and TGS session key which can be obtained by decrypting info package.</p></li></ol><h3 id="Process-2-Requesting-access-to-a-specific-service-principal"><a href="#Process-2-Requesting-access-to-a-specific-service-principal" class="headerlink" title="Process 2: Requesting access to a specific service(principal)"></a>Process 2: Requesting access to a specific service(principal)</h3><p>In this step, you just communicate with TGS.<br>6. The client sends the plaintext request, the authenticator and TGT to TGS.  </p><pre><code>The plaintext contains:  - service name/ID  - lifetime of the ticket for the service  The authenticator which encrypted by TGS session key contains:  - your name/ID  - timestamp  </code></pre><ol start="7"><li><p>TGS verifies if the service exists.</p></li><li><p>TGS decrypts TGT and the authenticator, then comparing properties.<br> TGS can use the TGS secret key to decrypt TGT. So TGS can obtain the TGT session key which can decrypt the authenticator. After that TGS needs to compare the information which is TGT provided by AS to the information which is the authenticator created by the client.</p></li><li><p>TGS sends the service session key and the service ticket.<br> The service ticket which encrypted by service secret key contains:</p><ul><li>your name&#x2F;ID</li><li>service name&#x2F;ID</li><li>your network address</li><li>timestamp</li><li>lifetime of this service ticket</li><li>service session key</li></ul><p> Info package which encrypted by TGS session key contains:</p><ul><li>service name&#x2F;ID</li><li>timestamp</li><li>lifetime of the service ticket</li><li>the service session key</li></ul></li><li><p>The client fetches the service ticket and the service session key which can be obtained by decrypting the info package.</p></li></ol><h3 id="Process-3-Communicating-with-the-service"><a href="#Process-3-Communicating-with-the-service" class="headerlink" title="Process 3: Communicating with the service"></a>Process 3: Communicating with the service</h3><p>From now, you just communicate with the service.<br>11. The client sends the authenticator and the service ticket to the service server.<br>    The authenticator encrypted by the service session key contains:<br>    - your name&#x2F;ID<br>    - timestamp</p><ol start="12"><li><p>The service decrypts the service ticket and the authenticator, then compares properties.<br>The service server can use the service secret key to decrypt the service ticket. So the service server can obtain the service session key which can decrypt the authenticator. After that, the service server needs to compare the information which is the service ticket provided by TGS to the information which is the authenticator created by the client.</p></li><li><p>The service sends the authenticator to the client in order to confirm its identity.<br>The authenticator encrypted by the service session key contains:</p><ul><li>the service name&#x2F;ID</li><li>timestamp</li></ul></li><li><p>The client decrypts the authenticator and knows it has been authenticated to use the service by cached the service ticket.</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/bfaac82a/kerberos.png&quot; alt=&quot;kerberos&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;What-is-Kerberos&quot;&gt;&lt;a href=&quot;#What-is-Kerberos&quot; class=&quot;headerlink&quot; title=&quot;What is Kerberos&quot;&gt;&lt;/a&gt;What is Kerberos&lt;/h2&gt;&lt;p&gt;In Greek mythology, Kerberos is a creature that is a three-headed dog that guards the gate toward the underworld. In a nutshell, it’s a security guardian.&lt;br&gt;Back to our network world, Kerberos is a computer network authorization protocol that allows clients and servers to communicate in a secure manner (symmetric key cryptography). It is designed by MIT. &lt;/p&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="Kerberos" scheme="https://liuninglin.github.io/tags/Kerberos/"/>
    
  </entry>
  
  <entry>
    <title>Using Flume to transfer data from Kafka to HDFS in Ambari(2.4.2) with Kerberos</title>
    <link href="https://liuninglin.github.io/posts/dc7e5002.html"/>
    <id>https://liuninglin.github.io/posts/dc7e5002.html</id>
    <published>2019-03-05T18:56:38.000Z</published>
    <updated>2022-10-01T20:38:34.746Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/dc7e5002/flume+kafka.png" alt="flume+kafka"></p><p>Firstly, you don’t need to read the following post if your Flume upper than 1.5.2. You can complete your configuration file by following official documents.**</p><span id="more"></span><h2 id="Versions-of-components"><a href="#Versions-of-components" class="headerlink" title="Versions of components"></a>Versions of components</h2><ul><li>Ambari: 2.4.2  </li><li>HDP: 2.4.3   <ul><li>HDFS: 2.7.1.2.4</li><li>Kafka: 0.9.0.2.4  </li><li>Flume: 1.5.2.2.4</li></ul></li></ul><h2 id="Unpredictable-problem"><a href="#Unpredictable-problem" class="headerlink" title="Unpredictable problem"></a>Unpredictable problem</h2><p>Assume that using Kafka to receive data from a topic, then store data to HDFS. It is obviously a good choice using Kafka source, memory channel, and HDFS sink. Nevertheless, it’s not easy like that after searching <a href="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.5/ds_flume/FlumeUserGuide.html">User Guide(1.5.2)</a>. </p><p><img src="/posts/dc7e5002/Using-Flume-to-transfer-data-from-Kafka-to-HDFS-in-Ambari-2-4-2-with-Kerberos%5Cflume_kafka_source.png" alt="flume_kafka_source.png"></p><p>After reading the contents above, it is pitiful that Kafka source not supporting Kerberos environment because of no properties for specifying principle and keytab file path. So why Flume 1.5.2 not supporting Kafka source? OK. Let’s check out the library within Flume (&#x2F;user&#x2F;hdp&#x2F;2.4.3.0-227&#x2F;flume&#x2F;lib). There are some libs specifying Kafka(0.8.2).</p><p><img src="/posts/dc7e5002/Using-Flume-to-transfer-data-from-Kafka-to-HDFS-in-Ambari-2-4-2-with-Kerberos%5Cflume_kafka_version.png" alt="flume_kafka_version"></p><p>Well, it’s so weird that Ambari-2.4.2 using Kafka-0.9.0, but Flume-1.5.2 in Ambari-2.4.2 not using Kafka-0.9.0 instead of using Kafka-0.8.2. I don’t know why, but I also found <a href="https://community.hortonworks.com/questions/6332/how-to-read-from-a-kafka-topic-using-spark-streami.html">similary problem</a> after googling. <a href="https://www.confluent.io/blog/apache-kafka-security-authorization-authentication-encryption/">Kafka starts to support Kerberos authorization after version 0.9.</a> So it’s obvious that we can’t use Kafka source in Flume in Ambari-2.4.2 with Kerberos. </p><p><img src="/posts/dc7e5002/Using-Flume-to-transfer-data-from-Kafka-to-HDFS-in-Ambari-2-4-2-with-Kerberos%5Ckafka_kerberos.png" alt="kafka_start_to_support_kerberos"></p><p>From this moment, we got stuck by this annoying version of conflict. After complaining, we also need to solve this problem. But, how?<br>Although Kafka-0.8.2 not supporting Kerberos, it ought to create a custom Kafka source. Appending Kerberos authorization code within your custom Kafka source that is a reasonable solution for this situation.</p><h2 id="Reasonable-solution"><a href="#Reasonable-solution" class="headerlink" title="Reasonable solution"></a>Reasonable solution</h2><h3 id="Creating-custom-Kafka-0-9-0-source"><a href="#Creating-custom-Kafka-0-9-0-source" class="headerlink" title="Creating custom Kafka(0.9.0) source"></a>Creating custom Kafka(0.9.0) source</h3><p>Writing your custom Kafka source which inheriting class AbstractSource<br>is just like a Kafka consumer.  </p><p><strong>Attention: The dependencies of Kafka need to use the version of 0.9.0.</strong></p><p>Java codes paste below.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.johnny.flume.source.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.EventDeliveryException;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.PollableSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.event.EventBuilder;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.source.AbstractSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.CommonClientConfigs;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.nio.charset.Charset;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomKafkaSource</span> <span class="keyword">extends</span> <span class="title class_">AbstractSource</span> <span class="keyword">implements</span> <span class="title class_">Configurable</span>, PollableSource &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> KafkaConsumer&lt;<span class="type">byte</span>[], String&gt; consumer;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Status <span class="title function_">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException &#123;</span><br><span class="line"></span><br><span class="line">        List&lt;Event&gt; eventList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;Event&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ConsumerRecords&lt;<span class="type">byte</span>[], String&gt; records = consumer.poll(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">            Event event;</span><br><span class="line">            Map&lt;String, String&gt; header;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;<span class="type">byte</span>[], String&gt; record : records) &#123;</span><br><span class="line">                header = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;String, String&gt;();</span><br><span class="line">                header.put(<span class="string">&quot;timestamp&quot;</span>, String.valueOf(System.currentTimeMillis()));</span><br><span class="line"></span><br><span class="line">                event = EventBuilder.withBody(record.value(), Charset.forName(<span class="string">&quot;UTF-8&quot;</span>), header);</span><br><span class="line">                eventList.add(event);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            <span class="keyword">return</span> Status.BACKOFF;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        getChannelProcessor().processEventBatch(eventList);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Status.READY;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        properties.put(<span class="string">&quot;zookeeper.connect&quot;</span>, context.getString(<span class="string">&quot;zk&quot;</span>));</span><br><span class="line">        properties.put(<span class="string">&quot;group.id&quot;</span>, context.getString(<span class="string">&quot;groupId&quot;</span>));</span><br><span class="line">        properties.put(<span class="string">&quot;auto.offset.reset&quot;</span>, <span class="string">&quot;earliest&quot;</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;bootstrap.servers&quot;</span>, context.getString(<span class="string">&quot;bootstrapServers&quot;</span>));</span><br><span class="line">        properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, <span class="string">&quot;SASL_PLAINTEXT&quot;</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="string">&quot;true&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">this</span>.consumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;<span class="type">byte</span>[], String&gt;(properties);</span><br><span class="line">        <span class="built_in">this</span>.consumer.subscribe(Arrays.asList(context.getString(<span class="string">&quot;topics&quot;</span>).split(<span class="string">&quot;,&quot;</span>)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title function_">stop</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">super</span>.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Using the code “context.getString()” can get specific property value from your configuration file.</p><h3 id="Generating-jar-package"><a href="#Generating-jar-package" class="headerlink" title="Generating jar package"></a>Generating jar package</h3><p>Utilizing your build automation tool - such as Maven, Gradle and etc - to generate your custom Kafka source jar.</p><h3 id="Replacing-Kafka-lib-and-Adding-custom-Kafka-source-jar"><a href="#Replacing-Kafka-lib-and-Adding-custom-Kafka-source-jar" class="headerlink" title="Replacing Kafka lib and Adding custom Kafka source jar"></a>Replacing Kafka lib and Adding custom Kafka source jar</h3><p>In this step, you ought to download a Kafka jar package (the version is 0.9.0), then copy the file “kafka-clients-0.9.0.0.jar” and the file that custom Kafka source jar to your lib path of Flume(&#x2F;usr&#x2F;hdp&#x2F;2.4.3.0-227&#x2F;flume&#x2F;lib). After that, you also need to delete the old version of Kafka client jar(kafka-clients-0.8.2.0.jar).</p><h3 id="Other-preparation-for-the-final-point"><a href="#Other-preparation-for-the-final-point" class="headerlink" title="Other preparation for the final point"></a>Other preparation for the final point</h3><ol><li>Creating a Kafka topic and ensuring that it’s working.</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">kinit -kt /etc/security/keytabs/kafka.service.keytab kafka/bigdata.com@CONNEXT.COM</span><br><span class="line"></span><br><span class="line">cd /usr/hdp/2.4.3.0-227/kafka/</span><br><span class="line"></span><br><span class="line">bin/kafka-topics.sh --create --zookeeper bigdata.com:2181 --replication-factor 1 --partitions 1 --topic flume-kafka</span><br><span class="line"></span><br><span class="line">bin/kafka-topics.sh --list --zookeeper bigdata.com:2181</span><br><span class="line"></span><br><span class="line">bin/kafka-topics.sh --describe --zookeeper bigdata.com:2181 --topic flume-kafka</span><br><span class="line"></span><br><span class="line">bin/kafka-console-producer.sh --topic flume-kafka --broker-list bigdata.com:6667 --security-protocol SASL_PLAINTEXT</span><br><span class="line"></span><br><span class="line">bin/kafka-console-consumer.sh --topic flume-kafka --zookeeper bigdata.com:2181 --from-beginning --security-protocol SASL_PLAINTEXT</span><br></pre></td></tr></table></figure><ol start="2"><li>Creating your storage folder in your HDFS for saving data.</li></ol><h3 id="Editing-your-flume-configuration-file"><a href="#Editing-your-flume-configuration-file" class="headerlink" title="Editing your flume configuration file"></a>Editing your flume configuration file</h3><p>Finally, it’s time to reach the peak. Just follow the configuration code below.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">amk.sources</span> <span class="string">=</span> <span class="string">k</span></span><br><span class="line"><span class="string">amk.sinks</span> <span class="string">=</span> <span class="string">h</span></span><br><span class="line"><span class="string">amk.channels</span> <span class="string">=</span> <span class="string">m</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Sources ####</span></span><br><span class="line"><span class="string">amk.sources.k.type</span> <span class="string">=</span> <span class="string">com.johnny.flume.source.kafka.CustomKafkaSource</span></span><br><span class="line"><span class="string">amk.sources.k.zk</span> <span class="string">=</span> <span class="string">bigdata.com:2181</span></span><br><span class="line"><span class="string">amk.sources.k.groupId</span> <span class="string">=</span> <span class="string">flume-kafka</span></span><br><span class="line"><span class="string">amk.sources.k.bootstrapServers</span> <span class="string">=</span> <span class="string">bigdata.com:6667</span></span><br><span class="line"><span class="string">amk.sources.k.topics</span> <span class="string">=</span> <span class="string">flume-kafka</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Sinks ####</span></span><br><span class="line"><span class="string">amk.sinks.h.type</span> <span class="string">=</span> <span class="string">hdfs</span></span><br><span class="line"><span class="string">amk.sinks.h.hdfs.fileType</span> <span class="string">=</span> <span class="string">DataStream</span></span><br><span class="line"><span class="string">amk.sinks.h.hdfs.path</span> <span class="string">=</span> <span class="string">/johnny/flume/events/%y-%m-%d</span></span><br><span class="line"><span class="string">amk.sinks.h.hdfs.filePrefix</span> <span class="string">=</span> <span class="string">events</span></span><br><span class="line"><span class="string">amk.sinks.h.hdfs.fileSuffix</span> <span class="string">=</span> <span class="string">.log</span></span><br><span class="line"><span class="string">amk.sinks.h.hdfs.round</span> <span class="string">=</span> <span class="literal">true</span></span><br><span class="line"><span class="string">amk.sinks.h.hdfs.roundValue</span> <span class="string">=</span> <span class="number">10</span></span><br><span class="line"><span class="string">amk.sinks.h.hdfs.roundUnit</span> <span class="string">=</span> <span class="string">minute</span></span><br><span class="line"><span class="string">amk.sinks.h.hdfs.useLocalTimeStamp</span> <span class="string">=</span> <span class="literal">true</span></span><br><span class="line"><span class="string">amk.sinks.h.hdfs.kerberosPrincipal</span> <span class="string">=</span> <span class="string">hdfs-bigdata@CONNEXT.COM</span></span><br><span class="line"><span class="string">amk.sinks.h.hdfs.kerberosKeytab</span> <span class="string">=</span> <span class="string">/etc/security/keytabs/hdfs.headless.keytab</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Channels ####</span></span><br><span class="line"><span class="string">amk.channels.m.type</span> <span class="string">=</span> <span class="string">memory</span></span><br><span class="line"></span><br><span class="line"><span class="string">amk.sources.k.channels</span> <span class="string">=</span> <span class="string">m</span></span><br><span class="line"><span class="string">amk.sinks.h.channel</span> <span class="string">=</span> <span class="string">m</span></span><br></pre></td></tr></table></figure><h2 id="Others-need-to-notice"><a href="#Others-need-to-notice" class="headerlink" title="Others need to notice"></a>Others need to notice</h2><p>Similarly, we also ought to write a custom Kafka sink in Flume(1.5.2) if we need it.<br>The java code pastes below. </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.johnny.flume.sink.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.sink.AbstractSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.CommonClientConfigs;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomKafkaSink</span> <span class="keyword">extends</span> <span class="title class_">AbstractSink</span> <span class="keyword">implements</span> <span class="title class_">Configurable</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> KafkaProducer&lt;String, String&gt; producer;</span><br><span class="line">    <span class="keyword">public</span> String topic;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Status <span class="title function_">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">Status</span> <span class="variable">status</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="type">Channel</span> <span class="variable">ch</span> <span class="operator">=</span> getChannel();</span><br><span class="line">        <span class="type">Transaction</span> <span class="variable">txn</span> <span class="operator">=</span> ch.getTransaction();</span><br><span class="line">        txn.begin();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line"></span><br><span class="line">            <span class="type">Event</span> <span class="variable">event</span> <span class="operator">=</span> ch.take();</span><br><span class="line">            <span class="keyword">if</span> (event == <span class="literal">null</span>) &#123;</span><br><span class="line">                status = Status.BACKOFF;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="type">byte</span>[] byte_message = event.getBody();</span><br><span class="line">            <span class="comment">//生产者</span></span><br><span class="line">            ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="built_in">this</span>.topic, <span class="keyword">new</span> <span class="title class_">String</span>(byte_message));</span><br><span class="line">            producer.send(record);</span><br><span class="line"></span><br><span class="line">            txn.commit();</span><br><span class="line">            status = Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">            txn.rollback();</span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line">            <span class="keyword">if</span> (t <span class="keyword">instanceof</span> Error) &#123;</span><br><span class="line">                <span class="keyword">throw</span> (Error) t;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            txn.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        properties.put(<span class="string">&quot;bootstrap.servers&quot;</span>, context.getString(<span class="string">&quot;bootstrapServers&quot;</span>));</span><br><span class="line">        properties.put(<span class="string">&quot;metadata.broker.list&quot;</span>, context.getString(<span class="string">&quot;brokerList&quot;</span>));</span><br><span class="line">        properties.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;serializer.class&quot;</span>, <span class="string">&quot;kafka.serializer.StringEncoder&quot;</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;request.required.acks&quot;</span>, context.getString(<span class="string">&quot;acks&quot;</span>));</span><br><span class="line">        properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, <span class="string">&quot;SASL_PLAINTEXT&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">this</span>.producer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line">        <span class="built_in">this</span>.topic = context.getString(<span class="string">&quot;topic&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The related configuration file pastes below.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#### Sinks ####</span></span><br><span class="line"><span class="string">amk.sinks.k.type</span> <span class="string">=</span> <span class="string">com.johnny.flume.sink.kafka.CustomKafkaSink</span></span><br><span class="line"><span class="string">amk.sinks.k.bootstrapServers</span> <span class="string">=</span> <span class="string">bigdata.com:6667</span></span><br><span class="line"><span class="string">amk.sinks.k.brokerList</span> <span class="string">=</span> <span class="string">bigdata.com:6667</span></span><br><span class="line"><span class="string">amk.sinks.k.acks</span> <span class="string">=</span> <span class="number">1</span></span><br><span class="line"><span class="string">amk.sinks.k.topic</span> <span class="string">=</span> <span class="string">flume-kafka</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/dc7e5002/flume+kafka.png&quot; alt=&quot;flume+kafka&quot;&gt;&lt;/p&gt;
&lt;p&gt;Firstly, you don’t need to read the following post if your Flume upper than 1.5.2. You can complete your configuration file by following official documents.**&lt;/p&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="Ambari" scheme="https://liuninglin.github.io/tags/Ambari/"/>
    
    <category term="Kafka" scheme="https://liuninglin.github.io/tags/Kafka/"/>
    
    <category term="Flume" scheme="https://liuninglin.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Using Flume in Ambari with Kerberos</title>
    <link href="https://liuninglin.github.io/posts/a833198.html"/>
    <id>https://liuninglin.github.io/posts/a833198.html</id>
    <published>2019-03-05T15:36:27.000Z</published>
    <updated>2022-10-01T20:38:34.743Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/a833198/flume.jpeg" alt="flume"></p><h2 id="What-is-Flume"><a href="#What-is-Flume" class="headerlink" title="What is Flume"></a>What is Flume</h2><blockquote><p>Flume is a distributed, reliable, and available service for efficiently <strong>collecting, aggregating, and moving</strong> large amounts of log data.</p></blockquote><p>An <strong>Event</strong> is a unit of data, and events that carrying payloads flows from source to channel to sink. All above running in a flume agent that runs in a JVM.</p><span id="more"></span><p><img src="/posts/a833198/flume_principle.png" alt="flume_principle"></p><p>Three key components of Flume:  </p><ol><li>Source</li></ol><p>The purpose of a source is to receive data from an external client and store it in a configured channel.  </p><ol start="2"><li>Channel</li></ol><p>The channel is just like a bridge which receives data from a source and buffers them till they are consumed by sinks. </p><ol start="3"><li>Sink</li></ol><p>Sinks can consume data from a channel and deliver it to another destination. The destination of the sink might be another agent or storage.</p><p><strong>Attention:</strong><br>In HDP3.0, Ambari doesn’t use Flume continuously instead of using Nifi.</p><h2 id="Kerberos-authorization"><a href="#Kerberos-authorization" class="headerlink" title="Kerberos authorization"></a>Kerberos authorization</h2><p>Before we use flume, we need to configure it in Ambari with Kerberos. Visit Ambari UI and click the “Flume” service to change the configuration file.</p><p><img src="/posts/a833198/flume_configuration.png" alt="flume_configuration"></p><p>More importantly, you need to append security content at the end of the property “flume-env template” when you using Kafka components with Kerberos. Every Kafka client needs a JAAS file to get a TGT from TGS. </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_OPTS=&quot;$JAVA_OPTS -Djava.security.auth.login.config=/home/flume/kafka-jaas.conf&quot;</span><br></pre></td></tr></table></figure><h2 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h2><p>There is a simple example that transferring log data from one pc to another pc, and I paste the configuration code below.<br>Using avro-source and avro-sink is a perfect choice to transfer data from one agent to another agent.</p><h3 id="One-PC-configuration"><a href="#One-PC-configuration" class="headerlink" title="One PC configuration"></a>One PC configuration</h3><p>source: exec-source<br>channel: memory<br>sink: avro-sink</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">exec-memory-avro.sources = exec-source</span><br><span class="line">exec-memory-avro.sinks = avro-sink</span><br><span class="line">exec-memory-avro.channels = memory-channel</span><br><span class="line"></span><br><span class="line">exec-memory-avro.sources.exec-source.type = exec</span><br><span class="line">exec-memory-avro.sources.exec-source.command = tail -F /Users/JohnnyLiu/Documents/local_flume/data.log</span><br><span class="line">exec-memory-avro.sources.exec-source.shell = /bin/sh -c</span><br><span class="line"></span><br><span class="line">exec-memory-avro.sinks.avro-sink.type = avro</span><br><span class="line">exec-memory-avro.sinks.avro-sink.hostname = bigdata.com</span><br><span class="line">exec-memory-avro.sinks.avro-sink.port = 44444</span><br><span class="line"></span><br><span class="line">exec-memory-avro.channels.memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">exec-memory-avro.sources.exec-source.channels = memory-channel</span><br><span class="line">exec-memory-avro.sinks.avro-sink.channel = memory-channel</span><br></pre></td></tr></table></figure><h3 id="Another-PC-configuration"><a href="#Another-PC-configuration" class="headerlink" title="Another PC configuration"></a>Another PC configuration</h3><p>source: avro-source<br>channel: memory<br>sink: logger</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">avro-memory-logger.sources = avro-source</span><br><span class="line">avro-memory-logger.sinks = logger-sink</span><br><span class="line">avro-memory-logger.channels = memory-channel</span><br><span class="line"></span><br><span class="line">avro-memory-logger.sources.avro-source.type = avro</span><br><span class="line">avro-memory-logger.sources.avro-source.bind = bigdata.com</span><br><span class="line">avro-memory-logger.sources.avro-source.port = 44444</span><br><span class="line"></span><br><span class="line">avro-memory-logger.sinks.logger-sink.type = logger</span><br><span class="line"></span><br><span class="line">avro-memory-logger.channels.memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">avro-memory-logger.sources.avro-source.channels = memory-channel</span><br><span class="line">avro-memory-logger.sinks.logger-sink.channel = memory-channel</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/a833198/flume.jpeg&quot; alt=&quot;flume&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;What-is-Flume&quot;&gt;&lt;a href=&quot;#What-is-Flume&quot; class=&quot;headerlink&quot; title=&quot;What is Flume&quot;&gt;&lt;/a&gt;What is Flume&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Flume is a distributed, reliable, and available service for efficiently &lt;strong&gt;collecting, aggregating, and moving&lt;/strong&gt; large amounts of log data.	&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;An &lt;strong&gt;Event&lt;/strong&gt; is a unit of data, and events that carrying payloads flows from source to channel to sink. All above running in a flume agent that runs in a JVM.&lt;/p&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="Ambari" scheme="https://liuninglin.github.io/tags/Ambari/"/>
    
    <category term="Kerberos" scheme="https://liuninglin.github.io/tags/Kerberos/"/>
    
    <category term="Flume" scheme="https://liuninglin.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Running Kafka consumer and producer in Kerberos Authorization</title>
    <link href="https://liuninglin.github.io/posts/2d304b2.html"/>
    <id>https://liuninglin.github.io/posts/2d304b2.html</id>
    <published>2019-03-04T22:12:38.000Z</published>
    <updated>2022-10-01T20:38:34.638Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/2d304b2/kafka.jpeg" alt="kafka"></p><span id="more"></span><p>No! No! No! Not this guy, we are talking about Apache Kafka.</p><p><img src="/posts/2d304b2/apachekafka.jpeg" alt="apachekafka"></p><h2 id="What-is-Kafka"><a href="#What-is-Kafka" class="headerlink" title="What is Kafka"></a>What is Kafka</h2><blockquote><p>Kafka is used for building real-time data pipelines and streaming apps. It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies.</p></blockquote><p>Feature:  </p><ol><li>Scalability  </li><li>Fault-tolerant  </li><li>Multi-source  </li><li>Real-time streaming data</li><li>Fast</li></ol><p>Versions of my Ambari components</p><ul><li>Ambari 2.7.1.0</li><li>HDP 3.0.1.0  </li><li>HDF 3.2.0.0</li></ul><h2 id="Preparation"><a href="#Preparation" class="headerlink" title="Preparation"></a>Preparation</h2><h3 id="Update-configuration-properties"><a href="#Update-configuration-properties" class="headerlink" title="Update configuration properties"></a>Update configuration properties</h3><p>When you using Ambari to manage your Kafka, you don’t need to complete some annoying installation and configuration steps. But you still need to change or add some properties for Kerberos authorization.</p><ol><li><p>Change the property “listener” in the “Kafka Broker” tab.<br> Because of using Kerberos, the protocol of the listeners must be updated.</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">listeners = SASL_PLAINTEXT://localhost:6667</span><br></pre></td></tr></table></figure></li><li><p>Update the protocol of brokers’ security.<br> Change your brokers’ protocol (“security.inter.broker.protocol<br>“) in the “Advanced Kafka-broker” tab.</p></li><li><p>Add “KAFKA_OPTS” for the JAAS configuration file.<br> After enabling Kerberos, Ambari sets up a JAAS (Java Authorization and Authorization Service) login configuration file for the Kafka client. Settings in this file will be used for any client (consumer, producer) that connects to a Kerberos-enabled Kafka cluster.<br> You don’t need to change the content of the JAAS configuration file, you just need to add a command in the “kafka-env template” in the “Advanced Kafka-env” tab.</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export KAFKA_PLAIN_PARAMS=&quot;-Djava.security.auth.login.config=/usr/hdp/3.0.1.0-187/kafka/conf/kafka_jaas.conf&quot;</span><br><span class="line">export KAFKA_OPTS=&quot;$KAFKA_PLAIN_PARAMS $KAFKA_OPTS&quot;</span><br></pre></td></tr></table></figure><p> You definitely should change the location of your JAAS file.</p></li></ol><h3 id="Create-a-Kafka-topic"><a href="#Create-a-Kafka-topic" class="headerlink" title="Create a Kafka topic"></a>Create a Kafka topic</h3><p>It is a key step that creating a Kafka topic before running a consumer and a producer.<br>Excepting the topic creation command, the other beneficial topic commands shown below.</p><ol><li><p>Creating</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/hdp/3.0.1.0-187/kafka/bin/kafka-topics.sh --create --zookeeper ambari.com:2181 --replication-factor 1 --partitions 1 --topic flume-kafka</span><br></pre></td></tr></table></figure></li><li><p>Listing all topics</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/hdp/3.0.1.0-187/kafka/bin/kafka-topics.sh --list --zookeeper ambari.com:2181</span><br></pre></td></tr></table></figure></li><li><p>Describing a specific topic</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/hdp/3.0.1.0-187/kafka/bin/kafka-topics.sh --describe --zookeeper ambari.com:2181 --topic flume-kafka</span><br></pre></td></tr></table></figure></li></ol><h2 id="Running-a-Consumer-and-a-Producer"><a href="#Running-a-Consumer-and-a-Producer" class="headerlink" title="Running a Consumer and a Producer"></a>Running a Consumer and a Producer</h2><ol><li><p>Initiating Kerberos authorization for Kafka client.</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kinit -kt /etc/security/keytabs/kafka.service.keytab kafka/hostname@REALM.COM</span><br></pre></td></tr></table></figure></li><li><p>Running a consumer</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/hdp/3.0.1.0-187/kafka/bin/kafka-console-consumer.sh --topic flume-kafka --zookeeper ambari.com:2181 --from-beginning --security-protocol SASL_PLAINTEXT</span><br></pre></td></tr></table></figure><p> The security protocol must be specific.</p></li><li><p>Running a producer</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/hdp/3.0.1.0-187/kafka/bin/kafka-console-producer.sh --topic flume-kafka --broker-list ambari.com:6667 --security-protocol SASL_PLAINTEXT</span><br></pre></td></tr></table></figure><p> The security protocol must be specific.</p></li></ol><p>So, now you can use the terminal tool to test your consumer and producer.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/2d304b2/kafka.jpeg&quot; alt=&quot;kafka&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="Ambari" scheme="https://liuninglin.github.io/tags/Ambari/"/>
    
    <category term="Kafka" scheme="https://liuninglin.github.io/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Using spark-submit to submit a Spark application in Kerberos environment</title>
    <link href="https://liuninglin.github.io/posts/b0e85a85.html"/>
    <id>https://liuninglin.github.io/posts/b0e85a85.html</id>
    <published>2019-02-28T21:23:42.000Z</published>
    <updated>2022-10-01T20:38:34.750Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/b0e85a85/spark.jpg" alt="spark"></p><span id="more"></span><p>Spark-submit is a shell script that allows you to deploy a spark application for execution, kill or request status of spark applications. So, it is generally a depolyment tool for spark application.</p><h3 id="How-spark-runs-an-application"><a href="#How-spark-runs-an-application" class="headerlink" title="How spark runs an application"></a>How spark runs an application</h3><ol><li>Create SparkContext from driver.</li><li>Request resources from Cluster Manager.</li><li>CM find Worker Node, then response to spark driver.</li><li>Spark driver connects to Worker Nodes directly, then communicate each other (such as sending tasks).</li></ol><p>So, we can notice that serveral components are ciratical - spark driver, CM, Worker Node - in this submission process. And spark can use different CM - pseudo cluster manager, standalone CM, yarn, mesos and kubernets - to request resources and schedule tasks for spark applications. Spark also provides 2 different types of deployment mode - client and cluster - for running spark driver which can creates a SparkContext.</p><h3 id="Cluster-Manager-types"><a href="#Cluster-Manager-types" class="headerlink" title="Cluster Manager types"></a>Cluster Manager types</h3><p>CM which acquires resources on the cluster is a critical key componet for a distributed system.  </p><ol><li><p>Pseudo cluster manager<br> In this kind of non-distributed single-JVM deployment mode, Spark creates all the execution components - driver, excutor and master - in the same single JVM.<br> Using this kind of cluster manager to test your spark application is very convient and easy.</p><p> Run spark locally with two worker threads:</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--master local[2]</span><br></pre></td></tr></table></figure><p> Run spark locally with as many as logical cores on your machine:</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--master local[*]</span><br></pre></td></tr></table></figure></li><li><p>Standalone<br> It is a simple cluster manager providing by Spark. We can use it as a development tool and a testing tool among the project.</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--master spark://hostname:port</span><br></pre></td></tr></table></figure></li><li><p>Hadoop Yarn<br> The resources manager in Hadoop 2.<br> I’s a excellent choice in ambari environment.</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--master yarn</span><br></pre></td></tr></table></figure></li><li><p>Apache Mesos<br> A general cluster manager that can run Hadoop MapReduce and service application.</p></li><li><p>Kubernetes (K8S)<br> An open-source system for automating deployment, scaling, and management of containerized applications.</p></li></ol><h3 id="Deployment-mode-types"><a href="#Deployment-mode-types" class="headerlink" title="Deployment mode types"></a>Deployment mode types</h3><ol><li><p>Client<br> Spark driver runs where the job is submitted outside of the cluster.</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--deploy-mode client</span><br></pre></td></tr></table></figure></li><li><p>Cluster<br> By contrast with clident deployment mode, spark driver runs inside the cluster. </p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--deploy-mode cluster</span><br></pre></td></tr></table></figure></li></ol><h3 id="Steps-for-using-spark-submit-command-to-launch-a-spark-application-in-ambari-with-kerberos-authorization"><a href="#Steps-for-using-spark-submit-command-to-launch-a-spark-application-in-ambari-with-kerberos-authorization" class="headerlink" title="Steps for using spark-submit command to launch a spark application in ambari with kerberos authorization"></a>Steps for using spark-submit command to launch a spark application in ambari with kerberos authorization</h3><ol><li><p>Get TGT<br>Before we using this tool, we need to get TGT for spark service. Using command kinit to get ticket.</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kinit -kt /etc/security/keytabs/spark.headless.keytab spark-bigdata@CONNEXT.COM </span><br></pre></td></tr></table></figure></li><li><p>Submit application</p><ol><li><p>Pseudo cluster manager</p><p> <strong>Notices:</strong></p><ul><li>The asterisk can be replaced by specific number that showing how many worker nodes to be applied. And the asterisk is just announced that acquiring worker nodes as many as logical cores on your machine.  </li><li>Specifying package path and your application class name to CM for informing entry point of your spark application.</li></ul> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/usr/hdp/current/spark-client/bin/spark-submit \</span><br><span class="line">--master local[*] \</span><br><span class="line">--class com.johnny.demo.SparkDemo \</span><br><span class="line">/root/spark-demo-1.0.0.jar \</span><br></pre></td></tr></table></figure></li><li><p>Yarn-cluster</p><p> <strong>Notices:</strong></p><ul><li>Specifying entry point of your spark application which just likes the mode of pseudo cluster manager.</li><li>Your application jar files and other files all need to upload to your cluster. Not local file system.</li></ul> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/usr/hdp/current/spark-client/bin/spark-submit \</span><br><span class="line">--class com.johnny.demo.SparkDemo \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--files /usr/hdp/current/spark-client/conf/hive-site.xml \</span><br><span class="line">--jars /usr/hdp/current/spark-client/lib/datanucleus-api-jdo-3.2.6.jar,/usr/hdp/current/spark-client/lib/datanucleus-rdbms-3.2.9.jar,/usr/hdp/current/spark-client/lib/datanucleus-core-3.2.10.jar \</span><br><span class="line">hdfs://bigdata.com:8020/johnny/oozie/workflow/shell-action/demo1/spark-demo-1.0.0.jar</span><br></pre></td></tr></table></figure></li></ol></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/b0e85a85/spark.jpg&quot; alt=&quot;spark&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="Ambari" scheme="https://liuninglin.github.io/tags/Ambari/"/>
    
    <category term="Spark" scheme="https://liuninglin.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>The Two Ways of Connecting Hive For Developing or Testing</title>
    <link href="https://liuninglin.github.io/posts/87dabd80.html"/>
    <id>https://liuninglin.github.io/posts/87dabd80.html</id>
    <published>2019-02-26T19:48:32.000Z</published>
    <updated>2022-10-01T20:38:34.666Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/87dabd80/The-Two-Ways-of-Connecting-Hive-For-Developing-or-Testing%5Chive.jpg" alt="hive"></p><span id="more"></span><h2 id="Hive-CLI"><a href="#Hive-CLI" class="headerlink" title="Hive CLI"></a>Hive CLI</h2><p>Hive CLI a legacy tool that had two main use cases. The first is that it served as a thick client for SQL on Hadoop. And the second is that it served as a command-line tool for Hiver Server. Hive Server had been deprecated and removed from Hive. So, it is not an ideal way to connect Hive by using Hive CLI.</p><p>Command below for using Hive CLI.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br></pre></td></tr></table></figure><h2 id="Beeline-CLI"><a href="#Beeline-CLI" class="headerlink" title="Beeline CLI"></a>Beeline CLI</h2><p>It is a client tool for connecting Hive Server2 through JDBC. Then you can use SQL to obtain or process data.<br>Because of using JDBC, there are two tunnels for transmitting data. The first way is zookeeper which is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. The second way is the apache thrift server which is RPC framework for scalable cross-language service development.</p><p>Before we use beeline, we need to get TGT from TGS for using hive service when your Hadoop clusters using Kerberos authorization. After that, you can use Hive directly.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kinit -kt [your hive service keytab file path] [your hive service principal]</span><br></pre></td></tr></table></figure><p>You can checkout your kerberos.csv file to get a specific service keytab file path and service principal.</p><ol><li>Zookeeper</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">beeline </span><br><span class="line"></span><br><span class="line">!connect jdbc:hive2://bigdata.com:2181/sephora;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;principal=hive/bigdata.com@CONNEXT.COM</span><br></pre></td></tr></table></figure><p>So, you maybe want to get your own JDBC connection URL by using zookeeper. You can visit your own Ambari UI, then click “[Services]-&gt;[Hive]”. In your hive dashboard, you can get your own URL.</p><p><img src="/posts/87dabd80/The-Two-Ways-of-Connecting-Hive-For-Developing-or-Testing%5Chive-connect-zookeeper-url.png" alt="JDBC URL by using zookeeper"></p><p>After this, we may be encounter a bug that needs to input username and password. So, just click “Enter” two times because of owning the authorization for hive service.</p><blockquote><p><a href="https://issues.apache.org/jira/browse/HIVE-9144">https://issues.apache.org/jira/browse/HIVE-9144</a></p></blockquote><p><img src="/posts/87dabd80/The-Two-Ways-of-Connecting-Hive-For-Developing-or-Testing%5Chive-connect-zookeeper.png" alt="zookeeper"></p><ol start="2"><li>Thrift Server</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">beeline</span><br><span class="line"></span><br><span class="line">!connect jdbc:hive2://bigdata.com:10001/sephora;transportMode=http;httpPath=cliservice;principal=hive/bigdata.com@CONNEXT.COM</span><br></pre></td></tr></table></figure><p>We can get parameter values by visiting Ambari UI and checking the configuration of Hive.</p><p><img src="/posts/87dabd80/The-Two-Ways-of-Connecting-Hive-For-Developing-or-Testing%5Chive-connect-thrift-url.png" alt="jdbc url by using thrift"></p><p>Just like using zookeeper, we also encounter that bug. So, just click “Enter” two times.</p><p><img src="/posts/87dabd80/The-Two-Ways-of-Connecting-Hive-For-Developing-or-Testing%5Chive-connect-thrift.png" alt="thrift"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/87dabd80/The-Two-Ways-of-Connecting-Hive-For-Developing-or-Testing%5Chive.jpg&quot; alt=&quot;hive&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="Ambari" scheme="https://liuninglin.github.io/tags/Ambari/"/>
    
    <category term="Hive" scheme="https://liuninglin.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Linux Command - netstat</title>
    <link href="https://liuninglin.github.io/posts/187005a0.html"/>
    <id>https://liuninglin.github.io/posts/187005a0.html</id>
    <published>2019-02-26T15:53:19.000Z</published>
    <updated>2022-10-01T20:38:34.629Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/187005a0/netstat.jpg" alt="netstat"></p><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><blockquote><p>source: <a href>https://www.tecmint.com/20-netstat-commands-for-linux-network-management/</a></p></blockquote><p>netstat(network statistics) is a command line tool for monitoring connections both incoming and outgoing as well as viewing routing tables, interface statistics, etc.</p><span id="more"></span><h2 id="Options"><a href="#Options" class="headerlink" title="Options"></a>Options</h2><p>So, let’s begin to learn some useful options for netstat.</p><ol><li><p>-a</p><p>Show all.</p></li><li><p>-n</p><p>Show networks as numbers, not hostnames.</p></li><li><p>-p</p><p>Show protocol.</p></li></ol><h2 id="Most-repeated-command"><a href="#Most-repeated-command" class="headerlink" title="Most repeated command"></a>Most repeated command</h2><p>You can use the practical command below to find listening programs</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -anp | grep 8080</span><br></pre></td></tr></table></figure><p>And you also can use telnet to check specific port.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">telnet [hostname or ip] [port]</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/187005a0/netstat.jpg&quot; alt=&quot;netstat&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Definition&quot;&gt;&lt;a href=&quot;#Definition&quot; class=&quot;headerlink&quot; title=&quot;Definition&quot;&gt;&lt;/a&gt;Definition&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;source: &lt;a href&gt;https://www.tecmint.com/20-netstat-commands-for-linux-network-management/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;netstat(network statistics) is a command line tool for monitoring connections both incoming and outgoing as well as viewing routing tables, interface statistics, etc.&lt;/p&gt;</summary>
    
    
    
    <category term="Operation" scheme="https://liuninglin.github.io/categories/Operation/"/>
    
    <category term="Linux" scheme="https://liuninglin.github.io/categories/Operation/Linux/"/>
    
    
    <category term="Linux" scheme="https://liuninglin.github.io/tags/Linux/"/>
    
    <category term="Linux Command" scheme="https://liuninglin.github.io/tags/Linux-Command/"/>
    
  </entry>
  
  <entry>
    <title>Enable Kerberos on Ambari</title>
    <link href="https://liuninglin.github.io/posts/3f0454be.html"/>
    <id>https://liuninglin.github.io/posts/3f0454be.html</id>
    <published>2019-02-11T15:41:50.000Z</published>
    <updated>2022-10-01T22:15:07.869Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/3f0454be/kerberos.jpg" alt="kerberos"></p><h2 id="Install-a-new-MIT-KDC"><a href="#Install-a-new-MIT-KDC" class="headerlink" title="Install a new MIT KDC"></a>Install a new MIT KDC</h2><h3 id="Install-the-KDC-server"><a href="#Install-the-KDC-server" class="headerlink" title="Install the KDC server"></a>Install the KDC server</h3><ol><li><p>Install a new version of the KDC server</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y krb5-server krb5-libs krb5-workstation</span><br></pre></td></tr></table></figure></li></ol><span id="more"></span><ol start="2"><li><p>Update the KDC server configuration file</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/krb5.conf</span><br></pre></td></tr></table></figure><ol><li><p>Update the name of realms and ensure the value of “renew_lifetime” is “7d”</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[libdefaults]</span><br><span class="line">default_realm = BIGDATA.COM</span><br><span class="line">dns_lookup_realm = false</span><br><span class="line">dns_lookup_kdc = false</span><br><span class="line">ticket_lifetime = 24h</span><br><span class="line">renew_lifetime = 7d</span><br><span class="line">forwardable = true</span><br></pre></td></tr></table></figure></li><li><p>Update the item “realms”</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[realms]</span><br><span class="line">BIGDATA.COM = &#123;</span><br><span class="line">kdc = master1.bigdata.com</span><br><span class="line">admin_server = master1.bigdata.com</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Update the item “domain_realm”</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[domain_realm]</span><br><span class="line">.bigdata.com = BIGDATA.COM</span><br><span class="line">bigdata.com = BIGDATA.COM</span><br></pre></td></tr></table></figure></li></ol></li></ol><h3 id="Create-the-Kerberos-database"><a href="#Create-the-Kerberos-database" class="headerlink" title="Create the Kerberos database"></a>Create the Kerberos database</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kdb5_util create -s</span><br></pre></td></tr></table></figure><h3 id="Start-the-KDC"><a href="#Start-the-KDC" class="headerlink" title="Start the KDC"></a>Start the KDC</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/etc/rc.d/init.d/krb5kdc start</span><br><span class="line">/etc/rc.d/init.d/kadmin start</span><br></pre></td></tr></table></figure><p>Set up the KDC server to auto-start on boot.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chkconfig krb5kdc on</span><br><span class="line">chkconfig kadmin on</span><br></pre></td></tr></table></figure><h3 id="Create-a-Kerberos-Admin"><a href="#Create-a-Kerberos-Admin" class="headerlink" title="Create a Kerberos Admin"></a>Create a Kerberos Admin</h3><p>You need to create an admin account, then provide this admin credentials for enabling Kerberos on Ambari.</p><ol><li><p>Create a KDC admin</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q &quot;addprinc admin/admin&quot;</span><br></pre></td></tr></table></figure></li><li><p>Ensure this admin account include the authorization to enter into the specific realms</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /var/kerberos/krb5kdc/kadm5.acl</span><br></pre></td></tr></table></figure> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*/admin@BIGDATA.COM *</span><br></pre></td></tr></table></figure></li><li><p>Restart the kadmin process</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/rc.d/init.d/kadmin restart</span><br></pre></td></tr></table></figure></li></ol><h2 id="Install-the-JCE"><a href="#Install-the-JCE" class="headerlink" title="Install the JCE"></a>Install the JCE</h2><p><strong>If you already use the JDK that contains the JCE, you ought to skip this step.</strong></p><ol><li><p>Download the specific version of the JCE file</p><p> For Oracle JDK 1.8:</p> <figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html</span><br></pre></td></tr></table></figure><p> For Oracle JDK 1.7:</p> <figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.oracle.com/technetwork/java/javase/downloads/jce-7-download-432124.html</span><br></pre></td></tr></table></figure></li><li><p>Add the JCE file to the JDK installation direction</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$JAVA_HOME/jre/lib/security/</span><br></pre></td></tr></table></figure></li></ol><h2 id="Enabling-the-Kerberos"><a href="#Enabling-the-Kerberos" class="headerlink" title="Enabling the Kerberos"></a>Enabling the Kerberos</h2><ol><li>Enter into the configuration of the kerberos</li></ol><p>  <img src="/posts/3f0454be/Enable-Kerberos-on-Ambarik1.png" alt="click the top of the website"></p><p>  <img src="/posts/3f0454be/k2.png" alt="enable the kerberos"></p><ol start="2"><li>Choose the type of KDC</li></ol><p>  <img src="/posts/3f0454be/k3.png" alt="choose the type of the KDC"></p><ol start="3"><li>Configure the Kerberos</li></ol><p>  <img src="/posts/3f0454be/k4.png" alt="configure the kerberos"></p><ol start="4"><li>Install and test the Kerberos client</li></ol><p>  <img src="/posts/3f0454be/k5.png" alt="install and test the kerberos client"></p><ol start="5"><li>Configure identities<br>  <strong>You can just click the button “next”.</strong></li></ol><p>  <img src="/posts/3f0454be/k6.png" alt="configure identities"></p><ol start="6"><li>Confirm Configuration<br>  <strong>You need to download the CSV file that contains a list of the principals and keytabs.</strong></li></ol><p>  <img src="/posts/3f0454be/k7.png" alt="confirm configuration"></p><ol start="7"><li>Stop services</li></ol><p>  <img src="/posts/3f0454be/k8.png" alt="stop services"></p><ol start="8"><li>Kerberos cluster</li></ol><p>  <img src="/posts/3f0454be/k9.png" alt="kerberize cluster"></p><ol start="9"><li>Start and test services</li></ol><p>  <img src="/posts/3f0454be/k10.png" alt="start and test services"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/3f0454be/kerberos.jpg&quot; alt=&quot;kerberos&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Install-a-new-MIT-KDC&quot;&gt;&lt;a href=&quot;#Install-a-new-MIT-KDC&quot; class=&quot;headerlink&quot; title=&quot;Install a new MIT KDC&quot;&gt;&lt;/a&gt;Install a new MIT KDC&lt;/h2&gt;&lt;h3 id=&quot;Install-the-KDC-server&quot;&gt;&lt;a href=&quot;#Install-the-KDC-server&quot; class=&quot;headerlink&quot; title=&quot;Install the KDC server&quot;&gt;&lt;/a&gt;Install the KDC server&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Install a new version of the KDC server&lt;/p&gt;
 &lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;yum install -y krb5-server krb5-libs krb5-workstation&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="Ambari" scheme="https://liuninglin.github.io/tags/Ambari/"/>
    
    <category term="Kerberos" scheme="https://liuninglin.github.io/tags/Kerberos/"/>
    
  </entry>
  
  <entry>
    <title>How to Recover a Crashed Mysql Database</title>
    <link href="https://liuninglin.github.io/posts/4cf6882d.html"/>
    <id>https://liuninglin.github.io/posts/4cf6882d.html</id>
    <published>2019-02-02T18:45:53.000Z</published>
    <updated>2022-10-01T20:38:34.621Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/4cf6882d/How-to-Recover-a-Crashed-Mysql-Database%5Ccrash.jpg" alt="crash"></p><span id="more"></span><p>Among the operation of Ambari, we may confront some annoying failure, such as a crashed MySQL database. So, we can run commands below to recover this kind of problem.</p><ol><li><p>Check the status of the crashed table.</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">check</span> <span class="keyword">table</span> [<span class="keyword">table</span> name]</span><br></pre></td></tr></table></figure></li><li><p>Repair crashed table.</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">repaire <span class="keyword">table</span> [<span class="keyword">table</span> name]</span><br></pre></td></tr></table></figure></li></ol><p>On the other hand, you can use commands below to repair whole crashed tables.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqlcheck -uroot -p --repair --all-databases</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myisamchk --recover *.MYI</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/4cf6882d/How-to-Recover-a-Crashed-Mysql-Database%5Ccrash.jpg&quot; alt=&quot;crash&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Database" scheme="https://liuninglin.github.io/categories/Database/"/>
    
    <category term="Mysql" scheme="https://liuninglin.github.io/categories/Database/Mysql/"/>
    
    
    <category term="Ambari" scheme="https://liuninglin.github.io/tags/Ambari/"/>
    
    <category term="Mysql" scheme="https://liuninglin.github.io/tags/Mysql/"/>
    
  </entry>
  
  <entry>
    <title>Tar command in Linux</title>
    <link href="https://liuninglin.github.io/posts/d36bfb53.html"/>
    <id>https://liuninglin.github.io/posts/d36bfb53.html</id>
    <published>2019-01-30T15:03:37.000Z</published>
    <updated>2022-10-01T20:38:34.663Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/d36bfb53/Tar-command-in-Linux%5Ctar.png" alt="tar">  </p><p>options:<br>-c: create the archive<br>-x: extract the archive<br>-f: create or extract with the specific archive filename<br>-t: displays or lists files in an archived file<br>-v: display info<br>-z: using gzip</p><p>-C: extract files into a specific directory, not the current directory</p><span id="more"></span><ol><li><p>Creating an uncompressed tar archive.  </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -cvf file.tar directory</span><br></pre></td></tr></table></figure></li><li><p>Creating a compressed tar archive by using gzip.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -cvzf file.tar.gz directory</span><br></pre></td></tr></table></figure></li><li><p>Extracting files from archive.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf file.tar</span><br></pre></td></tr></table></figure></li><li><p>Extracting gzip tar archive.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -xvzf file.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>Extracting files into a specific directory.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf file.tar -C /var/html/</span><br></pre></td></tr></table></figure></li><li><p>Displays or lists files in the archived file.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -tvf file.tar</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/d36bfb53/Tar-command-in-Linux%5Ctar.png&quot; alt=&quot;tar&quot;&gt;  &lt;/p&gt;
&lt;p&gt;options:&lt;br&gt;-c: create the archive&lt;br&gt;-x: extract the archive&lt;br&gt;-f: create or extract with the specific archive filename&lt;br&gt;-t: displays or lists files in an archived file&lt;br&gt;-v: display info&lt;br&gt;-z: using gzip&lt;/p&gt;
&lt;p&gt;-C: extract files into a specific directory, not the current directory&lt;/p&gt;</summary>
    
    
    
    <category term="Operation" scheme="https://liuninglin.github.io/categories/Operation/"/>
    
    <category term="Linux" scheme="https://liuninglin.github.io/categories/Operation/Linux/"/>
    
    
    <category term="Linux" scheme="https://liuninglin.github.io/tags/Linux/"/>
    
    <category term="Linux Command" scheme="https://liuninglin.github.io/tags/Linux-Command/"/>
    
  </entry>
  
  <entry>
    <title>MySQL Basic Operations for User Account Management</title>
    <link href="https://liuninglin.github.io/posts/f4c81e09.html"/>
    <id>https://liuninglin.github.io/posts/f4c81e09.html</id>
    <published>2019-01-29T21:22:43.000Z</published>
    <updated>2022-10-01T20:38:34.631Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/f4c81e09/Mysql-Basic-operations-for-User-Account-Management%5Cmysql.jpg" alt="mysql">  </p><ol><li><p>Install MySQL server (not MySQL package)</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install mysql-server</span><br></pre></td></tr></table></figure></li><li><p>Start and launch MySQL</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">service mysqld start</span><br><span class="line"></span><br><span class="line">mysql -uroot -p</span><br></pre></td></tr></table></figure></li></ol><span id="more"></span><ol start="3"><li><p>Add users.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE USER <span class="string">&#x27;user_name&#x27;</span>@<span class="string">&#x27;host&#x27;</span> IDENTIFIED BY <span class="string">&#x27;password&#x27;</span>;</span><br></pre></td></tr></table></figure><p> The value of host:  </p><ul><li>%: connect to mysql server from any other machine    </li><li>localhost: just local host  </li><li>ip_address: specific ip address</li></ul></li><li><p>Grant authority to users.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GRANT ALL PRIVILEGES on [db_name].[db_table_name] TO [mysql_user_name]@[the_value_of_host] IDENTIFIED BY <span class="string">&#x27;[mysql_user_password]&#x27;</span>;</span><br></pre></td></tr></table></figure><p> [all privileges] can be set with specific authority(“select,delete,update,create,drop”)</p><p> [db_name] can be set with all databases(“*”)</p><p> [db_table_name] can be set with all tables(“*”)</p></li><li><p>Update password.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER USER <span class="string">&#x27;user_name&#x27;</span>@<span class="string">&#x27;host&#x27;</span> IDENTIFIED BY <span class="string">&#x27;password&#x27;</span>;</span><br></pre></td></tr></table></figure></li><li><p>Delete users.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP USER <span class="string">&#x27;user_name&#x27;</span>@<span class="string">&#x27;host&#x27;</span>;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/f4c81e09/Mysql-Basic-operations-for-User-Account-Management%5Cmysql.jpg&quot; alt=&quot;mysql&quot;&gt;  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Install MySQL server (not MySQL package)&lt;/p&gt;
 &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;yum -y install mysql-server&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Start and launch MySQL&lt;/p&gt;
 &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;service mysqld start&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mysql -uroot -p&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Operation" scheme="https://liuninglin.github.io/categories/Operation/"/>
    
    <category term="Linux" scheme="https://liuninglin.github.io/categories/Operation/Linux/"/>
    
    
    <category term="Linux" scheme="https://liuninglin.github.io/tags/Linux/"/>
    
    <category term="Mysql" scheme="https://liuninglin.github.io/tags/Mysql/"/>
    
  </entry>
  
  <entry>
    <title>Auto-start Services on Boot in Linux</title>
    <link href="https://liuninglin.github.io/posts/857f09d5.html"/>
    <id>https://liuninglin.github.io/posts/857f09d5.html</id>
    <published>2019-01-29T21:04:09.000Z</published>
    <updated>2022-10-01T20:38:34.525Z</updated>
    
    <content type="html"><![CDATA[<h2 id="CentOS6"><a href="#CentOS6" class="headerlink" title="CentOS6"></a>CentOS6</h2><ol><li><p>Show the list of services that start on boot.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig --list</span><br></pre></td></tr></table></figure></li><li><p>Add a service to auto-start on boot.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chkconfig --add service_name</span><br><span class="line">chkconfig service_name on</span><br></pre></td></tr></table></figure></li></ol><span id="more"></span><ol start="3"><li><p>Confirm script is added successfully</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig --list service_name</span><br></pre></td></tr></table></figure></li><li><p>Disable an auto-start service.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chkconfig service_name off</span><br><span class="line">chkconfig --del service_name</span><br></pre></td></tr></table></figure></li></ol><h2 id="CentOS7"><a href="#CentOS7" class="headerlink" title="CentOS7"></a>CentOS7</h2><ol><li><p>Add a service to auto-start on boot.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> service_name</span><br></pre></td></tr></table></figure></li><li><p>Disable an auto-start service.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">disable</span> service_name</span><br></pre></td></tr></table></figure></li><li><p>Check the status of auto-start service.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status service_name</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;CentOS6&quot;&gt;&lt;a href=&quot;#CentOS6&quot; class=&quot;headerlink&quot; title=&quot;CentOS6&quot;&gt;&lt;/a&gt;CentOS6&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Show the list of services that start on boot.&lt;/p&gt;
 &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;chkconfig --list&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add a service to auto-start on boot.&lt;/p&gt;
 &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;chkconfig --add service_name&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;chkconfig service_name on&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Operation" scheme="https://liuninglin.github.io/categories/Operation/"/>
    
    <category term="Linux" scheme="https://liuninglin.github.io/categories/Operation/Linux/"/>
    
    
    <category term="Linux" scheme="https://liuninglin.github.io/tags/Linux/"/>
    
    <category term="Linux Command" scheme="https://liuninglin.github.io/tags/Linux-Command/"/>
    
  </entry>
  
  <entry>
    <title>Attention for Using Markdown code block</title>
    <link href="https://liuninglin.github.io/posts/d2fb4425.html"/>
    <id>https://liuninglin.github.io/posts/d2fb4425.html</id>
    <published>2019-01-28T20:19:52.000Z</published>
    <updated>2022-10-01T20:38:34.524Z</updated>
    
    <content type="html"><![CDATA[<p>When you are using markdown code block, you may meet a strange circumstance that disorder within code blocks. Just like the image below.  </p><span id="more"></span>  <p><img src="/posts/d2fb4425/Attention-for-Using-Markdown-code-block%5Cattention_for_using_code_block.png" alt="attention_for_using_code_block"><br>So, when you confront the same problem like me, you just need to <strong>delete your space symbols after your end of code block</strong>. If you put one more space after your code block, the system will not recognize that’s an end signal of the code block.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;When you are using markdown code block, you may meet a strange circumstance that disorder within code blocks. Just like the image below.  &lt;/p&gt;</summary>
    
    
    
    <category term="Language" scheme="https://liuninglin.github.io/categories/Language/"/>
    
    <category term="Markdown" scheme="https://liuninglin.github.io/categories/Language/Markdown/"/>
    
    
    <category term="Markdown" scheme="https://liuninglin.github.io/tags/Markdown/"/>
    
    <category term="Hexo" scheme="https://liuninglin.github.io/tags/Hexo/"/>
    
    <category term="Blog" scheme="https://liuninglin.github.io/tags/Blog/"/>
    
  </entry>
  
  <entry>
    <title>Step by Step Tutorial for Ambari Installation</title>
    <link href="https://liuninglin.github.io/posts/7f0f012a.html"/>
    <id>https://liuninglin.github.io/posts/7f0f012a.html</id>
    <published>2019-01-28T19:22:51.000Z</published>
    <updated>2022-10-01T20:38:34.662Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/7f0f012a/features.png" alt="features"> </p><h2 id="System-Requirements"><a href="#System-Requirements" class="headerlink" title="System Requirements"></a>System Requirements</h2><p>At first, we need to think about the setups of VMs. So, I list a table for VMs below.</p><table><thead><tr><th align="center">hostname</th><th align="center">ip</th><th align="center">operating system</th><th align="center">RAM</th><th align="center">disk space</th><th align="center">package</th></tr></thead><tbody><tr><td align="center">master1.bigdata.com</td><td align="center">192.168.110.150</td><td align="center">centos6.5 64位</td><td align="center">16g</td><td align="center">50g</td><td align="center">ambari-server<br>namenode</td></tr><tr><td align="center">slave1.bigdata.com</td><td align="center">192.168.110.151</td><td align="center">centos6.5 64位</td><td align="center">16g</td><td align="center">50g</td><td align="center">ambari-agent<br>datanode</td></tr><tr><td align="center">slave2.bigdata.com</td><td align="center">192.168.110.152</td><td align="center">centos6.5 64位</td><td align="center">16g</td><td align="center">50g</td><td align="center">ambari-agent<br>datanode</td></tr></tbody></table><span id="more"></span><h2 id="VM-Deployment"><a href="#VM-Deployment" class="headerlink" title="VM Deployment"></a>VM Deployment</h2><ol><li><p>Login VMware vsphere client system</p></li><li><p>Create 3 VMs with specific system requirements</p></li><li><p>Set the name of VM  </p></li><li><p>Choose data storage  </p></li><li><p>Choose hardware configuration  </p></li><li><p>Load centos6 ios file into CD-ROM when booting<br> You can use the minimum version of the centos. Then you need to click the option “Running system installation with system booting”.</p></li><li><p>Configuration complete  </p></li><li><p>Operating system installation  </p></li><li><p>Power on the VM</p><ol start="2"><li>Choose the first one.<br><img src="/posts/7f0f012a/li_1.png" alt="Choose the first one"></li></ol></li><li><p>Click “skip” to continue.<br>  <img src="/posts/7f0f012a/li_2.png" alt="Click &quot;skip&quot; to continue"></p></li><li><p>Choose language, keyborad and timezone.<br><img src="/posts/7f0f012a/li_3.png" alt="Choose language"><br>  <img src="/posts/7f0f012a/li_4.png" alt="Choose keyboard"><br>  <img src="/posts/7f0f012a/li_5.png" alt="Choose timezone"></p></li><li><p>Set root password.<br>  <img src="/posts/7f0f012a/li_6.png" alt="Set root password">  </p></li><li><p>Click “Using entire drive” to continue.<br><img src="/posts/7f0f012a/li_7.png" alt="Click &quot;Using entire drive&quot; to continue">  </p></li><li><p>Choose “Write changes to disk” to continue.<br> <img src="/posts/7f0f012a/li_8.png" alt="Choose &quot;Write changes to disk&quot; to continue">  </p></li><li><p>Reboot.<br><img src="/posts/7f0f012a/li_9.png" alt="Reboot"></p></li></ol><h2 id="VM-Preparation"><a href="#VM-Preparation" class="headerlink" title="VM Preparation"></a>VM Preparation</h2><h3 id="1-Configuration-and-Installation-for-VMs-master-and-slave-servers"><a href="#1-Configuration-and-Installation-for-VMs-master-and-slave-servers" class="headerlink" title="1. Configuration and Installation for VMs(master and slave servers)"></a>1. Configuration and Installation for VMs(master and slave servers)</h3><ol><li><p>Install java.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install java</span><br></pre></td></tr></table></figure></li><li><p>Enable network adapter.  </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line">vi /etc/sysconfig/network-scripts/ifcfg-eth1</span><br><span class="line">vi /etc/sysconfig/network-scripts/ifcfg-eth2</span><br></pre></td></tr></table></figure><p> Then, enable it to run after system booting</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ONBOOT=<span class="built_in">yes</span></span><br></pre></td></tr></table></figure></li><li><p>Using tool ping to check the status of accessing public network and local network.   </p></li><li><p>Change the system’s hostname on all servers.</p><ol><li><p>Update file(&#x2F;etc&#x2F;sysctl.conf), add a item </p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel.domainname=master1.bigdata.com</span><br></pre></td></tr></table></figure></li><li><p>Update file(&#x2F;etc&#x2F;hosts), add doamins.</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.110.150 master1.bigdata.com master1</span><br><span class="line">192.168.110.151 slave1.bigdata.com slave1</span><br><span class="line">192.168.110.152 slave2.bigdata.com slave2</span><br></pre></td></tr></table></figure></li><li><p>Update file(&#x2F;etc&#x2F;sysconfig&#x2F;network)</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME=master1.bigdata.com</span><br></pre></td></tr></table></figure></li><li><p>Reboot systems</p></li><li><p>Check the correction of hostname</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hostname</span><br><span class="line">hostname -f</span><br></pre></td></tr></table></figure></li></ol></li><li><p>Set up password-less SSH on all servers</p><ol><li><p>Disable SELinux</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/selinux/config</span><br></pre></td></tr></table></figure><p> Set disabled for SELinux:</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure></li><li><p>Enable access for using public key.<br> <strong>Warning: Filename is sshd_config, not ssh_config.</strong>  </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p> Delete comment signal(“#”) blow:  </p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RSAAuthentication yes</span><br><span class="line">PubkeyAuthentication yes</span><br><span class="line">AuthorizedKeysFile .ssh/authorized_keys</span><br></pre></td></tr></table></figure></li><li><p>Generate public key and private key on master server.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p> Then, click the default option for configuration.</p></li><li><p>Create directories and copy the public key to slave servers.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">slave$ <span class="built_in">mkdir</span> ~/.ssh</span><br><span class="line">slave$ <span class="built_in">cd</span> ~/.ssh</span><br><span class="line">slave$ <span class="built_in">touch</span> authorized_keys</span><br></pre></td></tr></table></figure><p> Login to master server, then copy public key:  </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">maseter$ scp ~/.ssh/id_rsa.pub root@slave1:~/.ssh</span><br></pre></td></tr></table></figure><p> Login to slave server, then copy content from public key to a file(authorized_keys):  </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slave$ <span class="built_in">cat</span> ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p> <strong>Notice: You need to copy your content of local ssh public key to your local authorized_keys if you use standalone mode.</strong></p></li></ol></li><li><p>Install ntpd on all servers.</p><ol><li><p>Check to see if you have installed ntpd.   </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa | grep ntp   </span><br></pre></td></tr></table></figure></li><li><p>If not, install ntpd  </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ntp </span><br></pre></td></tr></table></figure></li></ol></li><li><p>Close transparent huge page on all servers.</p><ol><li><p>Append code block to file(&#x2F;etc&#x2F;rc.local) for closing transparent huge page. </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/rc.local</span><br></pre></td></tr></table></figure><p> code block:  </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">test</span> -f /sys/kernel/mm/transparent_hugepage/enabled; <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">test</span> -f /sys/kernel/mm/transparent_hugepage/defrag; <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure></li><li><p>Reboot operating system.</p></li><li><p>Check the status of the transparent huge page to ensure the status is “never”.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> /sys/kernel/mm/transparent_hugepage/enabled</span><br></pre></td></tr></table></figure></li></ol></li></ol><h3 id="2-Set-up-a-local-repository-on-master-server"><a href="#2-Set-up-a-local-repository-on-master-server" class="headerlink" title="2. Set up a local repository on master server."></a>2. Set up a local repository on master server.</h3><ol><li><p>Install local repository tool.  </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install yum-utils createrepo</span><br></pre></td></tr></table></figure></li><li><p>Install the http service.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install httpd</span><br></pre></td></tr></table></figure></li><li><p>Start the http service.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service httpd start</span><br></pre></td></tr></table></figure></li><li><p>Install tool wget.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install wget</span><br></pre></td></tr></table></figure></li><li><p>Download Ambari &amp; HDP package files to the master server.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.4.2.0/ambari-2.4.2.0-centos6.tar.gz</span><br><span class="line">wget http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos6/HDP-UTILS-1.1.0.20-centos6.tar.gz</span><br><span class="line">wget http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.4.3.0/HDP-2.4.3.0-centos6-rpm.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>Copy Ambari and HDP packages to http dir, then untar them.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /var/www/html</span><br><span class="line"><span class="built_in">mkdir</span> hdp</span><br><span class="line">tar -xvf ambari-2.4.2.0-centos6.tar.gz -C /var/www/html</span><br><span class="line">tar -xvf HDP-2.4.3.0-centos6-rpm.tar.gz -C /var/www/html/hdp</span><br><span class="line">tar -xvf HDP-UTILS-1.1.0.20-centos6.tar.gz -C /var/www/html/hdp</span><br></pre></td></tr></table></figure></li><li><p>Check repository address for next step.</p><p> Ambari base URL and gpgkey:  </p> <figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">http://192.168.110.150/AMBARI-2.4.2.0/centos6/2.4.2.0-136/  </span><br><span class="line">http://192.168.110.150/AMBARI-2.4.2.0/centos6/2.4.2.0-136/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins</span><br></pre></td></tr></table></figure><p> HDP base URL and gpgkey:  </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">http://192.168.110.150/hdp/HDP/centos6/2.x/updates/2.4.3.0</span><br><span class="line">http://192.168.110.150/hdp/HDP/centos6/2.x/updates/2.4.3.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins</span><br></pre></td></tr></table></figure><p> HDP-UTILS base URL and gpgkey(gpgkey file same with HDP gpgkey file):  </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">http://192.168.110.150/hdp/HDP-UTILS-1.1.0.20/repos/centos6</span><br><span class="line">http://192.168.110.150/hdp/HDP/centos6/2.x/updates/2.4.3.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins</span><br></pre></td></tr></table></figure></li><li><p>Copy repository config files.<br> ambari repo file:</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -nv http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.4.3.0/ambari.repo -O /etc/yum.repos.d/ambari.repo</span><br></pre></td></tr></table></figure><p> hdp repo file:</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> /var/www/html/hdp/HDP/centos6/2.x/updates/2.4.3.0/hdp.repo /etc/yum.repos.d/</span><br></pre></td></tr></table></figure></li><li><p>Update repository config files.<br> You ought to renew base URL and gpgkey URL in your ambari.repo and hdp.repo files.</p></li></ol><h3 id="3-Ambari-server-setup-on-master-server"><a href="#3-Ambari-server-setup-on-master-server" class="headerlink" title="3. Ambari-server setup on master server"></a>3. Ambari-server setup on master server</h3><ol><li><p>Install ambari server </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install /var/www/html/AMBARI-2.4.2.0/centos6/2.4.2.0-136/ambari/ambari-server-2.4.2.0-136.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>Run command for ambari setup.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ambari-server setup</span><br></pre></td></tr></table></figure></li><li><p>If you do not disable the SELinux, you need to press “y”.</p></li><li><p>If your Ambari server under user root, you just need to press “n”.</p></li><li><p>If you do not disable iptalbes, you need to press “y” to continue.</p></li><li><p>Select the JDK version, then follow steps to install it.<br> <strong>If you use a custom JDK, you will maybe meet some troubles that will happen within Ambari Installation. So, choosing option one for the recommendation.</strong></p></li><li><p>Choose database type<br> You can choose embedded the database. If you choose MySQL, you need to install the MySQL database on the master server and input info for the Ambari setup. Then put a MySQL connector into a specific path and run the command below to specify the path of the connector.  </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar</span><br></pre></td></tr></table></figure></li><li><p>Run ambari-server</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ambari-server start</span><br></pre></td></tr></table></figure></li></ol><h3 id="4-Ambari-agent-setup-on-slave-servers"><a href="#4-Ambari-agent-setup-on-slave-servers" class="headerlink" title="4. Ambari-agent setup on slave servers."></a>4. Ambari-agent setup on slave servers.</h3><ol><li><p>Copy ambari-agent rpm file to slave servers.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp /var/www/html/AMBARI-2.4.2.0/centos6/2.4.2.0-136/ambari/ambari-agent-2.4.2.0-136.x86_64.rpm root@slave_server_ip:~</span><br></pre></td></tr></table></figure></li><li><p>Install ambari-agent.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install ~/ambari-agent-2.4.2.0-136.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>Renew ambari-server URL in config file.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/ambari-agent/ambari.ini</span><br></pre></td></tr></table></figure><p> Renew ambari-server URL below: </p> <figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[server]</span><br><span class="line">hostname = master1.bigdata.com</span><br></pre></td></tr></table></figure></li><li><p>Run ambari-agent.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ambari-agent start</span><br></pre></td></tr></table></figure></li></ol><h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p>Before you launch the Ambari installation web UI, you ought to ensure things below.</p><ol><li><p>Start the service httpd.</p></li><li><p>Stop the service iptables.  </p></li><li><p>Start the ambari-server on the master server and ambari-agent on slave servers.<br> If you want to start quickly, it is advisable to choose default options.</p></li><li><p>Open your browser, then access to the WebUI and click the item (“Launch Install Wizard”)<br> URL: 192.168.110.150:8080<br> Account: admin<br> Password: admin<br> <img src="/posts/7f0f012a/ai_1.png" alt="login"><br> <img src="/posts/7f0f012a/ai_2.png" alt="login_success">  </p></li><li><p>Get Started<br> <img src="/posts/7f0f012a/ai_3.png" alt="Get Started">  </p></li><li><p>Select Version<br> Choose local repository, then input base URL of HDP and HDP-UTILS.<br> <img src="/posts/7f0f012a/ai_4.png" alt="Select Version">  </p></li><li><p>Install Options<br> Input your master’s hostname and slaves’ hostnames. Then copy your master’s ssh private key into the item.<br> <img src="/posts/7f0f012a/ai_5.png" alt="Install Options">  </p></li><li><p>Confirm Hosts<br> <img src="/posts/7f0f012a/ai_6.png" alt="Confirm Hosts"> </p><p> Just click “ok” to continue.<br> <img src="/posts/7f0f012a/ai_7.png" alt="click ok"> </p></li><li><p>Choose Services<br> Choose what service you want to add to your system. You can add new services to your system later on.<br> <img src="/posts/7f0f012a/ai_8.png" alt="Choose Services"> </p><p> Just click “ok” to continue.<br> <img src="/posts/7f0f012a/ai_9.png" alt="click ok"> </p></li><li><p>Assign Masters<br><img src="/posts/7f0f012a/ai_10.png" alt="Assign Masters"> </p></li><li><p>Assign Slaves and Clients<br><img src="/posts/7f0f012a/ai_11.png" alt="Assign Slaves and Clients"> </p></li><li><p>Customize Services<br>You just need to input info for items with a red exclamation mark.<br><img src="/posts/7f0f012a/ai_12.png" alt="Customize Services"> </p></li><li><p>Review<br><img src="/posts/7f0f012a/ai_13.png" alt="Review"> </p></li><li><p>Install, Start and Test<br>If you get to this step successfully, you can drink a cup of coffee and enjoy this time.<br><img src="/posts/7f0f012a/ai_14.png" alt="Install, Start and Test"> </p></li><li><p>Summary<br>So now, say hello world to Ambari.<br><img src="/posts/7f0f012a/ai_15.png" alt="Install, Start and Test"></p></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/7f0f012a/features.png&quot; alt=&quot;features&quot;&gt; &lt;/p&gt;
&lt;h2 id=&quot;System-Requirements&quot;&gt;&lt;a href=&quot;#System-Requirements&quot; class=&quot;headerlink&quot; title=&quot;System Requirements&quot;&gt;&lt;/a&gt;System Requirements&lt;/h2&gt;&lt;p&gt;At first, we need to think about the setups of VMs. So, I list a table for VMs below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;hostname&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;ip&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;operating system&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;RAM&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;disk space&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;package&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;master1.bigdata.com&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;192.168.110.150&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;centos6.5 64位&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;16g&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;50g&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;ambari-server&lt;br&gt;namenode&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;slave1.bigdata.com&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;192.168.110.151&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;centos6.5 64位&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;16g&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;50g&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;ambari-agent&lt;br&gt;datanode&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;slave2.bigdata.com&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;192.168.110.152&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;centos6.5 64位&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;16g&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;50g&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;ambari-agent&lt;br&gt;datanode&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="Ambari" scheme="https://liuninglin.github.io/tags/Ambari/"/>
    
  </entry>
  
  <entry>
    <title>Odyssey</title>
    <link href="https://liuninglin.github.io/posts/56e69a1d.html"/>
    <id>https://liuninglin.github.io/posts/56e69a1d.html</id>
    <published>2019-01-25T07:33:51.000Z</published>
    <updated>2022-10-01T20:38:34.634Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/56e69a1d/odyssey%5Codyssey.jpg" alt="odyssey"><br>It is really a great determination for me to give up a comfortable and easy environment where I can complete all tasks successfully. Now, I had been to Shanghai for almost one month. And I need to study new development technology and make new friends in this unfamiliar city where I will be surrounded by loneliness. </p><span id="more"></span><p>Nevertheless, I really hope that I can meet all kinds of difficulties which offer me the huge experience to upgrade myself. So, I choose to jump out of my comfortable cage and forage challenges. In my opinion, It is a true tiger that runs above Africa prairie not one in the fancy cage.  </p><p>Finally, I literally hope that my goal will be hit after one year and start with a more rough journey (Odyssey).</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/56e69a1d/odyssey%5Codyssey.jpg&quot; alt=&quot;odyssey&quot;&gt;&lt;br&gt;It is really a great determination for me to give up a comfortable and easy environment where I can complete all tasks successfully. Now, I had been to Shanghai for almost one month. And I need to study new development technology and make new friends in this unfamiliar city where I will be surrounded by loneliness. &lt;/p&gt;</summary>
    
    
    
    <category term="Life log" scheme="https://liuninglin.github.io/categories/Life-log/"/>
    
    
    <category term="Shanghai" scheme="https://liuninglin.github.io/tags/Shanghai/"/>
    
    <category term="life" scheme="https://liuninglin.github.io/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>Using docker command to obtain dockers&#39; IP</title>
    <link href="https://liuninglin.github.io/posts/de75b356.html"/>
    <id>https://liuninglin.github.io/posts/de75b356.html</id>
    <published>2019-01-25T06:27:37.000Z</published>
    <updated>2022-10-01T20:38:34.749Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/de75b356/Using-docker-command-to-obtain-dockers-IP%5Cdocker.jpg" alt="docker"></p><p><strong>check specific container’s ip address:</strong>  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker inspect -f <span class="string">&#x27;&#123;&#123;range.NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;&#x27;</span> your_container_name_or_id</span><br></pre></td></tr></table></figure><p><strong>check all containers’ ip address:</strong>  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker inspect -f <span class="string">&#x27;&#123;&#123;.Name&#125;&#125; - &#123;&#123;range.NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;&#x27;</span> $(docker ps -aq)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/de75b356/Using-docker-command-to-obtain-dockers-IP%5Cdocker.jpg&quot; alt=&quot;docker&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;check specific container’s</summary>
      
    
    
    
    <category term="Operation" scheme="https://liuninglin.github.io/categories/Operation/"/>
    
    <category term="Docker" scheme="https://liuninglin.github.io/categories/Operation/Docker/"/>
    
    
    <category term="Docker" scheme="https://liuninglin.github.io/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Some terms in Big Data Ecosystem</title>
    <link href="https://liuninglin.github.io/posts/aa930aca.html"/>
    <id>https://liuninglin.github.io/posts/aa930aca.html</id>
    <published>2019-01-23T21:59:20.000Z</published>
    <updated>2022-10-01T20:38:34.640Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/aa930aca/Some-terms-in-Big-Data-Ecosystem%5Cbiddata.jpg" alt="bigdata"></p><ul><li><p><strong>DMP(Data Management Platform)</strong><br>A platform for collecting and managing data for digital marketing purposes.</p></li><li><p><strong>DSP(Demand-side Platform)</strong><br>A platform that allows buyers of online advertising to manage ad exchange and pay to buy some advertising banners according to Real-time bidding.</p></li><li><p><strong>SSP(Supply-side Plateform)</strong><br>A platform that allows web publishers or digital media owners to manage their advertising banners and receive money from this platform.</p></li></ul><span id="more"></span><ul><li><strong>MDM(Master Data Management)</strong><br>A business method that helps companies or enterprises to integrate, manage, store critical data in a single point of reference.<br>These data can provide for other business platforms or business systems(B2B, B2C, CRM, OMS, etc).<br>And master data also are useful for marketing decisions and marketing analysis.<br>In China, Alibaba announced a business term(“数据中台”) that is based on MDM.<br>So, in the next few years, we need to build a digital smart platform to collect all sources of business data that can provide<br>information for all kinds of business systems or platforms and bread down obstacles between business systems in companies or enterprises.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/aa930aca/Some-terms-in-Big-Data-Ecosystem%5Cbiddata.jpg&quot; alt=&quot;bigdata&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;DMP(Data Management Platform)&lt;/strong&gt;&lt;br&gt;A platform for collecting and managing data for digital marketing purposes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;DSP(Demand-side Platform)&lt;/strong&gt;&lt;br&gt;A platform that allows buyers of online advertising to manage ad exchange and pay to buy some advertising banners according to Real-time bidding.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;SSP(Supply-side Plateform)&lt;/strong&gt;&lt;br&gt;A platform that allows web publishers or digital media owners to manage their advertising banners and receive money from this platform.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Back-end" scheme="https://liuninglin.github.io/categories/Back-end/"/>
    
    <category term="BigData" scheme="https://liuninglin.github.io/categories/Back-end/BigData/"/>
    
    
    <category term="BigData" scheme="https://liuninglin.github.io/tags/BigData/"/>
    
    <category term="DMP" scheme="https://liuninglin.github.io/tags/DMP/"/>
    
    <category term="MDM" scheme="https://liuninglin.github.io/tags/MDM/"/>
    
  </entry>
  
  <entry>
    <title>Several Tricks for Using Hexo Efficiently</title>
    <link href="https://liuninglin.github.io/posts/eacbb695.html"/>
    <id>https://liuninglin.github.io/posts/eacbb695.html</id>
    <published>2019-01-23T20:58:05.000Z</published>
    <updated>2022-10-01T20:38:34.581Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/eacbb695/Hexo-Some-tricks-for-using-hexo-efficiently%5Cgithub_hexo_blog.png" alt="github_hexo_blog">  </p><h2 id="change-theme"><a href="#change-theme" class="headerlink" title="change theme"></a>change theme</h2><ol><li><p>add a theme to your Hexo (eg. theme next)</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure></li><li><p>update your blog config file(_config.yml, not your theme config file)</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Extensions</span></span><br><span class="line"><span class="comment">## Plugins: https://hexo.io/plugins/</span></span><br><span class="line"><span class="comment">## Themes: https://hexo.io/themes/</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">next</span></span><br></pre></td></tr></table></figure></li></ol><span id="more"></span><h2 id="add-RSS-plugin"><a href="#add-RSS-plugin" class="headerlink" title="add RSS plugin"></a>add RSS plugin</h2><ol><li><p>install RSS tools by npm</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-generator-feed</span><br></pre></td></tr></table></figure></li><li><p>update your blog config file(_config.yml, not your theme config file)</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">feed:</span> <span class="comment"># RSS订阅插件</span></span><br><span class="line"> <span class="attr">type:</span> <span class="string">atom</span></span><br><span class="line"> <span class="attr">path:</span> <span class="string">atom.xml</span></span><br><span class="line"> <span class="attr">limit:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">plugins:</span> <span class="string">hexo-generate-feed</span></span><br></pre></td></tr></table></figure></li><li><p>update your theme config file(theme&#x2F;_config.yml)</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set RSS to false to disable feed link.</span></span><br><span class="line"><span class="comment"># Leave RSS as empty to use site&#x27;s feed link, and install hexo-generator-feed: `npm install hexo-generator-feed --save`.</span></span><br><span class="line"><span class="comment"># Set RSS to specific value if you have burned your feed already.</span></span><br><span class="line"><span class="attr">rss:</span> <span class="string">/atom.xml</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="change-the-code-block-theme"><a href="#change-the-code-block-theme" class="headerlink" title="change the code block theme"></a>change the code block theme</h2><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Code Highlight theme</span></span><br><span class="line"><span class="comment"># Available values: normal | night | night eighties | night blue | night bright</span></span><br><span class="line"><span class="comment"># https://github.com/chriskempson/tomorrow-theme</span></span><br><span class="line"><span class="attr">highlight_theme:</span> <span class="string">night</span></span><br></pre></td></tr></table></figure><h2 id="record-the-number-of-viewers"><a href="#record-the-number-of-viewers" class="headerlink" title="record the number of viewers"></a>record the number of viewers</h2><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Show Views/Visitors of the website/page with busuanzi.</span></span><br><span class="line"><span class="comment"># Get more information on http://ibruce.info/2015/04/04/busuanzi</span></span><br><span class="line"><span class="attr">busuanzi_count:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">total_visitors:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">total_visitors_icon:</span> <span class="string">user</span></span><br><span class="line">  <span class="attr">total_views:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">total_views_icon:</span> <span class="string">eye</span></span><br><span class="line">  <span class="attr">post_views:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">post_views_icon:</span> <span class="string">eye</span></span><br></pre></td></tr></table></figure><h2 id="generate-the-excerpt"><a href="#generate-the-excerpt" class="headerlink" title="generate the excerpt"></a>generate the excerpt</h2><p>There are two methods for generating excerpts for the home page.</p><ol><li><p>update theme config file (theme&#x2F;next&#x2F;_config.yml)<br>It is not good for you when you are using markdown language because of the bad show effect.</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Automatically Excerpt. Not recommend.</span></span><br><span class="line"><span class="comment"># Use &lt;!-- more --&gt; in the post to control excerpt accurately.</span></span><br><span class="line"><span class="attr">auto_excerpt:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">length:</span> <span class="number">300</span></span><br></pre></td></tr></table></figure></li><li><p>add markdown code in your post to control excerpt accurately</p> <figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- more --&gt;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="add-search-plugin"><a href="#add-search-plugin" class="headerlink" title="add search plugin"></a>add search plugin</h2><ol><li><p>install search plugin</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-search --save</span><br></pre></td></tr></table></figure></li><li><p>update the blog config file</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">search:</span></span><br><span class="line">  <span class="attr">path:</span> <span class="string">search.xml</span></span><br><span class="line">  <span class="attr">field:</span> <span class="string">post</span></span><br><span class="line">  <span class="attr">format:</span> <span class="string">html</span></span><br><span class="line">  <span class="attr">limit:</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure></li><li><p>update the theme config file</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Local search</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/hexo-generator-searchdb</span></span><br><span class="line"><span class="attr">local_search:</span></span><br><span class="line"><span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># if auto, trigger search by changing input</span></span><br><span class="line"><span class="comment"># if manual, trigger search by pressing enter key or search button</span></span><br><span class="line"><span class="attr">trigger:</span> <span class="string">auto</span></span><br><span class="line"><span class="comment"># show top n results per article, show all results by setting to -1</span></span><br><span class="line"><span class="attr">top_n_per_article:</span> <span class="number">1</span></span><br><span class="line"><span class="comment"># unescape html strings to the readable one</span></span><br><span class="line"><span class="attr">unescape:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="add-share-plugin"><a href="#add-share-plugin" class="headerlink" title="add share plugin"></a>add share plugin</h2><ol><li><p>get the module from GitHub</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> themes/next</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/theme-next-needmoreshare2 <span class="built_in">source</span>/lib/needsharebutton</span><br></pre></td></tr></table></figure></li><li><p>update the theme config file</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">needmoreshare2:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">postbottom:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">options:</span></span><br><span class="line">      <span class="attr">iconStyle:</span> <span class="string">box</span></span><br><span class="line">      <span class="attr">boxForm:</span> <span class="string">horizontal</span></span><br><span class="line">      <span class="attr">position:</span> <span class="string">bottomCenter</span></span><br><span class="line">      <span class="attr">networks:</span> <span class="string">Weibo,Wechat,Twitter,Facebook</span></span><br><span class="line">  <span class="attr">float:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">options:</span></span><br><span class="line">      <span class="attr">iconStyle:</span> <span class="string">box</span></span><br><span class="line">      <span class="attr">boxForm:</span> <span class="string">horizontal</span></span><br><span class="line">      <span class="attr">position:</span> <span class="string">middleRight</span></span><br><span class="line">      <span class="attr">networks:</span> <span class="string">Weibo,Wechat,Twitter,Facebook</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="add-google-analytics"><a href="#add-google-analytics" class="headerlink" title="add google analytics"></a>add google analytics</h2><ol><li><p>update the theme config file</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Google Analytics</span></span><br><span class="line"><span class="attr">google_analytics:</span> <span class="string">your</span> <span class="string">google</span> <span class="string">track</span> <span class="string">code</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="add-reward"><a href="#add-reward" class="headerlink" title="add reward"></a>add reward</h2><ol><li><p>update the theme config file</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reward</span></span><br><span class="line"><span class="comment"># If true, reward would be displayed in every article by default.</span></span><br><span class="line"><span class="comment"># And you can show or hide one article specially through add page variable `reward: true/false`.</span></span><br><span class="line"><span class="attr">reward:</span></span><br><span class="line"><span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">comment:</span> <span class="string">Thanks</span> <span class="string">For</span> <span class="string">Your</span> <span class="string">Donation!</span></span><br><span class="line"><span class="attr">wechatpay:</span> <span class="string">/images/wechatpay.jpg</span></span><br><span class="line"><span class="attr">alipay:</span> <span class="string">/images/alipay.jpg</span></span><br><span class="line"><span class="comment">#bitcoin: /images/bitcoin.jpg</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="generate-a-permanent-post-address"><a href="#generate-a-permanent-post-address" class="headerlink" title="generate a permanent post address"></a>generate a permanent post address</h2><ol><li><p>install plugin</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-abbrlink --save</span><br></pre></td></tr></table></figure></li><li><p>update the blog config file</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># permalink: :year/:month/:day/:title/</span></span><br><span class="line"><span class="comment"># permalink_defaults:</span></span><br><span class="line"><span class="attr">permalink:</span> <span class="string">posts/:abbrlink.html</span></span><br><span class="line"><span class="attr">abbrlink:</span></span><br><span class="line"><span class="attr">alg:</span> <span class="string">crc32</span>  <span class="comment"># 算法：crc16(default) and crc32</span></span><br><span class="line"><span class="attr">rep:</span> <span class="string">hex</span>    <span class="comment"># 进制：dec(default) and hex</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="add-comment-module-valine"><a href="#add-comment-module-valine" class="headerlink" title="add comment module (valine)"></a>add comment module (valine)</h2><ol><li><p>sign up for a leancloud account  </p></li><li><p>create an application for your comment system  </p></li><li><p>copy AppID and AppKey from your dashboard  </p><p> <img src="/posts/eacbb695/Hexo-Some-tricks-for-using-hexo-efficiently%5Chexo_comment_module1.png" alt="hexo_comment_module1"></p></li><li><p>paste AppID and AppKey into your theme config file  </p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">valine:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span> <span class="comment"># When enable is set to be true, leancloud_visitors is recommended to be closed for the re-initialization problem within different leancloud adk version.</span></span><br><span class="line">  <span class="attr">appid:</span>  <span class="string">xxxxxxxxxx</span> <span class="comment"># your leancloud application appid</span></span><br><span class="line">  <span class="attr">appkey:</span>  <span class="string">xxxxxxxxxx</span> <span class="comment"># your leancloud application appkey</span></span><br><span class="line">  <span class="attr">notify:</span> <span class="literal">false</span> <span class="comment"># mail notifier, See: https://github.com/xCss/Valine/wiki</span></span><br><span class="line">  <span class="attr">verify:</span> <span class="literal">false</span> <span class="comment"># Verification code</span></span><br><span class="line">  <span class="attr">placeholder:</span> <span class="string">Just</span> <span class="string">go</span> <span class="string">go</span> <span class="comment"># comment box placeholder</span></span><br><span class="line">  <span class="attr">avatar:</span> <span class="string">mm</span> <span class="comment"># gravatar style</span></span><br><span class="line">  <span class="attr">guest_info:</span> <span class="string">nick,mail</span> <span class="comment"># custom comment header</span></span><br><span class="line">  <span class="attr">pageSize:</span> <span class="number">10</span> <span class="comment"># pagination size</span></span><br><span class="line">  <span class="attr">visitor:</span> <span class="literal">true</span> <span class="comment"># leancloud-counter-security is not supported for now. When a visitor is set to be true, appid and appkey are recommended to be the same as leancloud_visitors&#x27; for counter compatibility. Article reading statistic https://valine.js.org/visitor.html</span></span><br><span class="line">  <span class="attr">comment_count:</span> <span class="literal">true</span> <span class="comment"># if false, comment count will only be displayed in post page, not in home page</span></span><br></pre></td></tr></table></figure></li><li><p>create classes in your leancloud account to store your data  </p><p> <img src="/posts/eacbb695/Hexo-Some-tricks-for-using-hexo-efficiently%5Chexo_comment_module2.png" alt="hexo_comment_module2"></p></li><li><p><strong>[not required]</strong> you can change your valine lanuage (themes&#x2F;next&#x2F;layout&#x2F;_third-party&#x2F;comments&#x2F;valine.swig)  </p> <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="title class_">Valine</span>(&#123;</span><br><span class="line">    <span class="attr">el</span>: <span class="string">&#x27;#comments&#x27;</span>,</span><br><span class="line">    <span class="attr">lang</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">    <span class="attr">verify</span>: &#123;&#123; theme.<span class="property">valine</span>.<span class="property">verify</span> &#125;&#125;,</span><br><span class="line">    <span class="attr">notify</span>: &#123;&#123; theme.<span class="property">valine</span>.<span class="property">notify</span> &#125;&#125;,</span><br><span class="line">    <span class="attr">appId</span>: <span class="string">&#x27;&#123;&#123; theme.valine.appid &#125;&#125;&#x27;</span>,</span><br><span class="line">    <span class="attr">appKey</span>: <span class="string">&#x27;&#123;&#123; theme.valine.appkey &#125;&#125;&#x27;</span>,</span><br><span class="line">    <span class="attr">placeholder</span>: <span class="string">&#x27;&#123;&#123; theme.valine.placeholder &#125;&#125;&#x27;</span>,</span><br><span class="line">    <span class="attr">avatar</span>: <span class="string">&#x27;&#123;&#123; theme.valine.avatar &#125;&#125;&#x27;</span>,</span><br><span class="line">    <span class="attr">meta</span>: guest,</span><br><span class="line">    <span class="attr">pageSize</span>: <span class="string">&#x27;&#123;&#123; theme.valine.pageSize &#125;&#125;&#x27;</span> || <span class="number">10</span>,</span><br><span class="line">    <span class="attr">visitor</span>: &#123;&#123; theme.<span class="property">valine</span>.<span class="property">visitor</span> &#125;&#125;</span><br><span class="line">  &#125;);</span><br></pre></td></tr></table></figure></li><li><p><strong>[not required]</strong> you can also add a white list for security</p><p> <img src="/posts/eacbb695/Hexo-Some-tricks-for-using-hexo-efficiently%5Chexo_comment_module3.png" alt="hexo_comment_module3"></p></li></ol><h2 id="use-image-plugin"><a href="#use-image-plugin" class="headerlink" title="use image plugin"></a>use image plugin</h2><ol><li><p>install “hexo-asset-image” plugin</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure></li><li><p>update your blog config file (_config.yml)</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">post_asset_folder:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li><li><p>if you use the abbrlink plugin to change your permalink attribute in your blog config file, you need to edit this plugin to fit your config (node_modules&#x2F;hexo-asset-image&#x2F;index.js). Then you add a code block into this javascript file.  </p> <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> link = data.<span class="property">permalink</span>;</span><br><span class="line">link = link.<span class="title function_">replace</span>(<span class="string">&#x27;.html&#x27;</span>, <span class="string">&#x27;/&#x27;</span>);  <span class="comment">//新增加，针对修改后的permalink</span></span><br><span class="line"><span class="keyword">var</span> beginPos = <span class="title function_">getPosition</span>(link, <span class="string">&#x27;/&#x27;</span>, <span class="number">3</span>) + <span class="number">1</span>;</span><br></pre></td></tr></table></figure></li><li><p>Ok, you can generate your new static HTML file.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br></pre></td></tr></table></figure></li></ol><h2 id="add-creative-commons-module-to-your-blog"><a href="#add-creative-commons-module-to-your-blog" class="headerlink" title="add creative commons module to your blog"></a>add creative commons module to your blog</h2><p>It is convenient and useful for a bloger to announce your blog copyright announcement. So, you can add this below to your posts or add this to your profile information page.</p><ol><li><p>update your theme config file</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creative Commons 4.0 International License.</span></span><br><span class="line"><span class="comment"># https://creativecommons.org/share-your-work/licensing-types-examples</span></span><br><span class="line"><span class="comment"># Available values: by | by-nc | by-nc-nd | by-nc-sa | by-nd | by-sa | zero</span></span><br><span class="line"><span class="attr">creative_commons:</span></span><br><span class="line">  <span class="attr">license:</span> <span class="string">by-nc-sa</span></span><br><span class="line">  <span class="attr">sidebar:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">post:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/eacbb695/Hexo-Some-tricks-for-using-hexo-efficiently%5Cgithub_hexo_blog.png&quot; alt=&quot;github_hexo_blog&quot;&gt;  &lt;/p&gt;
&lt;h2 id=&quot;change-theme&quot;&gt;&lt;a href=&quot;#change-theme&quot; class=&quot;headerlink&quot; title=&quot;change theme&quot;&gt;&lt;/a&gt;change theme&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;add a theme to your Hexo (eg. theme next)&lt;/p&gt;
 &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;git &lt;span class=&quot;built_in&quot;&gt;clone&lt;/span&gt; https://github.com/theme-next/hexo-theme-next themes/next&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;update your blog config file(_config.yml, not your theme config file)&lt;/p&gt;
 &lt;figure class=&quot;highlight yml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Extensions&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;## Plugins: https://hexo.io/plugins/&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;## Themes: https://hexo.io/themes/&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;theme:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;next&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Front-end" scheme="https://liuninglin.github.io/categories/Front-end/"/>
    
    <category term="Hexo" scheme="https://liuninglin.github.io/categories/Front-end/Hexo/"/>
    
    
    <category term="Hexo" scheme="https://liuninglin.github.io/tags/Hexo/"/>
    
    <category term="Blog" scheme="https://liuninglin.github.io/tags/Blog/"/>
    
  </entry>
  
  <entry>
    <title>[Hexo]Create a blog with hexo in github</title>
    <link href="https://liuninglin.github.io/posts/fe740d97.html"/>
    <id>https://liuninglin.github.io/posts/fe740d97.html</id>
    <published>2019-01-23T20:43:07.000Z</published>
    <updated>2022-10-01T20:38:34.575Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/posts/fe740d97/Hexo-Create-a-blog-with-hexo-in-github%5Chexo.png" alt="hexo">  </p><ol><li><p>copy local ssh public key to github  </p></li><li><p>install npm, git</p></li><li><p>install hexo by npm  </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br></pre></td></tr></table></figure></li><li><p>init hexo</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></table></figure></li></ol><span id="more"></span><ol start="5"><li><p>install other softwares by npm</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br></pre></td></tr></table></figure></li><li><p>edit file “_config.yml” and write your git address</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">git</span>  </span><br><span class="line"><span class="attr">repository:</span> <span class="string">github项目地址</span></span><br><span class="line"><span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure></li><li><p>install hexo deployment tool</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></li><li><p>generate static pages by hexo</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure></li><li><p>deploy static pages to github</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure></li><li><p>using local tool to pre-check your blog</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure></li><li><p>access to your github blog</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://用户名.github.io</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/posts/fe740d97/Hexo-Create-a-blog-with-hexo-in-github%5Chexo.png&quot; alt=&quot;hexo&quot;&gt;  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;copy local ssh public key to github  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;install npm, git&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;install hexo by npm  &lt;/p&gt;
 &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;npm install&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;init hexo&lt;/p&gt;
 &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;hexo init&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Front-end" scheme="https://liuninglin.github.io/categories/Front-end/"/>
    
    <category term="Hexo" scheme="https://liuninglin.github.io/categories/Front-end/Hexo/"/>
    
    
    <category term="Hexo" scheme="https://liuninglin.github.io/tags/Hexo/"/>
    
    <category term="Blog" scheme="https://liuninglin.github.io/tags/Blog/"/>
    
  </entry>
  
</feed>
